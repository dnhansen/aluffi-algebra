% Document setup
\documentclass[article, a4paper, 11pt, oneside]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[UKenglish]{babel}

% Document info
\newcommand\doctitle{Aluffi, \emph{Algebra: Chapter 0}}
\newcommand\docauthor{Danny Nyg√•rd Hansen}

% Formatting and layout
\usepackage[autostyle]{csquotes}
\usepackage[final]{microtype}
\usepackage{xcolor}
\frenchspacing
\usepackage{latex-sty/articlepagestyle}
\usepackage{latex-sty/articlesectionstyle}
% \usepackage{latex-sty/amalgsymbol}

% Fonts
\usepackage[largesmallcaps]{kpfonts}
\DeclareSymbolFontAlphabet{\mathrm}{operators} % https://tex.stackexchange.com/questions/40874/kpfonts-siunitx-and-math-alphabets
\linespread{1.06}
\let\mathfrak\undefined
\usepackage{eufrak}
\usepackage{inconsolata}
% \usepackage{amssymb}

% Hyperlinks
\usepackage{hyperref}
\definecolor{linkcolor}{HTML}{4f4fa3}
\hypersetup{%
	pdftitle=\doctitle,
	pdfauthor=\docauthor,
	colorlinks,
	linkcolor=linkcolor,
	citecolor=linkcolor,
	urlcolor=linkcolor,
	bookmarksnumbered=true
}

% Equation numbering
\numberwithin{equation}{chapter}

% Footnotes
\footmarkstyle{\textsuperscript{#1}\hspace{0.25em}}

% Mathematics
\usepackage{latex-sty/basicmathcommands}
\usepackage{latex-sty/framedtheorems}
\usepackage{tikz-cd}
\tikzcdset{arrow style=math font} % https://tex.stackexchange.com/questions/300352/equalities-look-broken-with-tikz-cd-and-math-font
\usetikzlibrary{babel}

% Lists
\usepackage{enumitem}
\setenumerate[0]{label=\normalfont(\alph*)}
\setlist{  
  listparindent=\parindent,
  parsep=0pt,
}

% Bibliography
\usepackage[backend=biber, style=authoryear, maxcitenames=2, useprefix]{biblatex}
\addbibresource{references.bib}

% Title
\title{\doctitle}
\author{\docauthor}

\newcommand{\setF}{\mathbb{F}}
\newcommand{\ev}{\mathrm{ev}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calR}{\mathcal{R}}
\newcommand{\calO}{\mathcal{O}}
\newcommand{\strucS}{\mathfrak{S}}
\DeclarePairedDelimiter{\gen}{\langle}{\rangle} % Generating set
\newcommand{\frakL}{\mathfrak{L}}
\newcommand{\frakN}{\mathfrak{N}}
\newcommand{\frakA}{\mathfrak{A}}
\newcommand{\frakB}{\mathfrak{B}}
\newcommand{\ab}{\mathit{ab}}

\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\coker}{coker}
\DeclareMathOperator{\stab}{Stab}

% Categories
\newcommand{\cat}[1]{\mathcal{#1}}
\newcommand{\scat}[1]{\mathbf{#1}} % category supposed to be small
\newcommand{\ncat}[1]{\mathbf{#1}} % named categories like Set, Top

\newcommand{\scatC}{\scat{C}}

\newcommand{\catSet}{\ncat{Set}} % Category of sets
\newcommand{\catGrp}{\ncat{Grp}} % Category of groups
\newcommand{\catAb}{\ncat{Ab}} % Category of abelian groups
\newcommand{\catRing}{\ncat{Ring}} % Category of rings
\newcommand{\catCRing}{\ncat{CRing}} % Category of commutative rings
\newcommand{\catFld}{\ncat{Fld}} % Category of fields

\newcommand{\catMod}[1]{{#1\text{-}\scat{Mod}}}
\newcommand{\catRMod}{\catMod{R}}

\newcommand{\catAlg}[1]{{#1\text{-}\scat{Alg}}}
\newcommand{\catRAlg}{\catAlg{R}}

\newcommand{\End}{\mathrm{End}}
\newcommand{\Hom}{\mathrm{Hom}}

\DeclareMathOperator{\chr}{char}


%% Framed exercise environment

\mdfdefinestyle{swannexercise}{%
    skipabove=0.5em plus 0.4em minus 0.2em,
	skipbelow=0.5em plus 0.4em minus 0.2em,
	leftmargin=-5pt,
	rightmargin=-5pt,
	innerleftmargin=5pt,
	innerrightmargin=5pt,
	innertopmargin=5pt,
	innerbottommargin=4pt,
	linewidth=0pt,
	splittopskip=1.2em minus 0.2em,
	splitbottomskip=0.5em plus 0.2em minus 0.1em,
	backgroundcolor=backgroundcolor,
	frametitlebackgroundcolor=titlecolor,
	frametitlefont={\scshape},
    theoremseparator={},
    % theoremspace={},
	frametitleaboveskip=3pt,
	frametitlebelowskip=2pt
}

\mdtheorem[style=swannexercise]{exerciseframed}{Exercise}

\let\oldexerciseframed\exerciseframed
\renewcommand{\exerciseframed}{%
  \crefalias{theorem}{exerciseframed}%
  \oldexerciseframed}

\usepackage{listofitems}

\settocdepth{subsection}
\renewenvironment{exerciseframed}[1][]{%
    \setsepchar{.}%
    \readlist*\mylist{#1}%
    \def\smalllabel{\mylist[2].\mylist[3]}%
    \refstepcounter{exerciseframed}%
    \addcontentsline{toc}{subsection}{Exercise \smalllabel}%
    \begin{exerciseframed*}[\smalllabel]%
    \label{ex:#1}%
}{%
    \end{exerciseframed*}%
}

% https://tex.stackexchange.com/a/23491/63353
\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}

\newcommand{\exref}[1]{%
    \setsepchar{.}%
    \readlist*\mylist{#1}%
    \ifnum \arabic{chapter}=\mylist[1]
        \def\mylabel{\mylist[2].\mylist[3]}%
    \else
        \def\mylabel{\RNum{\mylist[1]}.\mylist[2].\mylist[3]}%
    \fi
    \hyperref[ex:#1]{Exercise~\mylabel}%
}

\theoremstyle{nonumberplain}
\theoremsymbol{\ensuremath{\square}}
\newtheorem{solution}{Solution}

\let\oldsolution\solution
\renewcommand{\solution}{%
  \crefalias{theorem}{solution}%
  \oldsolution}

\newcommand{\solutionlabelfont}[1]{{\normalfont\color{linkcolor}#1}}
\newlist{solutionsec}{enumerate}{1}
\setlist[solutionsec]{leftmargin=0pt, parsep=0pt, listparindent=\parindent, font=\solutionlabelfont, label=(\alph*), labelsep=0pt, labelwidth=20pt, itemindent=20pt, align=left, itemsep=10pt}


\renewcommand{\thechapter}{\Roman{chapter}}
% \renewcommand{\thesection}{\arabic{section}}

\DeclarePairedDelimiter{\ord}{\lvert}{\rvert}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\Inn}{Inn}

\usepackage{caption} % Links to figures jump correctly
\Crefname{figure}{Figure}{Figures}


\newenvironment{displaytheorem}{%
	\begin{displayquote}\itshape%
}{%
	\end{displayquote}%
}


\newcommand{\upset}{\operatorname{\uparrow}}
\newcommand{\downset}{\operatorname{\downarrow}}
\newcommand{\matgroup}[3]{\mathrm{#1}_{#2}(#3)}
\newcommand{\GL}[2]{\matgroup{GL}{#1}{#2}}
\newcommand{\SL}[2]{\matgroup{SL}{#1}{#2}}
\newcommand{\catGSet}[1][G]{{#1\text{-}\catSet}}
\newcommand{\frakI}{\mathfrak{I}}
\newcommand{\field}{\mathbb{F}}
\let\bigcoprod\coprod
\renewcommand{\coprod}{\sqcup}


\begin{document}

\maketitle

\chapter{Preliminaries: Set theory and categories}

\chapter{Groups, first encounter}

\section{Definition of group}

\begin{exerciseframed}[2.1.4]
    Suppose that $g^2 = e$ for all elements $g$ of a group $G$; prove that $G$ is commutative.
\end{exerciseframed}

\begin{solution}
    The hypothesis implies that $g = g\inv$ for all $g \in G$. For $g,h \in G$ we thus have
    %
    \begin{equation*}
        gh
        = (gh)\inv
        = h\inv g\inv
        = hg
    \end{equation*}
    %
    as desired.
\end{solution}


\begin{exerciseframed}[2.1.8]
    Let $G$ be a finite abelian group with exactly one element $f$ of order $2$. Prove that $\bigprod_{g \in G} g = f$.
\end{exerciseframed}

\begin{solution}
    Every element $g$ in $G$ different from $e$ and $f$ has order greater than two, hence $g \neq g\inv$. The product $\bigprod_{g \in G \setminus \{e,f\}} g$ therefore contains all such elements along with their inverses, and thus equals $e$. The claim follows.
\end{solution}


\begin{exerciseframed}[2.1.9]
    Let $G$ be a finite group, of order $n$, and let $m$ be the number of elements $g \in G$ of order exactly $2$. Prove that $n-m$ is odd. Deduce that if $n$ is even, then $G$ necessarily contains elements of order $2$.
\end{exerciseframed}

\begin{solution}
    Let $G'$ denote the set of elements in $G$ with order greater than $2$. We claim that $\card{G'}$ is even, and we give two arguments for this fact. First, simply notice that the elements of $G'$ come in pairs $\{g, g\inv\}$ with $g \neq g\inv$.

    For a more precise argument (using group theory language we haven't seen yet), consider the inversion map $g \mapsto g\inv$. This restricts to a well-defined map $\iota \colon G' \to G'$, and $\iota$ is a permutation of $G'$. Letting the cyclic group $\gen{\iota} \leq S_{G'}$ act on $G'$ splits $G'$ into orbits of size two, and since these orbits determine a partition of $G'$, $\card{G'}$ must be even.

    Now notice that $G'$ contains $n-m-1$ elements since $e$ has order $1$, hence $n-m$ is odd. If $n$ is even, then $m$ must be odd and thus at least $1$.
\end{solution}


\begin{exerciseframed}[2.1.11]
    Prove that for all $g,h$ in a group $G$, $\ord{gh} = \ord{hg}$.
\end{exerciseframed}

\begin{solution}
    Let $a,g \in G$, and let $n = \ord{g}$. Then
    %
    \begin{equation*}
        (aga\inv)^n
            = a g^n a\inv
            = e,
    \end{equation*}
    %
    so the order of $aga\inv$ divides the order of $g$. Substituting $g \to aga\inv$ and $a \to a\inv$ shows that $\ord{g}$ also divides $\ord{aga\inv}$, so $\ord{g} = \ord{aga\inv}$. Finally substituting $g \to gh$ and $a \to h$ proves the claim.

    Alternatively, the conjugation map $g \mapsto aga\inv$ is an isomorphism, so it preserves orders.
\end{solution}


\begin{exerciseframed}[2.1.13]
    Give an example showing that $\ord{gh}$ is not necessarily equal to $\lcm(\ord{g}, \ord{h})$, even if $g$ and $h$ commute.
\end{exerciseframed}

\begin{solution}
    In $\ints/4\ints$ we have $\ord{[2]_4} = 2$ and $\ord{[2]_4 + [2]_4} = \ord{[0]_4} = 1$.
\end{solution}


\begin{exerciseframed}[2.1.14]
    Prove that if $g$ and $h$ commute \emph{and} $\gcd(\ord{g}, \ord{h}) = 1$, then $\ord{gh} = \ord{g} \, \ord{h}$.
\end{exerciseframed}

\begin{solution}
    First recall that $\lcm(\ord{g}, \ord{h}) = \ord{g} \, \ord{h}$, so Proposition~1.14 implies that $\ord{gh}$ divides $\ord{g} \, \ord{h}$. Conversely, letting $N = \ord{gh}$ we have
    %
    \begin{equation*}
        e
            = (gh)^{\ord{g}N}
            = g^{\ord{g}N} h^{\ord{g}N}
            = h^{\ord{g}N},
    \end{equation*}
    %
    so $\ord{h}$ divides $\ord{g}N$. But since $\ord{g}$ and $\ord{h}$ are relatively prime, $\ord{h}$ divides $N$. So does $\ord{g}$, so again using relative primality we find that $\ord{g} \, \ord{h}$ divides $N$. In total, $\ord{gh} = \ord{g} \, \ord{h}$.
\end{solution}


\section{Examples of groups}

\begin{exerciseframed}[2.2.1]
    One can associate an $n \times n$ matrix $M_\sigma$ with a permutation $\sigma \in S_n$ by letting the entry at\footnotemark{} $(i,\sigma(i))$ be $1$ and letting all other entries be $0$. Prove that, with this notation,
    %
    \begin{equation*}
        M_\sigma M_\tau = M_{\tau\sigma}
    \end{equation*}
    %
    for all $\sigma, \tau \in S_n$, where the product on the right is the ordinary product of matrices.
\end{exerciseframed}\footnotetext{Contrary to Aluffi, we prefer to let permutation act on the left.}

\begin{solution}
    Notice that, for $1 \leq i,j \leq n$,
    %
    \begin{equation*}
        (M_\sigma M_\tau)_{ij}
            = \sum_{k=1}^n (M_\sigma)_{ik} (M_\tau)_{kj},
    \end{equation*}
    %
    and that the summand $(M_\sigma)_{ik} (M_\tau)_{kj}$ is $1$ just when $\sigma(i) = k$ and $\tau\sigma(i) = j$, and $0$ otherwise. Thus,
    %
    \begin{equation*}
        (M_\sigma M_\tau)_{ij} =
            \begin{cases}
                1, & \tau\sigma(i) = j, \\
                0, & \text{otherwise},
            \end{cases}
    \end{equation*}
    %
    which is just the definition of the matrix $M_{\tau\sigma}$.
\end{solution}


\begin{exerciseframed}[2.2.5]
    Describe generators and relations for all dihedral groups $D_{2n}$.
\end{exerciseframed}

\begin{solution}
    Consider a regular $n$-gon, let $x$ be reflection about a line through its centre and a vertex, and let $y$ be the counterclockwise rotation by $2\pi/n$. Then $x$ and $y$ generate $D_{2n}$ subject to a series of relations. First of all, clearly $x^2 = e$ and $y^n = e$ (more precisely, $x$ and $y$ have order $2$ and $n$ respectively). Furthermore, a geometric argument shows that $(xy)^2 = e$, or equivalently that $yx = xy^{n-1}$. By applying this third relation successively, any product $x^{i_1} y^{i_2} x^{i_3} y^{i_4} \cdots$ can be reduced to one on the form $x^i y^j$. Using the other two relations we find that we can choose $i$ and $j$ such that $0 \leq i \leq 1$ and $0 \leq j < n$, which yields $2n$ products on this form.

    Next we show that all these products are different. Given two products $x^{i_1} y^{j_1}$ and $x^{i_2} y^{j_2}$, if either $i_1 = i_2$ or $j_1 = j_2$ then this is obvious. So assume that $i_1 \neq i_2$ and $j_1 \neq j_2$. Without loss of generality also assume that $i_1 = 0$ and $i_2 = 1$. Now consider the equation
    %
    \begin{equation*}
        y^{j_2 - j_1} = x.
    \end{equation*}
    %
    It follows from Proposition~1.13 that $j_2 - j_1 = \pm n/2$. But the third relation above then implies that
    %
    \begin{equation*}
        y^{\frac{n}{2} + 1}
            = y^{\frac{n}{2} + n - 1},
    \end{equation*}
    %
    or $e = y^{n-2}$ which is impossible. Hence the equation has no solutions, and all products $x^i y^j$ are distinct.
\end{solution}


\begin{exerciseframed}[2.2.13]
    Prove that if $\gcd(m,n) = 1$, then there exist integers $a$ and $b$ such that
    %
    \begin{equation*}
        am + bn = 1.
    \end{equation*}
    %
    Conversely, prove that if $am + bn = 1$ for some integers $a$ and $b$, then $\gcd(m,n) = 1$.
\end{exerciseframed}

\begin{solution}
    By Corollary~2.5, the class $[m]_n$ generates $\ints/n\ints$. Hence there exists an $a \in \ints$ such that $a [m]_n = [1]_n$. But then $qn = am - 1$ for some $q \in \ints$, i.e. $am + (-q)n = 1$.

    Conversely, if $am + bn = 1$ and $d$ divides both $m$ and $n$, then $d$ also divides $1$ and hence $d = \pm 1$.
\end{solution}


\section[The category Grp][The category $\catGrp$]{The category $\catGrp$}

\begin{remarkbreak}[Universal algebra]
    Let $A$ be a set and let $n \in \naturals$. An \emph{operation on $A$} is a set function $A^n \to A$. The set $A^0 \cong 1$ is a singleton and is only determined up to isomorphism in $\catSet$. The number $n$ is called the operation's \emph{arity}, and we say that the operation is \emph{$n$-ary}. A $0$-ary operation is also called a \emph{constant}. If $c \colon \{*\} \to A$ is a constant, then instead of $c(*)$ we simply write $c$.
    
    An \emph{algebra} is a pair $\frakA = \langle A, F \rangle$, where $A$ is a set\footnote{Many authors require $A$ to be nonempty, e.g. Manzano and Bergman} and $F$ is a collection (of arbitrary cardinality) of operations on $A$. Writing $F = \set{f_i}{i \in I}$ for some index set $I$, the \emph{(similarity) type} of $\frakA$ is the function $\rho \colon I \to \naturals$ where $\rho(i)$ is the arity of $f_i$. If $I$ is a finite set, then writing $I = \{1, \ldots, k\}$ we display the type of an algebra by the tuple $\langle \rho(1), \ldots, \rho(k) \rangle$, and we also refer to this tuple as the type of the algebra. Notice that if any operation in $F$ is a constant, then $A$ must be nonempty.

    Let $\frakA = \langle A, F \rangle$ and $\frakB = \langle B, G \rangle$ be algebras of the same similarity type $\rho \colon I \to \naturals$, and write $F = \set{f_i}{i \in I}$ and $G = \set{g_i}{i \in I}$. A set function $\phi \colon A \to B$ is called a \emph{homomorphism} from $\frakA$ to $\frakB$ if
    %
    \begin{equation*}
        \phi \left( f_i(a_1, \ldots, a_{\rho(i)}) \right)
            = g_i \left( \phi(a_1), \ldots, \phi(a_{\rho(i)}) \right)
    \end{equation*}
    %
    for all $i \in I$ and all $a_1, \ldots, a_{\rho(i)} \in A$. And this is equivalent to the diagram (in $\catSet$)
    %
    \begin{equation*}
        \begin{tikzcd}
            A^n
                \ar[r, "\phi \times \cdots \times \phi"]
                \ar[d, "f_i", swap]
            & B^n
                \ar[d, "g_i"] \\
            A
                \ar[r, "\phi", swap]
            & B
        \end{tikzcd}
    \end{equation*}
    %
    commuting for all $i \in I$. In the case where $\rho(i) = 0$ we interpret the $0$-fold product of $\phi$ with itself as the unique map $1 \to 1$, and the above square is equivalent to the triangle
    %
    \begin{equation*}
        \begin{tikzcd}[column sep=small]
            & 1
                \ar[dl, "f_i", swap]
                \ar[dr, "g_i"] \\
            A
                \ar[rr, "\phi", swap]
            && B
        \end{tikzcd}
    \end{equation*}
    %
    Then $f_i$ and $g_i$ are constants, and this diagram means that $\phi(f_i) = g_i$, i.e. that $\phi$ preserves this constant.

    By pasting together two of the above types of diagrams, it is easy to see that function composition preserves homomorphisms. Since the identity on each algebra is obviously a homomorphism, each similarity type gives rise to a category.
\end{remarkbreak}


\begin{remarkbreak}[Images and preimages of subalgebras]
    Let $\frakA = \langle A, \calF^\frakA \rangle$ and $\frakB = \langle B, \calF^\frakB \rangle$ be algebras of the same type $\rho \colon \calF \to \naturals$, where $\calF$ is a family of operation symbols. Further let $S \subseteq A$ and $T \subseteq B$ be subalgebras, and let $\phi \colon \frakA \to \frakB$ a homomorphism.

    We first claim that $\phi(S)$ is a subalgebra of $\frakB$. Consider $f \in \calF$ and let $n = \rho(f)$. For any $y_1, \ldots, y_n \in \phi(S)$ there exist $x_1, \ldots, x_n \in S$ such that $\phi(x_i) = y_i$ for $i = 1, \ldots n$. Then $f^\frakA(x_1, \ldots, x_n) \in S$ since $S$ is a subalgebra, and because $\phi$ is a homomorphism we have
    %
    \begin{equation*}
        f^\frakB(y_1, \ldots, y_n)
            = f^\frakB \bigl( \phi(x_1), \ldots, \phi(x_n) \bigr)
            = \phi \bigl( f^\frakA(x_1, \ldots, x_n) \bigr)
            \in \phi(S)
    \end{equation*}
    %
    as required.

    Next we claim that $\phi\preim(T)$ is either empty (if this is not allowed in the definition of an algebra) or a subalgebra of $\frakA$. If $x_1, \ldots, x_n \in \phi\preim(T)$, then $\phi(x_1), \ldots, \phi(x_n) \in T$. Hence
    %
    \begin{equation*}
        \phi \bigl( f^\frakA(x_1, \ldots, x_n) \bigr)
            = f^\frakB \bigl( \phi(x_1), \ldots, \phi(x_n) \bigr)
            \in T
    \end{equation*}
    %
    since $T$ is a subalgebra. Hence $f^\frakA(x_1, \ldots, x_n) \in \phi\inv(T)$ as claimed. Furthermore, if there is a nullary symbol $f$ in $\calF$, then $\phi\preim(T)$ is nonempty since it must contain the image of $f^\frakA$.
\end{remarkbreak}


\begin{remarkbreak}[Homomorphisms and generating subalgebras]
    \label{rem:generating-subalgebra}
    If $\frakA = \langle A, F \rangle$ is an algebra and $S \subseteq A$, then we denote the smallest subalgebra of $\frakA$ containing $S$ by $\gen{S}$, or $\gen{S}_{\frakA}$ if needed.

    Often we are interested in studying a family of algebras of the same type (groups, rings, modules over a fixed ring, etc.). In this case we fix a set $\calF$ of \emph{operation symbols} and a similarity type $\rho \colon \calF \to \naturals$. An algebra of this type is then an algebra on the form $\frakA = \langle A, \calF^\frakA \rangle$, where $\calF^\frakA$ is an \emph{interpretation} of the symbols in $\calF$. That is a set $\set{f^\frakA}{f \in \calF}$ of operations on $A$ such that $f^\frakA$ has arity $\rho(f)$.

    Let $\frakA = \langle A, \calF^\frakA \rangle$ and $\frakB = \langle B, \calF^\frakB \rangle$ be algebras of the same type, where $\calF$ is a family of operation symbols. If $\phi \colon \frakA \to \frakB$ is a homomorphism and $S \subseteq A$, then we claim that $\gen{\phi(S)}_\frakB = \phi(\gen{S}_\frakA)$. The inclusion \enquote{$\subseteq$} is obvious since the image of a subalgebra is a subalgebra. For the other inclusion, first define $S_0 = S$ and
    %
    \begin{equation*}
        S_{n+1}
            = S_n \union \set[\big]{f^\frakA(a_1, \ldots, a_k)}{a_1, \ldots, a_k \in S_n \text{ and } f \in \calF}
    \end{equation*}
    %
    for $n \in \naturals$. Then clearly $\gen{S}_\frakA = \bigunion_{n\in\naturals} S_n$. Since function images respect unions, it suffices to show that $\phi(S_n) \subseteq \gen{\phi(S)}_\frakB$ for all $n \in \naturals$. This is obvious for $n = 0$. Next assume that the inclusion holds for some $n \in \naturals$, let $f \in \calF$ have rank $k$, and let $a_1, \ldots, a_k \in S_n$. Then
    %
    \begin{equation*}
        \phi \bigl( f^\frakA(a_1, \ldots, a_k) \bigr)
            = f^\frakB \bigl( \phi(a_1), \ldots, \phi(a_k) \bigr)
            \in \gen{\phi(S)}_\frakB
    \end{equation*}
    %
    by induction, since each $\phi(a_i)$ lies in $\phi(S_n)$.
\end{remarkbreak}


\begin{remark}
    The universal algebra definition of groups is as follows: First, a \emph{semigroup} is an algebra $\langle S, \,\cdot\, \rangle$, where $\cdot$ is a binary operation (in infix notation), satisfying the associative law
    %
    \begin{equation*}
        x \cdot (y \cdot z)
            = (x \cdot y) \cdot z.
    \end{equation*}
    %
    A \emph{monoid} is an algebra $\langle M, \,\cdot\,, e \rangle$, where $e$ is a constant, such that $\langle M, \,\cdot\, \rangle$ is a semigroup, and which satisfies the identities
    %
    \begin{equation*}
        x \cdot e = x
        \quad \text{and} \quad
        e \cdot x = x.
    \end{equation*}
    %
    Finally, a \emph{group} is an algebra $\langle G, \,\cdot\,, \iota, e \rangle$, where $\iota$ is a unary operation, such that $\langle G, \,\cdot\,, e \rangle$ is a monoid, and which satisfies
    %
    \begin{equation*}
        x \cdot \iota(x)
            = e
            = \iota(x) \cdot x.
    \end{equation*}
    %
    We usually denote the operation $\cdot$ by concatenation, and we write $\iota(x) = x\inv$.

    For a map $\phi \colon G \to H$ between groups, it turns out that for $\phi$ to be a group homomorphism it is sufficient that $\phi$ commutes with the group operation. This is obviously also the case for semigroups, but for monoids we do require it to respect both the group operation and preserve the identity.
\end{remark}


\begin{exerciseframed}[2.3.3]
    Show that if $G,H$ are \emph{abelian} groups, then $G \prod H$ satisfies the universal property for coproducts in $\catAb$.
\end{exerciseframed}

\begin{solution}
    Let $\phi_G \colon G \to K$ and $\phi_H \colon H \to K$ be homomorphisms into an abelian group $K$. Define a map $\psi \colon G \prod H \to K$ by
    %
    \begin{equation*}
        \psi(g,h)
            = \phi_G(g) \phi_H(h).
    \end{equation*}
    %
    We first show that $\psi$ is a group homomorphism. For $g_1,g_2 \in G$ and $h_1, h_2 \in H$ we have
    %
    \begin{align*}
        \psi((g_1,h_1)(g_2,h_2))
            &= \psi(g_1 g_2, h_1 h_2)
             = \phi_G(g_1 g_2) \phi_H(h_1 h_2) \\
            &= \phi_G(g_1) \phi_G(g_2) \phi_H(h_1) \phi_H(h_2) \\
            &= \phi_G(g_1) \phi_H(h_1) \phi_G(g_2) \phi_H(h_2) \\
            &= \psi(g_1, h_1) \psi(g_2, h_2).
    \end{align*}
    %
    In the fourth equality we used that $K$ is abelian. Next we show that the diagram
    %
    \begin{equation*}
        \begin{tikzcd}
            G
                \ar[dr, "\iota_G"]
                \ar[drr, "\phi_G", bend left] \\
            & G \prod H
                \ar[r, "\psi"]
            & K \\
            H
                \ar[ur, "\iota_H", swap]
                \ar[urr, "\phi_H", bend right, swap]
        \end{tikzcd}
    \end{equation*}
    %
    commutes, where $\iota_G(g) = (g, e_H)$ and $\iota_H(h) = (e_G, h)$. For the upper triangle we have
    %
    \begin{equation*}
        (\psi \circ \iota_G)(g)
            = \psi(g, e_G)
            = \phi_G(g) \phi_H(e_G)
            = \phi_G(g) e_K
            = \phi_G(g),
    \end{equation*}
    %
    and similarly for the lower triangle. Finally notice that $\psi$ is unique with this property, since if $\chi \colon G \prod H \to K$ is any such homomorphism we have
    %
    \begin{equation*}
        \chi(g,h)
            = \chi(g,e_H) \chi(e_G,h)
            = (\chi \circ \iota_G)(g) (\chi \circ \iota_H)(h)
            = \phi_G(g) \phi_H(h),
    \end{equation*}
    %
    so $\chi = \psi$.
\end{solution}


\begin{exerciseframed}[2.3.4]
    Let $G,H$ be groups, and assume that $G \cong H \times G$. Can you conclude that $H$ is trivial?
\end{exerciseframed}

\begin{solution}
    Let $H$ be any nontrivial group, and let $G = \bigprod_{n\in\naturals} H$. Then the map $\phi \colon G \to H \prod  G$ given by
    %
    \begin{equation*}
        \phi(h_1, h_2, h_3, \ldots)
            = \bigl( h_1, (h_2, h_3, \ldots) \bigr)
    \end{equation*}
    %
    is an isomorphism.
\end{solution}


\begin{exerciseframed}[2.3.5]
    Prove that $\rationals$ is not the direct product of two nontrivial groups.
\end{exerciseframed}

\begin{solution}
    Let $G$ and $H$ be groups such that there is an isomorphism $\phi \colon \rationals \to G \prod H$. Assume without loss of generality that $G$ is nontrivial, and consider the map $\phi_G = \pi_G \circ \phi$. We claim that $\phi_G$ is injective.

    First notice that if $g \in G$ has finite order then $g = 0_G$, since $(g,0_H)$ has finite order in $G \prod H$. Let $p,q \in \ints$ with $p,q \neq 0$, and notice that $\phi_G(p/q) = 0_G$ implies that
    %
    \begin{equation*}
        0_G
            = q \phi_G \Bigl( \frac{p}{q} \Bigr)
            = \phi_G(p)
            = p \phi_G(1).
    \end{equation*}
    %
    Hence $\phi_G(1) = 0_G$, and so $\ints \subseteq \ker \phi_G$. Furthermore, if $a,b \in \ints$ with $b \neq 0$, then
    %
    \begin{equation*}
        b \phi_G \Bigl( \frac{a}{b} \Bigr)
            = \phi_G(a)
            = 0_G,
    \end{equation*}
    %
    so $\phi_G(a/b)$ has finite order and hence equals $0_G$. Thus if $\ker \phi_G$ is nontrivial, then $\ker \phi_G = \rationals$. But since $\phi_G$ is surjective and $G$ is nontrivial, this is impossible. Hence $\phi_G$ is injective. On the other hand, the kernel of $\phi_G$ is clearly $1 \prod H$, so $H$ must be trivial.
\end{solution}


\begin{exerciseframed}[2.3.6]
    Consider the product $C_2 \prod C_3$ of the cyclic groups $C_2,C_3$. By \exref{2.3.3}, this group is a coproduct of $C_2$ and $C_3$ in $\catAb$. Show that it is \emph{not} a coproduct of $C_2$ and $C_3$ in $\catGrp$.
\end{exerciseframed}

\begin{solution}
    Denote by $g$ and $h$ generators of $C_2$ and $C_3$ respectively, and define group homomorphisms $\phi_2 \colon C_2 \to S_3$ and $\phi_3 \colon C_3 \to S_3$ by
    %
    \begin{equation*}
        \phi_2(g) = (1 \; 2)
        \quad \text{and} \quad
        \phi_3(h) = (1 \; 2 \; 3).
    \end{equation*}
    %
    Assume that $C_2 \prod C_3$ is a coproduct of $C_2$ and $C_3$ in $\catGrp$. Then there exists a homomorphism $\psi \colon C_2 \prod C_3 \to S_3$ such that $\phi_2 = \psi \circ \iota_2$ and $\phi_3 = \psi \circ \iota_3$. Since $C_2 \prod C_3$ is commutative, it follows that
    %
    \begin{equation*}
        (1 \; 2) (1 \; 2 \; 3)
            = \psi( \iota_2(g) \iota_3(h) )
            = \psi( \iota_3(h) \iota_2(g) )
            = (1 \; 2 \; 3) (1 \; 2).
    \end{equation*}
    %
    But this is false, so $C_2 \prod C_3$ is not a coproduct of $C_2$ and $C_3$ in $\catGrp$.
\end{solution}


\begin{exerciseframed}[2.3.8]
    Define a group $G$ with two generators $x,y$ subject (only) to the relations $x^2 = e_G$, $y^3 = e_G$. Prove that $G$ is a coproduct of $C_2$ and $C_3$ in $\catGrp$.
\end{exerciseframed}

\begin{solution}
    Denote the generators of $C_2$ and $C_3$ by $g$ and $h$ respectively, and let $\phi_2 \colon C_2 \to H$ and $\phi_3 \colon C_3 \to H$ be homomorphisms into a group $H$. Define a map $\psi \colon G \to H$ by letting $\psi(x) = \phi_2(g)$ and $\psi(y) = \phi_3(h)$ and extending to all elements in $G$ by requiring that $\psi$ be a homomorphism. Then $\phi_2 = \psi \circ \iota_2$ and $\phi_3 = \psi \circ \iota_3$, and $\psi$ is clearly unique with this property, so $G$ is indeed a coproduct.
\end{solution}


\section{Group homomorphisms}

\begin{remarkbreak}[Cyclic groups]
    We prove that a group $G$ is generated by a single element if and only if it is cyclic. The \enquote{if} part is clear, so assume that $G$ is generated by some element $g$ and consider the exponential map $\epsilon_g \colon \ints \to G$ given by $\epsilon_g(n) = g^n$. This is clearly a surjective group homomorphism, so if it is injective then $G \cong \ints$, and if it is trivial then $G \cong 1$.
    
    Otherwise choose $m_1 < m_2$ such that $g^{m_1} = g^{m_2}$. Then $g^{m_2 - m_1} = e$, so let $m$ be the smallest positive integer such that $g^m = e$. It follows that the kernel of $\epsilon_g$ is $m\ints$, so in this case $G \cong \ints/m\ints$.
\end{remarkbreak}

\begin{exerciseframed}[2.4.1]
    Check that the function $\pi_m^n$ defined in ¬ß4.1 is well-defined and makes the diagram commute. Verify that it is a group homomorphism. % Why is the hypothesis $m \mid n$ necessary?
\end{exerciseframed}

\begin{solution}
    Recall that $\pi_m^n \colon \ints/n\ints \to \ints/m\ints$ is defined by $\pi_m^n([a]_n) = [a]_m$, assuming that $m \mid n$. To show that this is well-defined, let $a,b \in \ints$ with $a \equiv b \pmod n$. This means that $n \mid a - b$, and hence that $m \mid a - b$, i.e. that $a \equiv b \pmod m$. In other words, $[a]_n = [b]_n$ implies that $[a]_m = [b]_m$, and thus $\pi_m^n$ is well-defined. It is also obvious that the diagram
    %
    \begin{equation*}
        \begin{tikzcd}
            \ints
                \ar[d, "\pi_n", swap]
                \ar[dr, "\pi_m"] \\
            \ints/n\ints
                \ar[r, "\pi_m^n", swap]
            & \ints/m\ints
        \end{tikzcd}
    \end{equation*}
    %
    commutes, since $\pi_n(a) = [a]_n$ and $\pi_m(a) = [a]_m$.

    Finally we show that $\pi_m^n$ is a homomorphism. For $a,b \in \ints$ we have
    %
    \begin{align*}
        \pi_m^n([a]_n + [b]_n)
            &= \pi_m^n([a + b]_n)
             = [a + b]_m
             = [a]_m + [b]_m \\
            &= \pi_m^n([a]_n) + \pi_m^n([b]_n)
    \end{align*}
    %
    as desired.
\end{solution}


\begin{exerciseframed}[2.4.4]
    Prove that no two of the groups $(\ints, +)$, $(\rationals, +)$, $(\reals, +)$ are isomorphic to one another. Can you decide whether $(\reals, +)$, $(\complex, +)$ \emph{are} isomorphic to one another?
\end{exerciseframed}

\begin{solution}
    Firstly, $\reals$ is uncountable so cannot be isomorphic to $\ints$ or $\rationals$. Secondly, $\ints$ is cyclic but $\rationals$ is not: This follows since if $p \in \rationals^*$, then $p/2 \not\in \gen{p}$, and hence $p$ is not a generator of $\rationals$.

    Next we claim that $\reals$ and $\complex$ are indeed isomorphic. Both $\reals$ and $\complex \cong \reals^2$ are $\rationals$-vector spaces, so let $\calB$ be a Hamel basis of $\reals$ (using the axiom of choice, not sure if the claim holds without it). Then
    %
    \begin{equation*}
        \calC
            = \set{(b,0)}{b \in \calB} \union \set{(0,b)}{b \in \calB}
    \end{equation*}
    %
    is a Hamel basis of $\complex$. Again using the axiom of choice we have $\card{\calB} = \card{\calB \prod \calB}$, and since $\card{\calB} \leq \card{\calC} \leq \card{\calB \prod \calB}$, $\reals$ and $\complex$ are equidimensional as $\rationals$-vector spaces. They are thus isomorphic as vector spaces, and hence as abelian groups.
\end{solution}


\begin{exerciseframed}[2.4.8]
    Let $G$ be a group, and let $g \in G$. Prove that the function $\gamma_g \colon G \to G$ defined by $\gamma_g(a) = gag\inv$ is an automorphism of $G$. (The automorphisms $\gamma_g$ are called \enquote{inner} automorphisms of $G$.) Prove that the function $G \to \Aut(G)$ defined by $g \mapsto \gamma_g$ is a homomorphism. Prove that this homomorphism is trivial if and only if $G$ is abelian.
\end{exerciseframed}

\begin{solution}
    For $a,b \in G$ we have
    %
    \begin{equation*}
        \gamma_g(ab)
            = g(ab)g\inv
            = (gag\inv)(gbg\inv)
            = \gamma_g(a) \gamma_g(b),
    \end{equation*}
    %
    so $\gamma_g$ is a homomorphism. It is obviously invertible with $\gamma_g\inv = \gamma_{g\inv}$, hence an isomorphism.

    Now let also $h \in G$. Then
    %
    \begin{equation*}
        (\gamma_{gh})(a)
            = (gh)a(gh)\inv
            = g(hah\inv)g\inv
            = \gamma_g(hah\inv)
            = (\gamma_g \circ \gamma_h)(a),
    \end{equation*}
    %
    so $g \mapsto \gamma_g$ is a homomorphism.
\end{solution}


\begin{exerciseframed}[2.4.9]
    Prove that if $m,n$ are positive integers such that $\gcd(m,n) = 1$, then $C_{mn} \cong C_m \prod C_n$.
\end{exerciseframed}

\begin{solution}
    The map $\pi = (\pi_m^{mn}, \pi_n^{mn})$ is a group homomorphism, and since the sets $C_{mn}$ and $C_m \prod C_n$ have the same cardinality, it suffices to show that $\pi$ is injective. Using additive notation, if $\pi([a]_{mn}) = \pi([b]_{mn})$ then $[a]_m = [b]_m$, i.e. $m \mid a - b$. Similarly $n \mid a - b$, and since $\gcd(m,n) = 1$ we have $mn \mid a - b$. It follows that $[a]_{mn} = [b]_{mn}$ as desired.
\end{solution}


\begin{exerciseframed}[2.4.14]
    Prove that the order of the group of automorphisms of a cyclic group $C_n$ is the number of positive integers $r < n$ that are relatively prime to $n$. (This is called \emph{Euler's $\phi$-function}; cf. \exref{2.6.14}.)
\end{exerciseframed}

\begin{solution}
    We use additive notation. Let $\phi \in \Aut_\catGrp(\ints/n\ints)$ be an automorphism, and let $k \in \ints$. Then
    %
    \begin{equation*}
        \phi([k])
            = k \phi([1]),
    \end{equation*}
    %
    so every automorphism (indeed every \emph{endo}morphism) is given by multiplication by some element $\phi([1])$. Next notice that since $\phi$ is injective, we must have $\phi([1]) \in (\ints/n\ints)^*$, otherwise we would have $\phi([k]) = [0]$ for some $k \neq 0$. Hence the map
    %
    \begin{align*}
        \mu \colon (\ints/n\ints)^* &\to \Aut_\catGrp(\ints/n\ints), \\
        k &\mapsto \mu_k,
    \end{align*}
    %
    where $\mu_k$ is multiplication by $k$, is surjective if it is well-defined. But it is, since $\mu_k\inv = \mu_{k\inv}$. It is also clearly injective, which proves the desired claim.

    In fact $\mu$ is a group homomorphism, since
    %
    \begin{equation*}
        \mu_{k+l}(x)
            = (k+l)x
            = kx + lx
            = \mu_k(x) + \mu_l(x),
    \end{equation*}
    %
    hence a group isomorphism.
\end{solution}


\section{Free groups}

\begin{exerciseframed}[2.5.3]
    Use the universal property of free groups to prove that the map $j \colon A \to F(A)$ is injective, for all sets $A$.
\end{exerciseframed}

\begin{solution}
    This is obvious for sets with less than two elements, so assume that there are elements $a,b \in A$ with $a \neq b$. Define a set function $f \colon A \to \ints$ by letting $f(a) = 1$ and $f(x) = 0$ for $x \neq a$. By the universal property there exists a group homomorphism $\phi \colon F(A) \to \ints$ such that $f = \phi \circ j$. Then
    %
    \begin{equation*}
        \phi(j(a))
            = f(a)
            \neq f(b)
            = \phi(j(b)),
    \end{equation*}
    %
    so we must have $j(a) \neq j(b)$, and thus $j$ is injective.
\end{solution}


\begin{exerciseframed}[2.5.8]
    Prove that $F(A \coprod B) = F(A) * F(B)$ for all sets $A,B$.
\end{exerciseframed}

\begin{solution}
    Given homomorphisms $\phi_A \colon F(A) \to G$ and $\phi_B \colon F(B) \to G$ into a group $G$, we must prove the existence and uniqueness of a homomorphism $\psi \colon F(A \coprod B) \to G$ such that the diagram
    %
    \begin{equation*}
        \begin{tikzcd}
            F(A)
                \ar[dr, "k_A"]
                \ar[drr, "\phi_A", bend left] \\
            & F(A \coprod B)
                \ar[r, "\psi", dashed]
            & G \\
            F(B)
                \ar[ur, "k_B", swap]
                \ar[urr, "\phi_B", bend right, swap]
        \end{tikzcd}
    \end{equation*}
    %
    commutes, for suitable definitions of $k_A$ and $k_B$. Denoting the injection from $A$ into $F(A)$ by $j_A$, we let $f_A = \phi_A \circ j_A$, and we define $f_B$ analogously. The universal property for coproducts in $\catSet$ yields a unique set function $f \colon A \coprod B \to G$ making the diagram
    %
    \begin{equation*}
        \begin{tikzcd}
            A
                \ar[dr, "i_A"]
                \ar[drr, "f_A", bend left] \\
            & A \coprod B
                \ar[r, "f", dashed]
            & G \\
            B
                \ar[ur, "i_B", swap]
                \ar[urr, "f_B", bend right, swap]
        \end{tikzcd}
    \end{equation*}
    %
    commute. The universal property for free groups then yields a unique homomorphism $\psi \colon F(A \coprod B) \to G$ such that
    %
    \begin{equation*}
        \begin{tikzcd}
            F(A \coprod B)
                \ar[r, "\psi", dashed]
            & G \\
            A \coprod B
                \ar[u, "j"]
                \ar[ur, "f", swap]
        \end{tikzcd}
    \end{equation*}
    %
    commutes. Choose $k_A$ to be the unique homomorphism such that $k_A \circ j_A  = j \circ i_A$. Then we have
    %
    \begin{equation*}
        \phi_A \circ j_A
            = f_A
            = f \circ i_A
            = \psi \circ j \circ i_A
            = \psi \circ k_A \circ j_A,
    \end{equation*}
    %
    and since $j_A$ is injective by \exref{2.5.3} it follows that $\psi_A = \psi \circ k_A$ as desired.

    It remains to be shown that $\psi$ is unique with this property. But any such homomorphism making the first diagram commutes would induce arrows such that the other diagrams also commute, hence induce the same unique arrow $\psi$ in the final diagram. Hence $\psi$ is unique which proves the claim.
\end{solution}


\begin{exerciseframed}[2.5.10]
    Let $F = F^\ab(A)$.
    %
    \begin{enumerate}
        \item Define an equivalence relation $\sim$ on $F$ by setting $f' \sim f$ if and only if $f - f' = 2g$ for some $g \in F$. Prove that $F/{\sim}$ is a finite set if and only if $A$ is finite, and in that case $\card{F/{\sim}} = 2^{\card{A}}$.
        \item Assume $F^\ab(B) \cong F^\ab(A)$. If $A$ is finite, prove that $B$ is also, and that $A \cong B$ as sets.
    \end{enumerate}
\end{exerciseframed}

\begin{solution}
\begin{solutionsec}
    \item Writing $f = \sum_{a \in A} m_a j(a)$ and $f' = \sum_{a \in A} m'_a j(a)$ in the notation of ¬ß5.4, we find that
    %
    \begin{equation*}
        2g
            = f - f'
            = \sum_{a \in A} (m_a - m'_a) j(a)
    \end{equation*}
    %
    for some $g \in F$ if and only if $m_a \equiv m'_a \pmod 2$ for all $a \in A$. That is, the $\sim$-equivalence classes are determined by a choice of sign for each coefficient $m_a$. If $A$ is finite there are finitely many such choices, namely $2^{\card{A}}$. Conversely, it is clear that there are at least as many choices as elements in $A$, so $\card{F/{\sim}} \geq \card{A}$. Thus $F/{\sim}$ is finite if and only if $A$ is.

    \item Let $\phi \colon F^\ab(A) \to F^\ab(B)$ be an isomorphism. Let $\sim_A$ and $\sim_B$ denote the above equivalence relations on $F^\ab(A)$ and $F^\ab(B)$ respectively, and notice that $f \sim_A f'$ if and only if $\phi(f) \sim_B \phi(f')$. Thus the number of $\sim_A$- and $\sim_B$-equivalence classes agree, so (a) implies that $A$ is finite if and only if $B$ is finite. In this case we have
    %
    \begin{equation*}
        2^{\card{A}}
            = \card{F^\ab(A)/{\sim_A}}
            = \card{F^\ab(B)/{\sim_B}}
            = 2^{\card{B}}.
    \end{equation*}
    %
    It follows that $\card{A} = \card{B}$, and thus that $A \cong B$ as sets.
\end{solutionsec}
\end{solution}



\section{Subgroups}

\begin{remark}
    We restate Proposition~6.6 in more explicitly categorical language: Let $\phi \colon G \to G'$ be a homomorphism. The inclusion $i \colon \ker\phi \to G$ is an equaliser of $\phi$ and the trivial map $0 \colon G \to G'$. In other words, for any group homomorphism $\alpha \colon K \to G$ such that $\phi \circ \alpha = 0 \circ \alpha$ there is a unique homomorphism $\overline\alpha \colon K \to \ker\phi$ such that the following diagram commutes:
    %
    \begin{equation*}
        \begin{tikzcd}[row sep=tiny]
            K
                \ar[dr, "\alpha"]
                \ar[dd, "\overline\alpha", swap, dashed]
            \\
            & G
                \ar[r, "\phi", shift left]
                \ar[r, "0", shift right, swap]
            & G'
            \\
            \ker\phi
                \ar[ur, "i", swap, hook]
        \end{tikzcd}
    \end{equation*}
    %
    For $k \in K$ we must have $(\phi \circ \alpha)(k) = e_{G'}$, so $\alpha(k) \in \ker\phi$. The unique choice of $\overline\alpha$ is then just $\alpha$ with codomain restricted to $\ker\phi$.
\end{remark}


% Exercise II.6.2: Reference in IV.1.20

\begin{exerciseframed}[2.6.4]
    Let $G$ be a group, and let $g \in G$. Verify that the image of the exponential map $\epsilon_g \colon \ints \to G$ is a cyclic group (in the sense of Definition~4.7).
\end{exerciseframed}

\begin{solution}
    If $\epsilon_g$ is injective, then $\ints \cong \epsilon_g(\ints)$. Otherwise $\ker \epsilon_g$ is nontrivial, so let $n$ be the least positive number in $\ker \epsilon_g$, and consider the map $\phi \colon \ints/n\ints \to \epsilon_g(\ints)$ given by $\phi([a]_n) = g^a$. This is easily seen to be well-defined, bijective and a group homomorphism, hence an isomorphism.

    Alternatively, the first isomorphism theorem implies that $\ints/\ker\epsilon_g \cong \epsilon_g(\ints)$, and $\ker\epsilon_g = n\ints$ for some $n \in \naturals$.
\end{solution}


\begin{exerciseframed}[2.6.6]
    Prove that the union of a family of subgroups of a group $G$ is not necessarily a subgroup of $G$. In fact:
    %
    \begin{enumerate}
        \item Let $H,H'$ be subgroups of a group $G$. Prove that $H \union H'$ is a subgroup of $G$ only if $H \subseteq H'$ or $H' \subseteq H$.
        \item On the other hand, let $H_0 \subseteq H_1 \subseteq H_2 \subseteq \cdots$ be subgroups of a group $G$. Prove that $\bigunion_{i \geq 0} H_i$ \emph{is} a subgroup of $G$.
    \end{enumerate}
\end{exerciseframed}

\begin{solution}
\begin{solutionsec}
    \item Assume that $H \union H'$ is a subgroup of $G$ and let $h \in H$ and $h' \in H'$. Then $hh' \in H \union H'$, say $hh' \in H$. But then $h' = h\inv (hh') \in H$, so $h' \in H$ and hence $H' \subseteq H$. Similarly if $hh' \in H'$.
    
    \item Write $H = \bigunion_{i \geq 0} H_i$. If $g,h \in H$, then $g \in H_i$ and $h \in H_j$ for some $i,j \in \naturals$.\footnote{The natural numbers include zero.} Hence $g,h \in H_i \union H_j = H_{i \join j} \subseteq H$. We furthermore have $g\inv \in H_i \subseteq H$.
\end{solutionsec}
\end{solution}


\begin{exerciseframed}[2.6.7]
    Show that \emph{inner} automorphisms (cf. \exref{2.4.8}) form a subgroup of $\Aut(G)$; this subgroup is denoted $\Inn(G)$. Prove that $\Inn(G)$ is cyclic if and only if $\Inn(G)$ is trivial if and only if $G$ is abelian. Deduce that if $\Aut(G)$ is cyclic, then $G$ is abelian.
\end{exerciseframed}

\begin{solution}
    It is clear that $\Inn(G)$ is a subgroup of $\Aut(G)$. Assume that $\Inn(G)$ is cyclic, and let $a \in G$ be such that $\gamma_a$ generates $\Inn(G)$. For $g \in G$ we then have $\gamma_g = \gamma_a^n = \gamma_{a^n}$ for some $n \in \ints$. Hence
    %
    \begin{equation*}
        gag\inv
            = \gamma_g(a)
            = \gamma_{a^n}(a)
            = a^n a a^{-n}
            = a,
    \end{equation*}
    %
    so $a$ commutes with every $g \in G$. For $b \in G$ we thus have
    %
    \begin{equation*}
        \gamma_g(b)
            = \gamma_{a^n}(b)
            = a^n b a^{-n}
            = b,
    \end{equation*}
    %
    so $\gamma_g$ is the identity map for every $g \in G$. Therefore $\Inn(G)$ is trivial, which is obviously equivalent to $G$ being abelian.

    Finally, if $\Aut(G)$ is cyclic then Propositions~6.9 and 6.11 imply that $\Inn(G)$ is also cyclic. But then $G$ is abelian.
\end{solution}


\begin{exerciseframed}[2.6.8]
    Prove that an \emph{abelian} group $G$ is finitely generated if and only if there is a surjective homomorphism
    %
    \begin{equation*}
        \underbrace{\ints \oplus \cdots \oplus \ints}_{\text{$n$ times}}
            \twoheadrightarrow G
    \end{equation*}
    %
    for some $n$.
\end{exerciseframed}

\begin{solution}
    First notice the general fact that if $\phi \colon G \to H$ is any group homomorphism and $A \subseteq G$, then $\phi(\gen{A}) = \gen{\phi(A)}$: The inequality $\supseteq$ is obvious, so let $h \in \phi(\gen{A})$. Then $h = \phi(g)$ for some $g \in \gen{A}$, and $g$ is on the form $g_1 \cdots g_n$ for $g_i \in A$. Hence $h = \phi(g_1) \cdots \phi(g_1) \in \gen{\phi(A)}$ as claimed.

    If $\phi \colon \ints^{\oplus n} \to G$ is surjective, then since $\ints^{\oplus n} = \gen{A}$ with $A = \{1, \ldots, n\}$ we have $G = \gen{\phi(1), \ldots, \phi(n)}$. Conversely, if $G$ is generated by a finite set $\{g_1, \ldots, g_n\}$, then we construct a homomorphism $\phi \colon \ints^{\oplus n} \to G$ as follows: Define a set function $f \colon A \to G$ by $f(i) = g_i$. Since $\ints^{\oplus n} = F^\ab(A)$, $f$ induces a homomorphism $\phi \colon \ints^{\oplus n} \to G$. But the image of $\phi$ is a subgroup containing $g_1, \ldots, g_n$, hence contains $\gen{g_1, \ldots, g_n} = G$. Thus $\phi$ is surjective.
\end{solution}


\begin{exerciseframed}[2.6.9]
    Prove that every finitely generated subgroup of $\rationals$ is cyclic. Prove that $\rationals$ is not finitely generated.
\end{exerciseframed}

\begin{solution}
    Let
    %
    \begin{equation*}
        G
            = \gen[\Big]{ \frac{p_1}{q_1}, \ldots, \frac{p_n}{q_n} }
    \end{equation*}
    %
    be a finitely generated subgroup of $\rationals$, and let $m = \lcm(q_1, \ldots, q_n)$. Then each $p_i/q_i$ is an integer multiple of $1/m$, and so $G \subseteq \gen{1/m}$. But every subgroup of a cyclic group is cyclic, so $G$ is cyclic.

    Since $\rationals$ is not cyclic (cf. \exref{2.4.4}), it is not finitely generated.
\end{solution}

\newcommand{\divides}{\mid}

\begin{exerciseframed}[2.6.14]
    If $m$ is a positive integer, denote by $\phi(m)$ the number of positive integers $r < m$ that are relatively prime to $m$; this is called \emph{Euler's $\phi$- (or \textquote{totient}) function}. In other words, $\phi(m)$ is the order of the group $(\ints/m\ints)^*$; cf. Proposition 2.6. Put together the following observations:
    %
    \begin{enumerate}
        \item $\phi(m)$ equals the number of generators of $C_m$,

        \item every element of $C_n$ generates a subgroup of $C_n$,

        \item the discussion following Proposition 6.11 (in particular, every subgroup of $C_n$ is isomorphic to $C_m$ for some $m \divides n$),
    \end{enumerate}
    %
    to obtain a proof of the formula
    %
    \begin{equation*}
        \sum_{m > 0, m \divides n} \phi(m) = n.
    \end{equation*}
\end{exerciseframed}

\begin{solution}
    \begin{solutionsec}
        \item This is just Corollary~2.5, which says that $[n]_m$ generates $\ints/m\ints$ iff $\gcd(m,n) = 1$.

        \item This is obvious.

        \item In additive notation, every subgroup $H$ of $\ints/n\ints$ is cyclic, hence isomorphic to $\ints/m\ints$ for some $m$. Lagrange's theorem then implies that $m \divides n$.

        Alternatively, Proposition~6.11 says that $H$ is generated by $[d]_n$ for some $d \divides n$. But then
        %
        \begin{equation*}
            m
                \defn \abs{ [d]_n }
                = \frac{n}{\gcd(d,n)}
        \end{equation*}
        %
        by Proposition~2.3, and $m$ is also a divisor of $n$.
    \end{solutionsec}
    
    We now prove the formula. Denote by $G_m$ the subset of $C_n$ that generate a subgroup of order $m > 0$. Then $\card{G_m} = \phi(m)$ by (a). Notice that every element in $C_n$ lies in some $G_m$ by (b), and that the $G_m$ are disjoint so that they form a partition of $C_n$. Finally, $G_m = \emptyset$ unless $m \divides n$ by (c). In total we have
    %
    \begin{equation*}
        n
            = \card{C_n}
            = \sum_{m > 0} \card{G_m}
            = \sum_{m > 0, m \divides n} \card{G_m}
            = \sum_{m > 0, m \divides n} \phi(m),
    \end{equation*}
    %
    as desired.
\end{solution}


\begin{exerciseframed}[2.6.16]
    The homomorphism $\phi \colon \ints/3\ints \to S_3$ given by
    %
    \begin{equation*}
        \phi([0]) =
        \begin{pmatrix}
            1 & 2 & 3 \\
            1 & 2 & 3
        \end{pmatrix},
        \quad
        \phi([1]) =
        \begin{pmatrix}
            1 & 2 & 3 \\
            3 & 1 & 2
        \end{pmatrix},
        \quad
        \phi([2]) =
        \begin{pmatrix}
            1 & 2 & 3 \\
            2 & 3 & 1
        \end{pmatrix}
    \end{equation*}
    %
    is a monomorphism; show that it has \emph{no} left-inverse in $\catGrp$.
\end{exerciseframed}

\begin{solution}
    Assume towards a contradiction that $\phi$ has a left-inverse $\psi$. Since $\psi$ is neither injective nor trivial, the kernel of $\psi$ must be a proper nontrivial subgroup of $S_3$. It is also normal, and the only such subgroup is $A_3$. But $A_3$ is precisely the image of $\phi$, so $\psi$ cannot be its left-inverse.
\end{solution}


\section{Quotient groups}

\begin{remark}
    We elaborate on the condition $H \subseteq \ker\phi$ in the statement of Theorem~7.12.

    If $f \colon X \to Y$ is a set function, then we define the \emph{kernel} of $f$, written $\ker f$, as the equivalence relation on $X$ given by $x' \sim_f x$ if and only if $f(x) = f(x')$. In $\catSet$ we cannot say much more about this relation, but in $\catGrp$ we recover the usual notion of kernel as follows: If $\phi \colon G \to G'$ is a group homomorphism and $a,b \in G$, then $a \sim_\phi b$ if and only if $\phi(a) = \phi(b)$. But since $\phi$ is a homomorphism, this is equivalent to $\phi(a\inv b) = e_{G'}$, i.e. $a\inv b \in \ker\phi$ in the group-theoretic sense. Thus the equivalence relation generated by the subgroup $\ker\phi$ is precisely the relation $\ker\phi$, and in particular the notation $G/\ker\phi$ for the quotient group is unambiguous.

    Given a subgroup $H$ of $G$ we define a left-invariant equivalence relation on $G$ by $a \sim_H b$ if and only if $a\inv b \in H$.\footnote{We focus on left-invariant relations here, but we could just as well have defined right-invariant relations instead.} In this notation we thus have ${\sim_\phi} = {\sim_{\ker\phi}}$. If $K$ is another subgroup of $G$, then we claim that $H \subseteq K$ if and only if ${\sim_H} \subseteq {\sim_K}$. Assuming that $H \subseteq K$ and $a \sim_H b$ we have $a\inv b \in H \subseteq K$, so also $a \sim_K b$. Conversely, if ${\sim_H} \subseteq {\sim_K}$ and $g \in H$, then also $e\inv g \in H$ so $g \sim_H e$. It follows that $g \sim_K e$, so $g = e\inv g \in K$.

    We can now understand the condition $H \subseteq \ker\phi$. This says that ${\sim_H} \subseteq {\sim_{\ker\phi}} = {\sim_\phi}$, i.e. that $a \sim_H b$ implies $\phi(a) = \phi(b)$. When $H$ is normal, this is precisely the property that ensures the existence and uniqueness of a homomorphism $\tilde\phi \colon G/{\sim_H} \to G'$ such that $\tilde\phi \circ \pi = \phi$.
\end{remark}


\begin{exerciseframed}[2.7.10]
    Let $G$ be a group, and $H \subseteq G$ a subgroup. With notation as in \exref{2.6.7}, show that $H$ is normal in $G$ if and only if $\gamma(H) \subseteq H$ for all $\gamma \in \Inn(G)$.
\end{exerciseframed}

\begin{solution}
    Since $H$ is normal if and only if $gHg\inv \subseteq H$ for all $g \in G$, and every inner automorphism is on the form $\gamma_g$ for some $g \in G$, the claim follows.
\end{solution}


\begin{exerciseframed}[2.7.11]
    Let $G$ be a group, and let $[G,G]$ be the subgroup of $G$ generated by all elements of the form $a b a\inv b\inv$. Prove that $[G,G]$ is normal in $G$. Prove that $G/[G,G]$ is commutative.
\end{exerciseframed}

\begin{solution}
    For $a,b \in G$, write $[a,b] = a b a\inv b\inv$. If $\phi \colon G \to H$ is any homomorphism into a group $H$, we have
    %
    \begin{equation*}
        \phi([a,b])
            = \phi(a b a\inv b\inv)
            = \phi(a) \phi(b) \phi(a)\inv \phi(b)\inv
            = [\phi(a),\phi(b)].
    \end{equation*}
    %
    Thus the homomorphic image of any commutator is itself a commutator, and thus $\phi([G,G]) \subseteq [H,H]$. It follows from \exref{2.7.10} that $[G,G]$ is normal.

    To show that $G/[G,G]$ is commutative, let $\pi \colon G \to G/[G,G]$ denote the quotient map and notice that
    %
    \begin{equation*}
        [\pi(a),\pi(b)] = \pi([a,b]) = e
    \end{equation*}
    %
    for all $a,b \in G$.
\end{solution}


\begin{exerciseframed}[2.7.12]
    Let $F = F(A)$ be a free group, and let $f \colon A \to G$ be a set-function from the set $A$ to a \emph{commutative} group $G$. Prove that $f$ induces a unique homomorphism $F/[F,F] \to G$. Conclude that $F/[F,F] \cong F^\ab(A)$.
\end{exerciseframed}

\begin{solution}
    First $f$ induces a unique homomorphism $\phi \colon F \to G$. Next notice that if $a,b \in F$, then $\phi([a,b]) = [\phi(a),\phi(b)] = e_G$ since $G$ is commutative, so $[F,F] \subseteq \ker\phi$. Hence Theorem~7.12 induces a unique homomorphism $\tilde\phi \colon F/[F,F] \to G$. Thus $\tilde\phi$ makes the diagram
    %
    \begin{equation*}
        \begin{tikzcd}
            F/[F,F]
                \ar[r, "\tilde\phi"]
            & G \\
            A
                \ar[u, "j'"]
                \ar[ur, "f", swap]
        \end{tikzcd}
    \end{equation*}
    %
    commute. If $\psi \colon F/[F,F] \to G$ is any such homomorphism and $\pi \colon F \to F/[F,F]$ is the quotient map, the diagram
    %
    \begin{equation*}
        \begin{tikzcd}
            F
                \ar[r, "\psi \circ \pi"]
            & G \\
            A
                \ar[u, "j"]
                \ar[ur, "f", swap]
        \end{tikzcd}
    \end{equation*}
    %
    also commutes. But then $\psi \circ \pi = \phi$, and so $\psi = \tilde\phi$. Thus $F/[F,F]$ satisfies the universal property of $F^\ab(A)$, and these are thus isomorphic.
\end{solution}


\begin{exerciseframed}[2.7.13]
    Let $A,B$ be sets and $F(A), F(B)$ the corresponding free groups. Assume $F(A) \cong F(B)$. If $A$ is finite, prove that $B$ is also and $A \cong B$.
\end{exerciseframed}

\begin{solution}
    First notice that $[F(A),F(A)]$ and $[F(B),F(B)]$ are isomorphic, since homomorphisms send commutators to commutators, so $F(A)/[F(A),F(A)]$ and $F(B)/[F(B),F(B)]$ are also isomorphic. Thus \exref{2.7.12} implies that $F^\ab(A) \cong F^\ab(B)$. By \exref{2.5.10} we then have $A \cong B$ as desired.
\end{solution}


\section{Canonical decomposition and Lagrange's theorem}

\begin{remark}
    Given a normal subgroup $H$ of a group $G$, Proposition~8.9 gives a bijection $u$ from subgroups $K \leq G$ that contain $H$ to subgroups $K/H$ of $G/H$. If $\pi \colon G \to G/H$ is the quotient map, then $u(K) = \pi(K)$, and Aluffi shows that this has inverse $v(K') = \pi\preim(K')$ for $K' \leq G/H$.

    We give a slightly different proof of this fact, based on the following concept: Given a set function $f \colon X \to Y$, a subset $A \subseteq X$ is said to be \emph{saturated with respect to $f$} if $A = f\preim(B)$ for some $B \subseteq Y$. The following are then equivalent:\footnote{This is Exercise~3.59 in Lee's \emph{Introduction to Topological Manifolds}. We shall only need the equivalence of (b) and (c) but include the rest for the sake of exposition.}
    %
    \begin{enumerate}
        \item $A$ is saturated.
        \item $A = f\preim(f(A))$.
        \item $A$ is a union of fibres.
        \item If $x \in A$, then $f(x) = f(x')$ implies that $x' \in A$, for all $x' \in X$.
    \end{enumerate}
    %
    We first prove this claim.

    \begin{proofsec}
        \item[(a) $\Leftrightarrow$ (b)]
        Let $A = f\preim(B)$. Then $f(A) \subseteq B$, so $f\preim(f(A)) \subseteq f\preim(B) = A$, and the opposite inclusion always holds. The opposite implication is obvious.

        \item[(a) $\Leftrightarrow$ (c)]
        Simply notice that
        %
        \begin{equation*}
            f\preim(B)
                = f\preim \biggl( \bigunion_{y \in B} \{y\} \biggr)
                = \bigunion_{y \in B} f\preim(y)
        \end{equation*}
        %
        for any $B \subseteq Y$, so $A$ is on the form $f\preim(B)$ if and only if it is a union of fibres.

        \item[(c) $\Leftrightarrow$ (d)]
        If $f(x) = f(x')$ then $x$ and $x'$ lie in the same fibre, and this fibre is either contained entirely in $A$ or is disjoint from $A$. Conversely, if $x \in A$ then $f\preim(x) \subseteq A$, since $f(x) = f(x')$ for all $x'$ in this preimage.
    \end{proofsec}

    The application of this concept to the proposition in question is as follows: Given any quotient map $q \colon X \to X/{\sim}$ in $\catSet$ (so in particular in $\catGrp$), any equivalence class $[x]$, considered as a subset of $X$, is equal to the fibre $q\preim([x])$.

    Now we can easily prove that $u$ and $v$ are each other's inverses: Firstly,
    %
    \begin{equation*}
        (u \circ v)(K') = \pi(\pi\preim(K')) = K'
    \end{equation*}
    %
    for subgroups $K'$ of $G/H$ since $\pi$ is surjective. Secondly, if $K$ is a subgroup of $G$ containing $H$, then it is a union of all cosets $aH$ for $a \in K$. But these cosets are equivalence classes, so $K$ is a union of fibres. The above then shows that
    %
    \begin{equation*}
        (v \circ u)(K) = \pi\preim(\pi(K)) = K
    \end{equation*}
    %
    as desired.
\end{remark}


\begin{remarkbreak}[Second isomorphism theorem]
    Given a group $G$ and subgroups $H$ and $K$ with $H$ normal, Proposition~8.11 ensures that $HK$ is a subgroup of $G$, $H$ is normal in $HK$, and $H \intersect K$ is a normal subgroup of $K$. The relevant sublattice of the lattice $\frakL(G)$ of subgroups of $G$ is seen in \cref{fig:lattice-of-subgroups}.
    
    \begin{figure}[!h]
        \centering
        \begin{tikzpicture}[scale=0.5]
            \node (G) at (0,4) {$G$};
            \node (HK) at (0,2) {$HK$};
            \node (H) at (-2,0) {$H$};
            \node (K) at (2,0) {$K$};
            \node (HinterK) at (0,-2) {$H \intersect K$};
            \node (e) at (0,-4) {$\{e\}$};
            \draw (G) -- (HK);
            \draw (HK) -- (K) -- (HinterK) -- (H) -- (HK);
            \draw (HinterK) -- (e);
        \end{tikzpicture}
        \caption{Sublattice of $\frakL(G)$.}
        \label{fig:lattice-of-subgroups}
    \end{figure}

    In this lattice we see that $HK = H \join K$ and $H \intersect K = H \meet K$. The latter is obvious, and the former follows since every subgroup containing $H$ and $K$ must contain all elements on the form $hk$ for $h \in H$ and $k \in K$. Recall that for positive integers $a,b$ we have
    %
    \begin{equation*}
        \frac{ \lcm(a,b) }{a}
            = \frac{b}{ \gcd(a,b) },
    \end{equation*}
    %
    and that $\lcm(a,b) = a \join b$ and $\gcd(a,b) = a \meet b$ in the lattice $\naturals$ ordered by divisibility. Thus we might expect that, in the lattice of subgroups of $G$, we would have
    %
    \begin{equation*}
        \frac{HK}{H}
            \cong \frac{K}{H \intersect K}.
    \end{equation*}
    %
    But this is precisely the content of the second isomorphism theorem.

    However, while $a$ and $b$ appear symmetrically, $H$ and $K$ do not, since only $H$ is assumed normal. But notice that in $\catAb$ they do. Also notice that, since $H$ is normal,
    %
    \begin{equation*}
        HK
            = \bigunion_{k \in K} Hk
            = \bigunion_{k \in K} kH
            = KH.
    \end{equation*}
    %
    In fact, this evidently holds whenever $K$ is \emph{any} subset of $G$.
\end{remarkbreak}




\begin{remarkbreak}[Modular lattices]
    Let $(\frakL,\join,\meet)$ be a lattice. An element $x \in \frakL$ is said to be \emph{modular} if\footnote{There are a couple of different definitions of modularity of \emph{elements}, but these are all equivalent for modularity of \emph{lattices}. Some authors use only one of the following two properties in the definition.}
    %
    \begin{enumerate}[label=(\arabic*)]
        \item $a \join (x \meet c) = (a \join x) \meet c$ for all $a,c \in \frakL$ with $a \leq c$, and

        \item $x \join (b \meet c) = (x \join b) \meet c$ for all $b,c \in \frakL$ with $x \leq c$.
    \end{enumerate}
    %
    Furthermore, $\frakL$ itself is said to be modular if every element of $\frakL$ is modular. If $G$ is a group, then a subgroup $H$ of $G$ is said to be modular if it is a modular element of the lattice $\mathfrak{L}(G)$ of subgroups of $G$.

    We first prove the \emph{modular property of groups}: If $H,K,L$ are subgroups of $G$ with $H \leq L$, then $HK \intersect L = H (K \intersect L)$. The inclusion \enquote{$\supseteq$} is clear since
    %
    \begin{equation*}
        H (K \intersect L)
            \subseteq HK \intersect HL
            = HK \intersect L.
    \end{equation*}
    %
    For the other inclusion, let $h \in H$ and $k \in K$ be such that $hk \in L$. Then since also $h \in L$ we have
    %
    \begin{equation*}
        k
            = h\inv hk
            \in L,
    \end{equation*}
    %
    so $k \in K \intersect L$. In total, $hk \in H(K \intersect L)$.

    A subgroup $H$ of $G$ is said to be \emph{permutable} (or \emph{quasinormal}) in $G$ if it commutes with every subgroup $K$ of $G$, i.e. $HK = KH$. Every normal subgroup of $G$ is thus permutable in $G$. We claim that $H$ is permutable in $G$ only if $H$ is modular. First notice that $HK = H \join K$ since $H$ and $K$ commute. Since also $H \intersect K = H \meet K$, the modularity property immediately implies the second of the two criteria. To prove the first criterion, it suffices to show that $H$ and $K \intersect L$ also commute if $H$ and $K$ do. If $h \in H$ and $k \in K \intersect L$, then since $H$ and $K$ commute there exist $h' \in H$ and $k' \in K$ such that $hk = k'h'$. But then notice that
    %
    \begin{equation*}
        k'
            = (hk)(h')\inv,
    \end{equation*}
    %
    which lies in $L$. Hence $H(K \intersect L) \subseteq (K \intersect L)H$, and the opposite inclusion follows similarly.

    Next we claim that the set $\frakN(G)$ of normal subgroups of $G$ forms a modular sublattice of $\frakL(G)$. If $M$ and $N$ are normal subgroups of $G$, then $MN$ is also normal, since for $g \in G$ we have
    %
    \begin{equation*}
        g(MN)
            = (gM)N
            = (Mg)N
            = M(gN)
            = M(Ng)
            = (MN)g.
    \end{equation*}
    %
    Furthermore, the subgroup $M \intersect N$ is also normal, since
    %
    \begin{equation*}
        g (M \intersect N)
            = gM \intersect gN
            = Mg \intersect Ng
            = (M \intersect N) g.
    \end{equation*}
    %
    Hence $\frakN(G)$ is indeed a sublattice. We showed above that every permutable (hence every normal) subgroup is modular, so $\frakN(G)$ is modular.

    Finally we provide some intuition for the definition of modularity in lattices. If $\frakL$ is a lattice and $a,x,b \in \frakL$ with $a \leq b$, we can \enquote{project} $x$ onto the up set $\upset a = \set{y \in \frakL}{a \leq y}$ by joining it with $a$, i.e. $a \join x$. Afterwards we may further project it onto the down set $\downset b = \set{y \in \frakL}{y \leq b}$ by meeting it with $b$, i.e. $(a \join x) \meet b$. Notice that since $a \leq b$, we still have $a \leq (a \join x) \meet b$. Hence we obtain a projection of $x$ onto the interval
    %
    \begin{equation*}
        [a,b]
            = (\upset a) \intersect (\downset b)
            = \set{y \in \frakL}{a \leq y \leq b}.
    \end{equation*}
    %
    Dually we might have instead used the projection $a \join (x \meet b)$, i.e. reversing the order of the two projections. A lattice is modular just when these two projections always give the same result, so in some sense projection onto an interval only really makes sense in a modular lattice.
\end{remarkbreak}


\begin{remarkbreak}[A \enquote{rank-nullity theorem} for finite groups]
    Let $\phi \colon G \to H$ be a group homomorphism, where $G$ is a finite group. This induces an isomorphism $\tilde\phi \colon G/\ker\phi \to \phi(G)$. Hence
    %
    \begin{equation*}
        \card{G}
            = \card{\ker\phi} \, [G : \ker\phi]
            = \card{\ker\phi} \, \card{\phi(G)}.
    \end{equation*}
    %
    In particular we see that the order of the image of $\phi$, which is a subgroup of the \emph{codomain}, divides the order of the \emph{domain}. Compare the rank-nullity theorem from linear algebra:
    %
    \begin{equation*}
        \dim V
            = \dim \ker T + \dim T(V),
    \end{equation*}
    %
    where $T \colon V \to W$ is a linear map.
\end{remarkbreak}


\begin{exerciseframed}[2.8.1]
    If a group $H$ may be realised as a subgroup of two groups $G_1$ and $G_2$, and if
    %
    \begin{equation*}
        \frac{G_1}{H}
            \cong \frac{G_2}{H},
    \end{equation*}
    %
    does it follow that $G_1 \cong G_2$?
\end{exerciseframed}

\begin{solution}
    It does not. Notice that
    %
    \begin{equation*}
        \frac{C_4}{C_2}
            \cong C_2
            \cong \frac{C_2 \prod C_2}{C_2},
    \end{equation*}
    %
    but that $C_4 \not\cong C_2 \prod C_2$.
\end{solution}


\begin{exerciseframed}[2.8.3]
    Prove that every finite group is finitely presented.
\end{exerciseframed}

\begin{solution}
    Let $G$ be a finite group and consider the free group $F(G)$ on the underlying set of $G$. If $n = \card{G}$ we denote the $n$ distinct elements of $G$ by $g_1, \ldots, g_n$. For $1 \leq i, j \leq n$ we let $g_{ij} = g_i g_j$. Let $\calR$ be the set of words in $F(G)$ on the form $g_i g_j g_{ij}\inv$, and let $R$ be the normal subgroup of $F(G)$ generated by $\calR$.

    By the univeral property of free groups, the identity map $\iota \colon G \to G$ from the set $G$ to the group $G$ induces a surjective homomorphism $\rho \colon F(G) \to G$. We claim that $\ker\rho = R$. We clearly have $R \subseteq \ker\rho$, so let $h_1 \cdots h_k \in \ker\rho$. By repeatedly applying the relations in $\calR$ we may reduce the length of this word and obtain a word $h \in \ker\rho$ of length one. But then $h$ is an element of the underlying set of $G$, and $h = \iota(h) = \rho(h) = e_G$. This lies in $R$, and applying the above sequence of relations from $\calR$ in reverse order we recover the word $h_1 \cdots h_k$, staying inside of $R$. Thus $\ker\rho \subseteq R$.

    Finally, the first isomorphism theorem implies that $F(G)/R \cong G$, and both $G$ and $\calR$ are finite, so this proves the claim.
\end{solution}



\begin{exerciseframed}[2.8.8]
    Prove that $\SL{n}{\reals}$ is a \emph{normal subgroup} of $\GL{n}{\reals}$ and \enquote{compute}
    %
    \begin{equation*}
        \frac{ \GL{n}{\reals} }{ \SL{n}{\reals} }
    \end{equation*}
    %
    as a well-known group.
\end{exerciseframed}

\begin{solution}
    Consider the determinant
    %
    \begin{equation*}
        \det \colon \GL{n}{\reals} \to \reals^*.
    \end{equation*}
    %
    This is a surjective homomorphism with kernel $\SL{n}{\reals}$, and the first isomorphism theorem implies that $\GL{n}{\reals}/\SL{n}{\reals} \cong \reals^*$.
\end{solution}


\begin{exerciseframed}[2.8.13]
    Let $G$ be a finite commutative group, and assume $\card{G}$ is odd. Prove that every element of $G$ is a square.
\end{exerciseframed}
%
Exercise~8.14 is solved by the same argument.

\begin{solution}
    We show that the map $g \mapsto g^2$ is surjective, and since $G$ is finite it suffices to show that it is injective. For $g,h \in G$ with $g^2 = h^2$ we have $(g\inv h)^2 = e$. But the order of $g\inv h$ cannot be even since $\card{G}$ is odd, so $g\inv h = e$, i.e. $g = h$.
\end{solution}


\begin{exerciseframed}[2.8.16]
    Generalize Fermat's little theorem to congruences modulo arbitrary (that is, possibly nonprime) integers.
\end{exerciseframed}

\begin{solution}
    We prove \emph{Euler's theorem}:
    %
    \begin{displaytheorem}
        If $n$ and $a$ are coprime positive integers, then
        %
        \begin{equation*}
            a^{\phi(n)} \equiv 1 \pmod n.
        \end{equation*}
    \end{displaytheorem}
    %
    The class $[a]_n$ lies in the multiplicative group $(\ints/n\ints)^*$ which has order $\phi(n)$, so Lagrange's theorem implies that
    %
    \begin{equation*}
        [a]_n^{\phi(n)}
            = [1]_n
    \end{equation*}
    %
    as claimed.

    We also have the following corollary:
    %
    \begin{displaytheorem}
        If $n$ is a product of distinct primes and $a,v \in \ints$, then
        %
        \begin{equation}
            \label{eq:Eulers-theorem-corollary}
            a^{1+\phi(n)v} \equiv a \pmod n.
        \end{equation}
    \end{displaytheorem}
    %
    If $n = p$ is a prime and $p \divides a$, then both sides are $0 \mod p$. If instead $a$ and $p$ are coprime, then the claim follows from Euler's theorem (or Fermat's little theorem).

    Next, if $m$ and $n$ are coprime and \cref{eq:Eulers-theorem-corollary} holds as stated as well as with $n$ replaced by $m$, then by \exref{5.6.8} we have
    %
    \begin{equation*}
        a^{1+\phi(mn)v}
            = a^{1+\phi(m)\phi(n)v}
            \equiv a \pmod m,
    \end{equation*}
    %
    and similarly mod $n$. Since $m$ and $n$ are coprime this also holds mod $mn$. The claim now follows by induction.
\end{solution}


\begin{exerciseframed}[2.8.17]
    Assume $G$ is a finite abelian group, and let $p$ be a prime divisor of $\card{G}$. Prove that there exists an element in $G$ of order $p$.
\end{exerciseframed}

\begin{solution}
    We argue by induction in $\card{G}$. If $\card{G} = 1$ then $\card{G}$ has no prime divisors, hence the claim is trivial. Assume then that $\card{G} > 1$, and let $g \neq e$ be an element of $G$. We claim that the cyclic group $\gen{g}$ contains an element of prime order. Let $q$ be a prime divisor of $\ord{g}$ and write $\ord{g} = qd$. The element $g^d$ then has order
    %
    \begin{equation*}
        \ord{g^d}
            = \frac{\lcm(\ord{g},d)}{d}
            = \frac{\ord{g}}{d}
            = q
    \end{equation*}
    %
    by Proposition~1.13 as claimed. If $q = p$ then we are done. If $q \neq p$ then let $H = \gen{g^d}$ and consider the quotient\footnote{Here we use that $G$ is abelian, since then $\gen{g^d}$ is a normal subgroup.} $G/H$. This has order $\card{G}/q < \card{G}$, and since $p$ divides $\card{G}/q$ it contains an element $aH$ of order $p$ by induction. By Proposition~4.1, $p = \ord{aH}$ divides $\ord{a}$, so write $\ord{a} = pk$. Then $a^k$ has order $p$ as desired.
\end{solution}


\begin{exerciseframed}[2.8.19]
    Let $G$ be a finite group, and let $d$ be a proper divisor of $\card{G}$. Is it necessarily true that there exists an element of $G$ of order $d$? Give a proof or a counterexample.
\end{exerciseframed}

\begin{solution}
    The (abelian!) group $C_2 \times C_2 \times C_2$ is of order $8$, but all its nontrivial elements are of order $2$.
\end{solution}


\begin{exerciseframed}[2.8.20]
    Assume $G$ is a finite abelian group, and let $d$ be a divisor of $\card{G}$. Prove that there exists a subgroup $H \subseteq G$ of order $d$.
\end{exerciseframed}

\begin{solution}
    We argue by induction in $\card{G}$. If $\card{G} = 1$ then this is obvious, so assume that $\card{G} > 1$ and let $d$ be a divisor of $\card{G}$. If $d$ is prime, then the claim follows directly from \exref{2.8.17}, so assume that $d$ is composite and let $d = kp$ with $p$ prime. Then there is an element $h \in G$ of order $p$ by \exref{2.8.17}, and the quotient $G/\gen{h}$ has order $k < \card{G}$. Induction yields a subgroup of $G/\gen{h}$ of order $k$, and by Proposition~8.9 this subgroup is on the form $H/\gen{h}$ for some subgroup $H$ of $G$. Finally notice that
    %
    \begin{equation*}
        \card{H}
            = [H:\gen{h}] \, \ord{h}
            = kp
            = d
    \end{equation*}
    %
    as desired.
\end{solution}


\begin{exerciseframed}[2.8.22]
    Let $\phi \colon G \to G'$ be a group homomorphism, and let $N$ be the smallest normal subgroup containing $\im\phi$. Prove that $G'/N$ satisfies the universal property of $\coker\phi$ in $\catGrp$.
\end{exerciseframed}

\begin{solution}
    First we rephrase the universal property of $\coker\phi$. If $0 \colon G \to G'$ is the trivial map, $\coker\phi$ is the coequaliser of $\phi$ and $0$. That is, given a homomorphism $\alpha \colon G' \to L$ such that $\alpha \circ \phi = \alpha \circ 0$ there is a unique homomorphism $\tilde\alpha \colon \coker\phi \to L$ such that the diagram
    %
    \begin{equation*}
        \begin{tikzcd}[row sep=tiny]
            && L \\
            G
                \ar[r, "\phi", shift left]
                \ar[r, "0", shift right, swap]
            & G'
                \ar[ur, "\alpha"]
                \ar[dr, "\pi", swap, twoheadrightarrow] \\
            && \coker\phi
                \ar[uu, "\tilde\alpha", swap, dashed]
        \end{tikzcd}
    \end{equation*}
    %
    commutes. The condition $\alpha \circ \phi = \alpha \circ 0$ implies that $\im\phi \subseteq \ker\alpha$, and since $\ker\alpha$ is normal it follows that $N \subseteq \ker\alpha$. Theorem~7.12 yields a unique homomorphism $\tilde\alpha \colon G'/N \to L$ such that $\tilde\alpha \circ \pi = \alpha$. Thus $G'/N \cong \coker\phi$.

    Also notice that the above works in $\catAb$, only here $\im\phi = N$.
\end{solution}


\begin{exerciseframed}[2.8.23]
    Consider the subgroup $H = \{e, (1\;2)\}$ of $S_3$. Show that the cokernel of the inclusion $\iota \colon H \hookrightarrow S_3$ is trivial, although $\iota$ is not surjective
\end{exerciseframed}

\begin{solution}
    In accordance with \exref{2.8.22} we compute the smallest normal subgroup $N$ of $S_3$ containing $\im\iota = H$. The only nontrivial proper normal subgroup of $S_3$ is $A_3$, but $(1\;2) \not\in A_3$. Hence $N = S_3$, so $\coker\iota \cong S_3/N$ is trivial.
\end{solution}


\begin{exerciseframed}[2.8.24]
    Show that epimorphisms in $\catGrp$ do not necessarily have right-inverses.
\end{exerciseframed}

\begin{solution}
    Consider the homomorphism $\pi_2 \colon \ints \to \ints/2\ints$ given by $\pi_2(n) = [n]_2$. This is surjective hence an epimorphism, but the only homomorphism $\ints/2\ints \to \ints$ is the trivial one.
\end{solution}


\section{Group actions}

\begin{remark}
    We elaborate on the orbit-stabiliser theorem (Proposition~9.9). Let $G$ be a group with a transitive left-action on a set $A$, and fix an element $a \in A$. We define an equivalence relation on $G$ by letting $g_1 \sim g_2$ if and only if $g_1 a = g_2 a$. This is the case just when $g_1\inv g_2 a = a$, i.e. when $g_1\inv g_2 \in \stab_G(a)$. Since $e_G \in \stab_G(a)$ we see that $\sim$ agrees with the equivalence relation induced by $\stab_G(a)$ as a subgroup of $G$.

    Next notice that the map $\phi \colon G \to A$ given by $\phi(g) = ga$ has the property that $\phi(g_1) = \phi(g_2)$ if and only if $g_1 \sim g_2$. Furthermore, since $G$ acts transitively on $A$, $\phi$ is also surjective. Thus $\phi$ induces a bijection $\tilde\phi \colon G/{\sim} \to A$.

    Letting $H = \stab_G(a)$, since we have $G/{\sim} = G/H$ we let $G$ act on $G/H$ by left-multiplication. We easily see that $\tilde\phi$ is equivariant:
    %
    \begin{equation*}
        \phi(g'(gH))
            = \phi(g'gH)
            = (g'g)a
            = g'(ga)
            = g' \phi(gH)
    \end{equation*}
    %
    for $g,g' \in G$.

    We can understand the theorem more informally as follows: Fixing an element of $a \in A$ introduces a sort of \enquote{origin} in $A$. Since $A$ is just a set, the choice of origin is arbitrary. Next we group the elements of $G$ based on where they send $a$ when acting on $A$. It turns out that two elements send $a$ to the same point if and only if they lie in the same $\stab_G(a)$-coset. But this makes sense, since quotienting out by $\stab_G(a)$ means that we force every element in $G$ that fixes $a$ to \enquote{do nothing}. So if $g_1H = g_2H$ with $H = \stab_G(a)$, then this means that $g_1$ and $g_2$ are the same up to \enquote{doing nothing}.

    Finally, the equivariance of $\tilde\phi$: Left-multiplication in $G/H$ is basically just composition of transformations, since $g'(gH) = (g'g)H = (g'H)(gH)$. And because composition of functions in general is defined pointwise, it makes sense that the same should be true in this case.
\end{remark}


\begin{exerciseframed}[2.9.7]
    Prove that stabilisers are indeed subgroups.
\end{exerciseframed}

\begin{solution}
    Let $G$ be a group acting on a set $A$, and let $a \in A$. Clearly $e_G \in \stab_G(a)$, and if $g,h \in \stab_G(a)$ then also $gh \in \stab_G(a)$. Finally we also have
    %
    \begin{equation*}
        g\inv a
            = g\inv (ga)
            = (g\inv g)a
            = a,
    \end{equation*}
    %
    so $g\inv \in \stab_G(a)$.
\end{solution}



\begin{exerciseframed}[2.9.8]
    For $G$ a group, verify that $\catGSet$ is indeed a category, and verify that the isomorphisms in $\catGSet$ are precisely the equivariant bijections.
\end{exerciseframed}

\begin{solution}
    Let $\phi \colon (\rho, A) \to (\sigma, B)$ and $\psi \colon (\sigma, B) \to (\tau, C)$ be equivariant maps. For $g \in G$ and $a \in A$ we have
    %
    \begin{equation*}
        (\psi \circ \phi)(ga)
            = \psi(\phi(ga))
            = \psi(g \phi(a))
            = g (\psi \circ \phi)(a),
    \end{equation*}
    %
    so $\psi \circ \phi$ is also equivariant. The identity map on a set is clearly also equivariant, so $\catGSet$ is indeed a category.

    Now assume that the set function $\phi$ is bijective and consider its inverse $\phi\inv$. Let $b \in B$ and put $a = \phi\inv(b)$. Since $\phi(ga) = g \phi(a)$, applying $\phi\inv$ to both sides yields
    %
    \begin{equation*}
        g \phi\inv(b)
            = ga
            = \phi\inv(g \phi(a))
            = \phi\inv(gb),
    \end{equation*}
    %
    so $\phi\inv$ is equivariant. It is already the inverse of $\phi$ in $\catSet$, so it is also the inverse of $\phi$ in $\catGSet$.
\end{solution}



\begin{exerciseframed}[2.9.9]
    Prove that $\catGSet$ has products and coproducts and that every finite object of $\catGSet$ is a coproduct of objects of the type $G/H$, where $H$ is a subgroup of $G$ and $G$ acts on $G/H$ by left-multiplication.
\end{exerciseframed}

\begin{solution}
\begin{proofsec}
    \item[Products]
    Let $A$ and $A'$ be sets, and let $\sigma \colon G \to \Aut_\catSet(A)$ and $\sigma' \colon G \to \Aut_\catSet(A')$ be actions of $G$ on $A$ and $A'$. These induce a homomorphism
    %
    \begin{equation*}
        \inner{\sigma}{\sigma'} \colon G
            \to \Aut_\catSet(A) \prod \Aut_\catSet(A')
            \subseteq \Aut_\catSet(A \prod A').
    \end{equation*}
    %
    The inclusion is understood as follows: A pair of maps $\phi \in \Aut_\catSet(A)$ and $\phi' \in \Aut_\catSet(A')$ determine a map $\phi \prod \phi' \in \Aut_\catSet(A \prod A')$ given by
    %
    \begin{equation*}
        (\phi \prod \phi')(a,a')
            = (\phi(a), \phi'(a')).
    \end{equation*}
    %
    This is clearly also an automorphism. Thus $\inner{\sigma}{\sigma'}$ is an action of $G$ on $A \prod A'$. For $g \in G$ we thus have $\inner{\sigma}{\sigma'}(g) = \sigma(g) \prod \sigma'(g)$, so $a \in A$ and $a' \in A'$ this is given explicitly by
    %
    \begin{equation*}
        \inner{\sigma}{\sigma'}(g)(a,a')
            = \bigl( \sigma(g)(a), \sigma'(g)(a') \bigr),
    \end{equation*}
    %
    or more simply by
    %
    \begin{equation*}
        g(a,a')
            = (ga, ga').
    \end{equation*}

    Let $Z$ be another object in $\catGSet$, and let $\phi \colon Z \to A$ and $\phi' \colon Z \to A'$ be equivariant maps. There is then a unique set map $\psi \colon Z \to A \prod A'$ such that the diagram
    %
    \begin{equation*}
        \begin{tikzcd}
            && A \\
            Z
                \ar[r, "\psi", dashed]
                \ar[urr, "\phi", bend left]
                \ar[drr, "\phi'", bend right, swap]
            & A \prod A'
                \ar[ur, "\pi_A", swap]
                \ar[dr, "\pi_{A'}"] \\
            && A'
        \end{tikzcd}
    \end{equation*}
    %
    commutes. It thus suffices to show that $\psi$, $\pi_A$ and $\pi_{A'}$ are equivariant. For $\pi_A$ we have
    %
    \begin{equation*}
        \pi_A(g(a,a'))
            = \pi_A(ga,ga')
            = ga
            = g \pi_A(a,a'),
    \end{equation*}
    %
    and for $\psi$,
    %
    \begin{equation*}
        \psi(gz)
            = (\phi(gz), \phi'(gz))
            = (g\phi(z), g\phi'(z))
            = g(\phi(z), \phi'(z))
            = g \psi(z).
    \end{equation*}
    %
    Thus $A \times A'$ equipped with the action $\inner{\sigma}{\sigma'}$ is a product of $A$ and $A'$ in $\catGSet$ as claimed.

    \item[Coproducts]
    For $g \in G$ we have automorphisms $\sigma(g) \colon A \to A$ and $\sigma'(g) \colon A' \to A'$. These induce an automorphism
    %
    \begin{equation*}
        \sigma(g) \oplus \sigma'(g) \colon
            A \coprod A' \to A \coprod A'
    \end{equation*}
    %
    given by $a \mapsto \sigma(g)(a)$ if $a \in A$, and $a \mapsto \sigma'(g)(a)$ if $a \in A'$. This in turn gives rise to a map $\sigma \oplus \sigma' \colon G \to \Aut_\catSet(A \coprod A')$ given by $(\sigma \oplus \sigma')(g) = \sigma(g) \oplus \sigma'(g)$, and we claim that this is an action of $G$ on $A \coprod A'$. Let $g,h \in G$, and assume that $a \in A$. Then
    %
    \begin{align*}
        (\sigma \oplus \sigma')(gh)(a)
            &= \bigl( \sigma(gh) \oplus \sigma'(gh) \bigr)(a)
             = \sigma(gh)(a)
             = \sigma(g) \circ \sigma(h) (a) \\
            &= \bigl( \sigma(g) \oplus \sigma'(g) \bigr) \circ \bigl( \sigma(h) \oplus \sigma'(h) \bigr) (a) \\
            &= (\sigma \oplus \sigma')(g) \circ (\sigma \oplus \sigma')(h) (a),
    \end{align*}
    %
    and similarly if $a \in A'$. Thus
    %
    \begin{equation*}
        (\sigma \oplus \sigma')(gh)
            = (\sigma \oplus \sigma')(g) \circ (\sigma \oplus \sigma')(h),
    \end{equation*}
    %
    so $\sigma \oplus \sigma'$ is a group homomorphism, hence an action of $G$ on $A \coprod A'$.

    Now let $W$ be another object in $\catGSet$, and let $\phi \colon A \to W$ and $\phi' \colon A' \to W$ be equivariant maps. There is then a unique set map $\chi \colon A \coprod A' \to W$ such that the diagram
    %
    \begin{equation*}
        \begin{tikzcd}
            A
                \ar[dr, "i_A"]
                \ar[drr, "\phi", bend left] \\
            & A \coprod A'
                \ar[r, "\chi", dashed]
            & W \\
            A
                \ar[ur, "i_{A'}", swap]
                \ar[urr, "\phi'", bend right, swap]
        \end{tikzcd}
    \end{equation*}
    %
    commutes. It thus suffices to show that $\chi$, $i_A$ and $i_{A'}$ are equivariant. For $i_A$ we simply have $i_A(ga) = ga = g i_A(a)$ for $a \in A$. For $\chi$ we have
    %
    \begin{equation*}
        \chi(ga)
            = \phi(ga)
            = g \phi(a)
            = g \chi(a)
    \end{equation*}
    %
    if $a \in A$, and similarly if $a \in A'$.

    \item[Finite objects]
    Let $A$ be a finite set equipped with an action $\sigma$, and let $\Omega$ be the set of distinct orbits of elements in $A$ under $\sigma$, and let $\sigma_\omega$ denote the restriction of $\sigma$ to $\omega \in \Omega$. Since $A$ is finite so is $\Omega$, and it is obvious that
    %
    \begin{equation*}
        \bigoplus_{\omega \in \Omega} \sigma_\omega
            = \sigma.
    \end{equation*}
    %
    Furthermore, every $\sigma_\omega$ is transitive, so Proposition~9.9 yields isomorphisms $\omega \cong G/H_\omega$ in $\catGSet$, where $H_\omega$ is the stabiliser of some element of $\omega$. It follows that
    %
    \begin{equation*}
        A
            \cong \bigcoprod_{\omega \in \Omega} \omega
            \cong \bigcoprod_{\omega \in \Omega} G/H_\omega.
    \end{equation*}
    %
    This proves the claim.
\end{proofsec}
\end{solution}


\chapter{Rings and modules}

\section{Definition of ring}

\begin{remark}
    Let $(R,+,\cdot)$ be a ring. The definition includes the hypothesis that $(R,+)$ be an \emph{abelian} group, but if $R$ has a multiplicative identity this is actually superfluous. The distributive laws imply that
    %
    \begin{equation*}
        (1+1)(r+s)
            = 1(r+s) + 1(r+s)
            = r + s + r + s,
    \end{equation*}
    %
    and that
    %
    \begin{equation*}
        (1+1)(r+s)
            = (1+1)r + (1+1)s
            = r + r + s + s,
    \end{equation*}
    %
    and then cancellation implies that $r + s = s + r$.
\end{remark}


\begin{exerciseframed}[3.1.5]
    Let $R$ be a ring. If $a,b$ are zero-divisors in $R$, is $a+b$ necessarily a zero-divisor?
\end{exerciseframed}

\begin{solution}
    We give a counterexample. Let
    %
    \begin{equation*}
        a =
        \begin{pmatrix}
            1 & 0 \\
            0 & 0
        \end{pmatrix}
        \quad \text{and} \quad
        b =
        \begin{pmatrix}
            0 & 0 \\
            0 & 1
        \end{pmatrix}
    \end{equation*}
    %
    in the matrix ring $\calM_n(\reals)$. Then $ab = ba = 0$, so both $a$ and $b$ are zero-divisors, but $a + b$ is the identity matrix.

    However, if $R$ is commutative, then $a + b$ is in fact a zero-divisor: For if $ac = 0$ and $bd = 0$ for some $c,d \in R$, then
    %
    \begin{equation*}
        (a+b)cd
            = acd + bdc
            = 0 \cdot d + 0 \cdot c
            = 0.
    \end{equation*}
\end{solution}


\begin{exerciseframed}[3.1.6]
    An element $a$ of a ring $R$ is \emph{nilpotent} if $a^n = 0$ for some $n$.
    %
    \begin{enumerate}
        \item Prove that if $a$ and $b$ are nilpotent in $R$ and $ab = ba$, then $a + b$ is also nilpotent.
        \item Is the hypothesis $ab = ba$ in the previous statement necessary for its conclusion to hold?
    \end{enumerate}
\end{exerciseframed}

\begin{solution}
\begin{solutionsec}
    \item Since $ab = ba$, the binomial theorem implies that
    %
    \begin{equation*}
        (a + b)^N
            = \sum_{i=0}^{N} \binom{N}{i} a^i b^{N-i}.
    \end{equation*}
    %
    Choose $m,n \in \naturals$ such that $a^m = 0$ and $b^n = 0$, and let $N = m+n$. Then if $i < m$ we have $N - i \geq n$ (i.e., each term in the sum contains either a large power of $a$ or of $b$), so it follows that $(a + b)^N = 0$.

    \item It is necessary. Consider the matrices
    %
    \begin{equation*}
        a =
        \begin{pmatrix}
            0 & 1 \\
            0 & 0
        \end{pmatrix}
        \quad \text{and} \quad
        b =
        \begin{pmatrix}
            0 & 0 \\
            1 & 0
        \end{pmatrix}
    \end{equation*}
    %
    in $\calM_2(\reals)$. Then $a^2 = b^2 = 0$, but $a+b$ is invertible and hence not nilpotent: indeed, $(a+b)^2$ is the identity matrix.
\end{solutionsec}
\end{solution}


\begin{exerciseframed}[3.1.17]
    Explain in what sense $R[x]$ agrees with the monoid ring $R[\naturals]$.
\end{exerciseframed}

\begin{solution}
    As a group, $R[\naturals]$ is just the direct sum $R^{\oplus\naturals}$ of the abelian groups $(R, +)$, which agrees with the definition of $R[x]$ as a group. As for the multiplication in $R[\naturals]$, this is given by
    %
    \begin{equation*}
        \biggl( \sum_{n \in \naturals} a_n \cdot n \biggr)
            \cdot \biggl( \sum_{n \in \naturals} b_n \cdot n \biggr)
            = \sum_{n \in \naturals} \sum_{n = n_1 + n_2} (a_{n_1} b_{n_2}) \cdot n.
    \end{equation*}
    %
    The multiplication in $R[x]$ is given by
    %
    \begin{equation*}
        \biggl( \sum_{n \in \naturals} a_n x^n \biggr)
            \cdot \biggl( \sum_{n \in \naturals} b_n x^n \biggr)
            = \sum_{n \in \naturals} \sum_{n = n_1 + n_2} (a_{n_1} b_{n_2}) x^n,
    \end{equation*}
    %
    which agrees with the multiplication on $R[\naturals]$ after substituting $n \to x^n$.
\end{solution}


\section[The category Ring][The category $\catRing$]{The category $\catRing$}

\begin{remark*}
    Let $S$ be a ring and $s \in S$. According to Example~2.2 there exists a unique ring homomorphism $\phi \colon \ints[x] \to S$ that sends $x$ to $s$. This is guaranteed by the proof of Proposition~2.1 as long as $s$ commutes with every element in $\iota(\ints)$, where $\iota \colon \ints \to S$ is the unique ring homomorphism. We claim that this is in fact the case:
    
    The \emph{set function} $\phi$ always exists. In order for it to be a ring homomorphism, notice that $s$ indeed needs only commute with $\iota(\ints)$, since the elements in $\phi(\ints[x])$ are on the form
    %
    \begin{equation*}
        \phi \biggl( \sum_{n \in \naturals} a_n x^n \biggr)
            = \sum_{n \in \naturals} \phi(a_n) \phi(x)^n
            = \sum_{n \in \naturals} \iota(a_n) s^n
    \end{equation*}
    %
    for $a_n \in \ints$. Clearly $s$ commutes with $\iota(1) = 1_S$, so assume that it commutes with $\iota(n)$ for some $n \in \naturals$. It follows that
    %
    \begin{equation*}
        s \iota(n+1)
            = s(\iota(n) + 1_S)
            = (\iota(n) + 1_S)s
            = \iota(n+1)s.
    \end{equation*}
    %
    The extension to $n \in \ints$ is obvious.

    We can also give a more structural argument for this final claim based on \cref{rem:generating-subalgebra}: As mentioned, $s$ commutes with $\iota(1)$. The set of elements of $S$ with which $s$ commute (i.e. the centraliser of $s$, cf. \exref{3.2.10}) is a subring containing $\iota(1)$, hence it contains $\gen{\iota(1)} = \iota(\gen{1}) = \iota(\ints)$ as desired.
\end{remark*}


\begin{remarkbreak}[Naturality of $\ints$]
    The \emph{group} $\ints$ is first constructured as the free group on one generator. It turns out that this is abelian, so we may consider the set $\End_\catAb(\ints)$. If $\phi,\psi \colon \ints \to \ints$ are group homomorphisms, we may define $\phi + \psi$ pointwise by
    %
    \begin{equation*}
        (\phi + \psi)(n) = \phi(n) + \psi(n),
    \end{equation*}
    %
    which makes $\End_\catAb(\ints)$ itself into an abelian group. Composition of elements in $\End_\catAb(\ints)$ makes it into a monoid. To show that it is a ring, we check the distributive laws: First of all,
    %
    \begin{equation*}
        (\phi + \psi) \circ \chi(n)
            = (\phi + \psi)(\chi(n))
            = \phi(\chi(n)) + \psi(\chi(n))
            = \phi \circ \chi(n) + \psi \circ \chi(n).
    \end{equation*}
    %
    Notice that we haven't used that the maps are group homomorphisms. Furthermore,
    %
    \begin{equation*}
        \chi \circ (\phi + \psi)(n)
            = \chi( \phi(n) + \psi(n) )
            = \chi(\phi(n)) + \chi(\psi(n))
            = \chi \circ \phi(n) + \chi \circ \psi(n),
    \end{equation*}
    %
    where the second equality uses that $\chi$ is a homomorphism. Hence $\End_\catAb(\ints)$ is a ring. Finally, Proposition~2.6 shows that $\End_\catAb(\ints)$ is isomorphic to $\ints$ equipped with the usual ring structure, showing that $\End_\catAb(\ints)$ is a commutative ring. But notice that we only use this isomorphism to show that $\End_\catAb(\ints)$ is commutative; its definition is entirely \enquote{natural} and does not depend on the integers as such.

    We may prove that $\End_\catAb(\ints) \cong \ints$ as follows: For $a \in \ints$ define $\alpha_a \colon \ints \to \ints$ by $\alpha_a(n) = an$. This is clearly a group homomorhism. Then define $\psi \colon \ints \to \End_\catAb(\ints)$ by $\psi(a) = \alpha_a$. Notice that
    %
    \begin{equation*}
        (\alpha_a + \alpha_b)(n)
            = \alpha_a(n) + \alpha_b(n)
            = an + bn
            = (a+b)n
            = \alpha_{a+b}(n)
    \end{equation*}
    %
    and
    %
    \begin{equation*}
        (\alpha_a \circ \alpha_b)(n)
            = \alpha_a (\alpha_b(n))
            = a(bn)
            = (ab)n
            = \alpha_{ab}(n),
    \end{equation*}
    %
    showing that $\psi$ is a ring homomorphism. Then define a map $\phi \colon \Hom_\catAb(\ints) \to \ints$ by $\phi(\alpha) = \alpha(1)$. Notice that
    %
    \begin{equation*}
        (\phi \circ \psi)(a)
            = \phi(\alpha_a)
            = \alpha_a(1)
            = a,
    \end{equation*}
    %
    and that
    %
    \begin{equation*}
        (\psi \circ \phi)(\alpha)
            = \psi(\alpha(1))
            = \alpha_{\alpha(1)},
    \end{equation*}
    %
    and furthermore that $\alpha_{\alpha(1)}(n) = \alpha(1)n = \alpha(n)$ since $\alpha$ is a group homomorphism. Thus $\phi$ and $\psi$ are inverses.

    Aluffi instead proves that $\phi$ is a ring homomorphism, but it seems more natural to prove that $\psi$ is instead. However, Proposition~2.7 is a generalisation of the above whose proof is identical.
\end{remarkbreak}


\begin{remark}
    We elaborate on the claim that Proposition~2.7 is a \enquote{ring analogue} of Cauchy's theorem. The latter says that any group $G$ acts faithfully on some set, namely itself by left multiplication. In other words, there is an injective group homomorphism
    %
    \begin{equation*}
        \sigma \colon G \to \Aut_\catSet(G).
    \end{equation*}
    %
    Notice that the automorphisms are on the \emph{set} $G$. Since $\sigma$ is injective we may identify $G$ with a subgroup of $\Aut_\catSet(G)$.

    Similarly, we recall that if $G$ is an abelian group, then $\End_\catAb(G)$ is a ring where the multiplication is given by function composition. The statement of Proposition~2.7 is that there is an injective ring homomorphism
    %
    \begin{equation*}
        \lambda \colon R \to \End_\catAb(R).
    \end{equation*}
    %
    And just as $\sigma(g)$ is left-multiplication by $g \in G$ in the \emph{group} $G$ considered as a \emph{set}, $\lambda(r) = \lambda_r$ is left-multiplication by $r \in R$ in the \emph{ring} $R$ considered as an \emph{abelian group}. The fact that multiplication does not make $R$ into a group is captured by the fact that $\lambda$ maps $R$ into the set of all endomorphisms of $R$, and not just the automorphisms.

    We might view $\sigma$ as providing a sort of \enquote{prototype} for group actions: To obtain more general actions we consider group homomorphisms $\sigma \colon G \to \Aut_\catSet(A)$, where $A$ is \emph{any} set. If we consider such a map $\sigma$ as belonging to the set $A$, then we arrive at concept of \emph{$G$-sets}. Transferring this interpretation to rings, we might contemplate actions $\lambda \colon R \to \End_\catAb(M)$ for any abelian group $M$. Similarly thinking of $\lambda$ as belonging to the group $M$, this is precisely what defines an $R$-module structure on $M$ (indeed, in analogy with $G\text{-}\catSet$ we could have denoted the category of $R$-modules by $R\text{-}\catAb$ instead of $\catRMod$). % Better commands for G-sets
\end{remark}


\begin{exerciseframed}[3.2.2]
    Let $R$ and $S$ be rings, and let $\phi \colon R \to S$ be a function preserving both operations $+$, $\cdot$.
    %
    \begin{enumerate}
        \item Prove that if $\phi$ is surjective, then necessarily $\phi(1_R) = 1_S$.
        \item Prove that if $\phi \neq 0$ and $S$ is an integral domain, then $\phi(1_R) = 1_S$.
    \end{enumerate}
\end{exerciseframed}

\begin{solution}
\begin{solutionsec}
    \item Assume that $\phi$ is surjective, and choose $r \in R$ such that $\phi(r) = 1_S$. Then
    %
    \begin{equation*}
        1_S
            = \phi(r)
            = \phi(1_R r)
            = \phi(1_R) \phi(r)
            = \phi(1_R)
    \end{equation*}
    %
    as desired.

    \item Assume that $\phi \neq 0$ and that $S$ is an integral domain. First notice that $\phi(1_R) \neq 0_S$, since otherwise $\phi = 0$. Also notice that $\phi$ is a group homomorphism between the underlying additive groups of $R$ and $S$, so $\phi(0_R) = 0_S$. It follows that
    %
    \begin{equation*}
        0_S
            = \phi(0_R)
            = \phi(1_R^2 - 1_R)
            = \phi(1_R)(\phi(1_R) - 1_S),
    \end{equation*}
    %
    and since $S$ is an integral domain we obtain $\phi(1_R) - 1_S = 0$ as claimed.
\end{solutionsec}
\end{solution}


\begin{exerciseframed}[3.2.6]
    Verify the \enquote{extension property} of polynomial rings, stated in Example~2.3.
\end{exerciseframed}

\begin{solution}
    If $\alpha \colon R \to S$ is a ring homomorphism, and $s \in S$ commutes with $\alpha(r)$ for all $r \in R$, then we must construct a unique ring homomorphism $\overline\alpha \colon R[x] \to S$ which extends $\alpha$ and sends $x$ to $s$.
    
    Similar to the proof of Proposition~2.1, since $\overline\alpha$ has to be a homomorphism, we require that
    %
    \begin{equation*}
        \overline\alpha \biggl( \sum_{i\in\naturals} r_i x^i \biggr)
            = \sum_{i\in\naturals} \alpha(r_i) s^i,
    \end{equation*}
    %
    where finitely many $r_i \in R$ are nonzero. This clearly preserves addition. As for multiplication:
    %
    \begin{align*}
        \overline\alpha \biggl( \sum_{i\in\naturals} r_i x^i \sum_{j\in\naturals} t_j x^j \biggr)
            &= \overline\alpha \biggl( \sum_{k\in\naturals} \sum_{i+j=k} r_i t_j x^{i+j} \biggr)
             = \sum_{k\in\naturals} \sum_{i+j=k} \alpha(r_i) \alpha(t_j) s^{i+j} \\
            &= \sum_{k\in\naturals} \sum_{i+j=k} \alpha(r_i) s^i \alpha(t_j) s^j
             = \sum_{i\in\naturals} \alpha(r_i) s^i \sum_{j\in\naturals} \alpha(t_j) s^j \\
            &= \overline\alpha \biggl( \sum_{i\in\naturals} r_i x^i \biggr) \, \overline\alpha \biggl( \sum_{j\in\naturals} t_j x^j \biggr).
    \end{align*}
    %
    Thus $\overline\alpha$ is a homomorphism, and it is clearly unique with the required properties.
\end{solution}


\begin{exerciseframed}[3.2.8]
    Prove that every subring of a field is an integral domain.
\end{exerciseframed}

\begin{solution}
    Let $R$ be a subring of a field $F$. We need only show that the only zero-divisor in $R$ is zero. But this is obvious, since if $a, b \in R$ are such that $ab = 0$ in $R$, then this equality also holds in $F$, so either $a = 0$ or $b = 0$.
\end{solution}


\begin{exerciseframed}[3.2.9]
    The \emph{centre} of a ring $R$ consists of the elements $a$ such that $ar = ra$ for all $r \in R$.
    %
    \begin{enumerate}
        \item Prove that the centre is a subring of $R$.
        \item Prove that the centre of a division ring is a field.
    \end{enumerate}
\end{exerciseframed}

\begin{solution}
\begin{solutionsec}
    \item Let $C$ denote the centre of $R$. Clearly $\pm 1_R \in C$. If $a,b \in C$ and $r \in R$, then
    %
    \begin{equation*}
        (a + b)r
            = ar + br
            = ra + rb
            = r(a + b)
    \end{equation*}
    %
    by the distributive property, and
    %
    \begin{equation*}
        (ab)r
            = arb
            = r(ab)
    \end{equation*}
    %
    by associativity. Thus $a + b \in C$ and $ab \in C$, so is a subring of $R$.

    \item Let $C$ be the centre of a division ring $R$. Since the elements of $C$ commute with all elements in $R$, in particular all elements in $C$, it follows that $C$ is commutative. It remains to be shown that all nonzero elements of $C$ have an inverse in $C$, so let $a \in C$. This has an inverse $a\inv$ in $R$, and we claim that $a\inv \in C$. For
    %
    \begin{equation*}
        a\inv r
            = a\inv r (a a\inv)
            = a\inv (r a) a\inv
            = a\inv (a r) a\inv
            = (a\inv a) r a\inv
            = r a\inv.
    \end{equation*}
\end{solutionsec}
\end{solution}


\begin{exerciseframed}[3.2.10]
    The \emph{centraliser} of an element $a$ of a ring $R$ consists of the elements $r \in R$ such that $ar = ra$.
    %
    \begin{enumerate}
        \item Prove that the centraliser of $a$ is a subring of $R$, for every $a \in R$.
        \item Prove that the centre of $R$ is the intersection of all its centralisers.
        \item Prove that every centraliser in a division ring is a division ring.
    \end{enumerate}
\end{exerciseframed}

\begin{solution}
\begin{solutionsec}
    \item Denote the centraliser of $a \in R$ by $C_a$. If $r,s \in C_a$, then
    %
    \begin{equation*}
        (r + s)a
            = ra + sa
            = ar + as
            = a(r + s)
    \end{equation*}
    %
    and
    %
    \begin{equation*}
        (rs)a
            = ras
            = a(rs),
    \end{equation*}
    %
    so $r + s \in C_a$ and $rs \in C_a$. Thus $C_a$ is a subring of $R$.

    \item Let $C$ denote the centre of $R$. Then $r \in C$ if and only if $ar = ra$ for all $a \in R$. But this is the case just when $r \in C_a$ for all $a \in R$. Thus $C = \bigintersect_{a \in R} C_a$. (Incidentally, this also shows that $C$ is a subring, since an arbitrary intersection of subrings is a subring.)
    
    \item Assume that $R$ is a division ring, and let $a \in R$. Given $r \in C_a$ we must show that $r\inv \in C_a$. But we have
    %
    \begin{equation*}
        r\inv a
            = r\inv a (r r\inv)
            = r\inv (a r) r\inv
            = r\inv (r a) r\inv
            = (r\inv r) a r\inv
            = a r\inv,
    \end{equation*}
    %
    which proves the claim.
\end{solutionsec}
\end{solution}


\begin{exerciseframed}[3.2.12]
    Consider the inclusion map $\iota \colon \ints \hookrightarrow \rationals$. Describe the cokernel of $\iota$ in $\catAb$ and its cokernel in $\catRing$ (as defined by the appropriate universal property in the style of the one given in ¬ßII.8.6).
\end{exerciseframed}

\begin{solution}
    In $\catAb$, the cokernel of $\iota$ is the quotient $\rationals/\ints$. This is (isomorphic to) the subgroup of the circle $S^1$ containing the results of rotations by $2\pi r$ radians for rational $r$. This is nontrivial, so $\iota$ is not an epimorphism (indeed it is clearly not surjective).

    If $\phi \colon R \to R'$ is a ring homomorphism, then a cokernel of $\phi$ would seem to be a ring $\coker\phi$ along with a homomorphism $\pi \colon R' \to \coker\phi$ such that, for any ring homomorphism $\alpha \colon R' \to S$ with $\alpha \circ \phi = \alpha \circ 0$, there is a unique homomorphism $\tilde\alpha \colon \coker\phi \to S$ such that the diagram
    %
    \begin{equation*}
        \begin{tikzcd}[row sep=tiny]
            && S \\
            R
                \ar[r, "\phi", shift left]
                \ar[r, "0", shift right, swap]
            & R'
                \ar[ur, "\alpha"]
                \ar[dr, "\pi", swap, twoheadrightarrow] \\
            && \coker\phi
                \ar[uu, "\tilde\alpha", swap, dashed]
        \end{tikzcd}
    \end{equation*}
    %
    commutes, where $0$ is a suitable (fixed) map, playing the role of the zero group homomorphism. In $\catRing$ it is not so clear what this map should be, since we cannot simply send everything to zero when $R'$ is nontrivial. In other words, since there is no null object in $\catRing$, there is no canonical way to get it to work using the language of (co)equalisers.

    Using Aluffi's slightly different (but in $\catGrp$ and $\catAb$ equivalent) definition of cokernels, we would instead require that $\pi$ be initial with respect to homomorphisms $\alpha \colon R' \to S$ with $\alpha \circ \phi = 0$. Then $\alpha$ should factor uniquely through $\coker\phi$:
    %
    \begin{equation*}
        \begin{tikzcd}[row sep=tiny]
            && S \\
            R
                \ar[r, "\phi"]
                \ar[urr, "0", bend left]
                \ar[drr, "0", bend right, swap]
            & R'
                \ar[ur, "\alpha"]
                \ar[dr, "\pi", swap, twoheadrightarrow] \\
            && \coker\phi
                \ar[uu, "\tilde\alpha", swap, dashed]
        \end{tikzcd}
    \end{equation*}
    %
    Hence we must in particular have $\pi \circ \phi = 0$, but the zero map is only a ring homomorphism if its codomain is zero, so $\coker\phi = 0$. We see that the cokernel of \emph{any} ring homomorphism is the zero ring, so there is no useful concept of cokernel in $\catRing$.
\end{solution}


\section{Ideals and quotient rings}

\begin{remarkbreak}[Ideals and homomorphisms]
    The image or preimage of a \emph{subring} under a ring homomorphism is a subring, but there is no reason (in universal algebra) to expect e.g. that the kernel of a homomorphism have any sort of special structure: Since a ring has two constants (i.e. nullary operations, namely $0$ and $1$), the subset $\{0\}$ is not generally a subring.

    In the interpretation of an ideal $I$ in a ring $R$ as a subset capable of being \enquote{set equal to zero}, then there seems to be no reason to expect the image of an ideal to also be an ideal. For instance, $2\ints$ is an ideal in $\ints$, but the image $2\ints$ of the inclusion map $\ints \hookrightarrow \rationals$ is not an ideal in $\rationals$, since forcing all even numbers to be zero in $\rationals$ would also force many other numbers to be zero (indeed all numbers since $\rationals$ is a field).

    On the other hand, it seems more natural for kernels to be ideals. If a homomorphism $\phi$ annihilates elements $a$ and $b$, then it should also annihilate all elements on the form $a + b$ or $ra$. If we extend the meaning of \enquote{annihilate} to also allow $\phi$ to map elements into some ideal of its codomain, then it also seems natural enough for the preimage of any ideal to be an ideal.
\end{remarkbreak}


\begin{remarkbreak}[Ideals in universal algebra]
    One might wonder whether there is an algebraic theory of (say, left) ideals. If $I$ is an ideal in a ring $R$, then $I$ is in particular an abelian group $\langle I, +, -, 0 \rangle$ under addition. Furthermore, for $a \in I$ and $r \in R$ we must have $ra \in I$, so each element $r$ in $R$ determines a unitary operation $\lambda_r$ given by $\lambda_r(a) = ra$. Hence an ideal is an algebra
    %
    \begin{equation*}
        \mathfrak{I}
            = \bigl\langle I, +, -, 0, \langle \lambda_r \mid r \in R \rangle \bigr\rangle
    \end{equation*}
    %
    satisfying the obvious identities, and further satisfying that $I \subseteq R$. We immediately notice that we are only able to construct an algebraic theory of ideals of a \emph{fixed} ring. But this already defeats the purpose since we want a theory of \emph{ideals} and not a theory of \emph{ideals in $R$}.

    However, if we allow $I$ to be any set and not just a subset of $R$, then we arrive precisely at the theory of (left) $R$-modules.
\end{remarkbreak}


\begin{exerciseframed}[3.3.2]
    Let $\phi \colon R \to S$ be a ring homomorphism, and let $J$ be an ideal of $S$. Prove that $I = \phi\preim(J)$ is an ideal of $R$.
\end{exerciseframed}

\begin{solution}
    We may of course prove this by verifying directly that $\phi\preim(J)$ satisfies the definition of an ideal. Alternatively, notice that $I$ is the kernel of the composition
    %
    \begin{equation*}
        \begin{tikzcd}
            R
                \ar[r, "\phi"]
            & S
                \ar[r, "\pi"]
            & S/J.
        \end{tikzcd}
    \end{equation*}
\end{solution}


\begin{exerciseframed}[3.3.3]
    Let $\phi \colon R \to S$ be a ring homomorphism, and let $J$ be an ideal of $R$.
    %
    \begin{enumerate}
        \item Show that $\phi(J)$ need not be an ideal of $S$.
        \item Assume that $\phi$ is surjective; then prove that $\phi(J)$ \emph{is} an ideal of $S$.
        \item Assume that $\phi$ is surjective, and let $I = \ker\phi$; thus we may identify $S$ with $R/I$. Let $\overline{J} = \phi(J)$, an ideal of $R/I$ by the previous point. Prove that
        %
        \begin{equation*}
            \frac{R/I}{\overline{J}}
                \cong \frac{R}{I+J}.
        \end{equation*}
    \end{enumerate}
\end{exerciseframed}

\begin{solution}
\begin{solutionsec}
    \item Let $\iota \colon \ints \to \rationals$ be the inclusion map. Then $\iota(n\ints) = n\ints$, but this is clearly not an ideal in $\rationals$.
    
    \item Since $J$ is a subgroup of $(R,+)$, $\phi(J)$ is also a subgroup of $(S,+)$. If $b \in \phi(J)$ and $s \in S$, then there exist $a \in J$ and $r \in R$ such that $b = \phi(a)$ and $s = \phi(r)$. But then
    %
    \begin{equation*}
        bs
            = \phi(a) \phi(r)
            = \phi(ar)
            \in \phi(J),
    \end{equation*}
    %
    since $ar \in J$. We similarly find that $sb \in \phi(J)$, so $\phi(J)$ is an ideal.

    \item Notice that $\phi(I+J) = \phi(J) = \overline{J}$. Substituting $J \to I + J$ in Proposition~3.11 then yields the claim, since $I+J$ is an ideal containing $I$.
\end{solutionsec}
\end{solution}


\begin{exerciseframed}[3.3.5]
    Let $J$ be a \emph{two-sided} ideal of the ring $\calM_n(R)$ of $n \times n$ matrices over a ring $R$. Prove that a matrix $A \in \calM_n(R)$ belongs to $J$ if and only if the matrices obtained by placing any entry of $A$ in any position, and $0$ elsewhere, belong to $J$.
\end{exerciseframed}

\begin{solution}
    Let $E_{ij} \in \calM_n(R)$ denote the matrix with $(E_{ij})_{kl} = \delta_{ik} \delta_{jl}$, i.e. the matrix with a $1$ in the $(i,j)$-th entry and $0$ elsewhere. For $A = (a_{ij}) \in \calM_n(R)$ we then have
    %
    \begin{equation*}
        E_{ij} A E_{kl} = a_{jk} E_{il}.
    \end{equation*}
    %
    The matrix on the right-hand side is precisely of the type described, and every matrix such described can we written on this form. If $A \in J$, then since $J$ is a two-sided ideal, all matrices on the form $a_{jk} E_{il}$ also lie in $J$. Since $A$ is a sum of matrices on this form, the converse also holds. This proves the claim.
\end{solution}


\begin{exerciseframed}[3.3.6]
    Let $J$ be a two-sided ideal of the ring $\calM_n(R)$ of $n \times n$ matrices over a ring $R$, and let $I \subseteq R$ be the set of $(1,1)$ entries of matrices in $J$. Prove that $I$ is a two-sided ideal of $R$ and $J$ consists precisely of those matrices whose entries all belong to $I$.
\end{exerciseframed}

\begin{solution}
    Let $a \in I$ and $r \in R$. Then $a$ is the $(1,1)$-th entry of some matrix in $J$. By \exref{3.3.5} the matrix $aE_{11}$ also belongs to $J$, and hence so does the product $(aE_{11})(rE_{11}) = arE_{11}$ since $J$ is an ideal. Thus $ar \in I$, and we similarly find that $ra \in I$, so $I$ is a two-sided ideal.

    If $A = (a_{ij}) \in \calM_n(R)$ is a matrix whose entries all belong to $I$, then the matrices $a_{ij} E_{11}$ all lie in $J$. By \exref{3.3.5} so do the matrices $a_{ij} E_{ij}$, and since $A$ is a sum of these matrices we also have $A \in J$. Conversely, if $A = (a_{ij}) \in J$, then also $a_{ij} E_{11} \in J$ by \exref{3.3.5}. But then $a_{ij} \in I$.
\end{solution}


\begin{exerciseframed}[3.3.8]
    Prove that a nonzero ring $R$ is a division ring if and only if its only left-ideals and right-ideals are $\{0\}$ and $R$.

    In particular, a nonzero commutative ring $R$ is a field if and only if the only ideals of $R$ are $\{0\}$ and $R$.
\end{exerciseframed}
%
We have added the assumption that $R$ be nonzero since the zero ring is not a field, yet $\{0\}$ and $R$ are its only (left- or right-) ideals (indeed they both coincide with the ring itself). Furthermore, Aluffi does not require division rings to be nonzero (cf. Definition 1.13), but he seems to assume this elsewhere so we do so as well.

\begin{solution}
    Assume that $R$ is a division ring, and let $I \neq \{0\}$ be a left-ideal of $R$. If $a \in I$ then also $1 = a\inv a \in I$, so $I = R$. Similarly for right-ideals.

    Conversely, assume that $\{0\}$ and $R$ are the only left-ideals and right-ideals of $R$. If $a \in R$ is nonzero, then $Ra$ and $aR$ are left- and right-ideals of $R$ different from $\{0\}$. But then we must have $Ra = aR = R$, so $1$ lies in both ideals. Thus there are elements $r_1, r_2 \in R$ such that $r_1 a = 1 = a r_2$, hence $a$ is a unit (and of course $r_1 = r_2$).
\end{solution}


\begin{exerciseframed}[3.3.9]
    Counterpoint to \exref{3.3.8}: It is \emph{not} true that a (nonzero) ring $R$ is a division ring if and only if its only two-sided ideals are $\{0\}$ and $R$. A nonzero ring with this property is said to be \emph{simple}; by \exref{3.3.8}, fields are the only simple \emph{commutative} rings.

    Prove that $\calM_n(\reals)$ is simple.
\end{exerciseframed}

\begin{solution}
    Let $J \neq (0)$ be a two-sided ideal of $\calM_n(\reals)$, and let $I$ be the set of $(1,1)$-th entries of matrices in $J$. Since $J \neq (0)$ there is a matrix in $J$ with a nonzero entry. By \exref{3.3.5} there is then matrix in $J$ with a nonzero $(1,1)$-th entry, so $I$ contains this element, and $I \neq (0)$.
    
    By \exref{3.3.6}, $I$ is an ideal in $\reals$, and since $\reals$ is a field we must have $I = (1)$. Again by \exref{3.3.6}, $J$ must contain all matrices in $\calM_n(\reals)$, so $J = (1)$. Thus $\calM_n(\reals)$ is simple.

    Notice that we only use the fact that $\reals$ is a field, so the same argument shows that $\calM_n(k)$ is simple for any field $k$.
\end{solution}


\begin{exerciseframed}[3.3.10]
    Let $\phi \colon k \to R$ be a ring homomorphism, where $k$ is a field and $R$ is a nonzero ring. Prove that $\phi$ is \emph{injective}.
\end{exerciseframed}

\begin{solution}
    Let $a \in k$ be nonzero. Then it is a unit, so $\phi(a)$ is a unit in $R$. Since $R$ is nonzero, we must have $\phi(a) \neq 0_R$. It follows that $a \not\in \ker\phi$, so $\phi$ is injective.

    Alternatively, since $R$ is nonzero $\phi$ is nontrivial, $\ker\phi \neq R$. And $\ker\phi$ is an ideal in $k$, so $\ker\phi = \{0\}$ by \exref{3.3.8}.
\end{solution}


\begin{exerciseframed}[3.3.12]
    Let $R$ be a \emph{commutative} ring. Prove that the set of nilpotent elements of $R$ is an ideal of $R$. (Cf. \exref{3.1.6}. This ideal is called the \emph{nilradical} of $R$.)
\end{exerciseframed}

\begin{solution}
    Denote the nilradical of $R$ by $N$. \exref{3.1.6} implies that $N$ is a subgroup, so let $a \in N$ and $r \in R$, and choose $n \in \naturals$ such that $a^n = 0$. Since $a$ and $r$ commute, it follows that $(ar)^n = a^n r^n = 0$, so $ar \in N$. Thus $N$ is an ideal.
\end{solution}


\begin{exerciseframed}[3.3.13]
    Let $R$ be a commutative ring, and let $N$ be its nilradical (cf. \exref{3.3.12}). Prove that $R/N$ contains no nonzero nilpotent elements. (Such a ring is said to be \emph{reduced}.)
\end{exerciseframed}

\begin{solution}
    Let $a + N \in R/N$ be nilpotent. Then there is an $n \in \naturals$ such that
    %
    \begin{equation*}
        0 + N
            = (a + N)^n
            = a^n + N,
    \end{equation*}
    %
    from which it follows that $a^n \in N$, i.e. that $a^n$ is nilpotent. But then $a$ is also nilpotent, so $a \in N$.
\end{solution}


\begin{exerciseframed}[3.3.14]
    Prove that the characteristic of an integral domain is either $0$ or a prime integer. Do you know any ring of characteristic $1$?
\end{exerciseframed}

\begin{solution}
    Let $R$ be an integral domain, and let $f \colon \ints \to R$ be the unique homomorphism. Notice that $\im f$ is also an integral domain. The canonical decomposition of $f$ implies that $\ints/n\ints \cong \im f$. But $\ints/n\ints$ is an integral domain if and only if $n$ is either $0$ (in which case this ring is just $\ints$) or a prime integer.

    The zero ring has characteristic $1$, since it is isomorphic to $\ints/1\ints$.
\end{solution}


\section{Ideals and quotients: Remarks and examples. Prime and maximal ideals}

\begin{remarkbreak}[Fields and maximal ideals]
    If $R$ is a commutative ring and $I \subseteq R$ is any subset, we know that it makes sense to consider the quotient ring $R/I$ only when $I$ is an ideal. By taking the quotient with $I$ we are effectively forcing every element of $I$ to be zero, and only these elements to be zero. First of all we must then have $0 \in I$, and $I$ must also be closed under addition since $0 + 0 = 0$. Clearly $-0 = 0$, so each element in $I$ must also have an additive inverse. Thus $I$ is a group.

    Furthermore, multiplication of any element in $R$ by zero must again yield zero, so $I$ must be closed under multiplication with any element in $R$. This is precisely the definition of an ideal.

    When is it possible to force an element to be zero? If we force a unit $u$ to be zero, then we would intuitively have $1 = u u\inv = 0 u\inv = 0$. If we don't want to end up with the zero ring, then we cannot force units to be zero. In a field every non-zero element is a unit, so fields cannot have any non-trivial ideals.

    Put another way: We \emph{can} force non-units to be zero. By doing this we first of all eliminate some of the non-units entirely, and some of the non-units may become units. For instance, in $\ints$ the number $2$ is not a unit, but in $\ints/3\ints$ the corresponding class $[2]_3$ \emph{is} a unit since $[2]_3 [2]_3 = [4]_3 = [1]_3$. The idea is that by collapsing the ideal $I$ to zero, we are forced to identify a bunch of other elements if their difference lies in $I$. Hence many elements in the original ring $R$ may correspond to the identity $1$ after collapsing $I$ to zero, and so it is easier for a product to equal $1$.

    Let $I$ be a maximal ideal. Then we are not able to force any more elements to be zero without reducing $R$ to the zero ring. Hence $R/I$ in some sense contains the minimal number of non-units. Indeed, if $R/I$ contained \emph{any} non-units whatsoever, say the element $r$, then taking the quotient by $(r)$ removes even more non-units. But we had already removed the maximal amount of non-units, so this is impossible. Hence $R/I$ cannot contain any non-units and is thus a field.
\end{remarkbreak}


\begin{remarkbreak}[Lattice of ideals]
    Let $R$ be a ring. Since $(R,+)$ is a (commutative) group we may consider its lattice $\frakL(R)$ of subgroups. If $H$ and $K$ are subgroups of $(R,+)$, then their join and meet in $\frakL(R)$ are $H + K$ (in additive notation) and $H \intersect K$ respectively. If $I$ and $J$ are ideals in $R$, then they are in particular subgroups, and it is easy to check that $I + J$ and $I \intersect J$ are also ideals. Hence the subset $\frakI(R) \subseteq \frakL(R)$ of ideals is in fact a sublattice.

    In groups we only have one operation with which to combine subgroups. In a ring we have an addition, used above to define $I + J$, but we also have a multiplication: We define the product of the ideals $I$ and $J$ in $R$ by
    %
    \begin{equation*}
        IJ
            = (I \cdot J)
            = ( ij \mid i \in I, j \in J ),
    \end{equation*}
    %
    i.e., $IJ$ is the ideal generated by products $ij$ for $i \in I$ and $j \in J$. Notice that $IJ \subseteq I \intersect J$, but that $I$ and $J$ do not generally lie in $IJ$. We give a series of properties of the product of ideals:
    %
    \begin{enumerate}
        \item If $R$ is commutative and $a,b \in R$, then $(a)(b) = (ab)$. Clearly $ab \in (a)(b)$ so $(ab) \subseteq (a)(b)$, and for the opposite inclusion notice that
        %
        \begin{equation*}
            \sum_{i=1}^n (r_i a) (s_i b)
                = ab \sum_{i=1}^n r_i s_i
                \in (ab),
        \end{equation*}
        %
        so $(a)(b) \subseteq (ab)$.

        In $\ints$ we have $(a) \intersect (b) = (\lcm(a,b))$, so here $(a)(b) = (a) \intersect (b)$ just when $a$ and $b$ are relatively prime.

        \item If $R$ is a PID, then of course $(a)(b) = (c)$ for some $c$, but as far as I know there is no general relation between $a,b,c$.
    \end{enumerate}
\end{remarkbreak}


\begin{exerciseframed}[3.4.1]
    Let $R$ be a ring, and let $\{I_\alpha\}_{\alpha \in A}$ be a family of ideals in $R$. We let
    %
    \begin{equation*}
        \sum_{\alpha \in A} I_\alpha
            = \set[\bigg]{
                \sum_{\alpha \in A} r_\alpha
            }{
                \text{$r_\alpha \in I_\alpha$ and $r_\alpha = 0$ for all but finitely many $\alpha$}
            }.
    \end{equation*}
    %
    Prove that $\sum_{\alpha \in A} I_\alpha$ is an ideal of $R$ and that it is the smallest ideal containing all of the ideals $I_\alpha$.
\end{exerciseframed}

\begin{solution}
    It is clearly an ideal. Let $J$ be an ideal containing all $I_\alpha$, and let $\sum_{\alpha \in A} r_\alpha$ be an element of $\sum_{\alpha \in A} I_\alpha$. Then $r_\alpha \in I_\alpha \subseteq J$ for all $\alpha$, and since $J$ is a subgroup of $R$ we have $\sum_{\alpha \in A} r_\alpha \in J$. This proves the claim.
\end{solution}


\begin{exerciseframed}[3.4.2]
    Prove that the homomorphic image of a Noetherian ring is Noetherian.
\end{exerciseframed}

\begin{solution}
    We begin with a lemma: If $R$ is a ring and $A \subseteq R$, then we denote by $(A)$ the ideal generated by $A$, i.e. the intersection of all ideals of $R$ containing $A$. If $\phi \colon R \to S$ is a ring homomorphism, then we claim that $\phi((A)) = (\phi(A))$, analogously to the situation for groups.

    The inclusion $\supseteq$ is obvious, so let $b \in \phi((A))$. Then $b = \phi(a)$ for some $a \in (A)$, and
    %
    \begin{equation*}
        a
            = r_1 a_1 + \cdots + r_n a_n
    \end{equation*}
    %
    for some $r_i \in R$ and $a_i \in A$. It follows that
    %
    \begin{equation*}
        b
            = \phi(r_1) \phi(a_1) + \cdots + \phi(r_n) \phi(a_n).
    \end{equation*}
    %
    Hence $b \in (\phi(A))$.

    Now let $\phi \colon R \to S$ be a surjective ring homomorphism with $R$ Noetherian, and let $J \subseteq S$ be an ideal. Then $I = \phi\preim(J)$ is an ideal in $R$, hence generated by a finite set $A \subseteq R$. Since $\phi$ is surjective we have $\phi(I) = J$, so by the lemma above $J = \phi((A)) = (\phi(A))$. Since $\phi(A)$ is finite, the claim follows.
\end{solution}


\begin{exerciseframed}[3.4.3]
    Prove that the ideal $(2,x)$ of $\ints[x]$ is not principal.
\end{exerciseframed}

\begin{solution}
    Let $I$ be a principal ideal of $\ints[x]$ containing $(2,x)$. Then there is some $f(x) \in \ints[x]$ such that $I = (f(x))$, and in particular $2 = p(x)f(x)$ and $x = q(x)f(x)$ for appropriate $p(x), q(x) \in \ints[x]$. The first equality implies that $\deg f(x) = 0$, and the second that $f(x)$ is monic. Hence $f(x) = 1$, but $1 \not\in (2,x)$ so $(2,x) \neq I$. The claim follows.
\end{solution}


\begin{exerciseframed}[3.4.4]
    Prove that if $k$ is a field, then $k[x]$ is a PID.
\end{exerciseframed}

\begin{solution}
    Let $I \subseteq k[x]$ be an ideal. If $I = (0)$ then $I$ is principal, so assume that $I \neq (0)$. Let $f(x)$ be a nonzero polynomial in $I$ with minimal degree. By multiplying by the reciprocal of the leading coefficient we may assume that $f(x)$ is monic. Let $g(x) \in I$. By division with remainder there exist $q(x), r(x) \in k[x]$ such that
    %
    \begin{equation*}
        g(x) = f(x) q(x) + r(x),
    \end{equation*}
    %
    and such that $\deg r(x) < \deg f(x)$. Then $r(x) \in I$, but since $\deg f(x)$ was assumed to be minimal in $I$, we must have $\deg r(x) = -\infty$, i.e. $r(x) = 0$. Thus $g(x) \in (f(x))$, so $I = (f(x))$.
\end{solution}


\begin{exerciseframed}[3.4.5]
    Let $I,J$ be ideals in a commutative ring $R$, such that $I + J = (1)$. Prove that $IJ = I \intersect J$.
\end{exerciseframed}
%
Ideals $I$ and $J$ in a (not necessarily commutative) ring $R$ are said to be \emph{comaximal} if $I + J = R$. A set $\mathcal{I}$ of ideals in $R$ is called \emph{(pairwise) comaximal} if $I$ and $J$ are comaximal for all distinct $I, J \in \mathcal{I}$.

\begin{solution}
    There exist $i \in I$ and $j \in J$ such that $i + j = 1$. For $a \in I \intersect J$ it follows that
    %
    \begin{equation*}
        a
            = a(i + j)
            = ai + aj
            = ia + aj.
    \end{equation*}
    %
    Since $a$ lies in both $I$ and $J$, this shows that $a \in IJ$.
\end{solution}


\begin{exerciseframed}[3.4.10]
    Let $d$ be an integer that is not the square of an integer, and consider the subset of $\complex$ defined by
    %
    \begin{equation*}
        \rationals(\sqrt{d})
            \defn \set{a + b\sqrt{d}}{a,b \in \rationals}.
    \end{equation*}
    %
    \begin{enumerate}
        \item Prove that $\rationals(\sqrt{d})$ is a subring of $\complex$.
        \item Define a function $N \colon \rationals(\sqrt{d}) \to \rationals$ by $N(a + b\sqrt{d}) \defn a^2 - b^2 d$. Prove that $N(zw) = N(z)N(w)$ and that $N(z) \neq 0$ if $z \in \rationals(\sqrt{d})$, $z \neq 0$.
        \item Prove that $\rationals(\sqrt{d})$ is a field and in fact the smallest subfield of $\complex$ containing both $\rationals$ and $\sqrt{d}$.
        \item Prove that $\rationals(\sqrt{d}) \cong \rationals[t]/(t^2 - d)$.
    \end{enumerate}
\end{exerciseframed}

\begin{solution}
\begin{solutionsec}
    \item For $a,b,c,e \in \rationals$ we have
    %
    \begin{equation*}
        (a + b\sqrt{d}) + (c + e\sqrt{d})
            = (a + b) + (b + e) \sqrt{d}
    \end{equation*}
    %
    and
    \begin{equation*}
        (a + b\sqrt{d})(c + e\sqrt{d})
            = (ac + bed) + (ae + bc) \sqrt{d}.
    \end{equation*}
    %
    Furthermore, $\rationals(\sqrt{d})$ clearly contains additive inverses of all its elements.

    \item Writing $z = a + b\sqrt{d}$ and $w = c + e\sqrt{d}$ we find that
    %
    \begin{align*}
        N(zw)
            &= N \bigl( (ac + bed) + (ae + bc) \sqrt{d} \bigr) \\
            &= (ac + bed)^2 - (ae + bc)^2 d \\
            &= (ac)^2 - (ae)^2 d - (bc)^2 d + (bed)^2 \\
            &= (a^2 - b^2 d)(c^2 - e^2 d) \\
            &= N(z) N(w)
    \end{align*}
    %
    as desired. Now assume that $N(z) = 0$. It follows that $a^2 = b^2 d$. For the prime factorisations of each side to agree, since $d$ is not a square, we must have $a = b = 0$. Hence $z = 0$ as claimed.

    \item We prove that $\rationals(\sqrt{d})$ is a field, so let $z = a + b \sqrt{d} \in \rationals(\sqrt{d})$. Define $z^* = a - b \sqrt{d}$ and notice that $N(z) = zz^*$. If $z \neq 0$ then $N(z) \neq 0$, so it follows that $z^*/N(z)$ is the multiplicative inverse of $z$.
    
    As for minimality, a subfield of $\complex$ containing $\rationals$ and $\sqrt{d}$ must contain combinations on the form $a + b \sqrt{d}$ in order to be closed under addition and multiplication. Hence $\rationals(\sqrt{d})$ is the smallest such subfield.

    \item We have a pair of isomorphisms
    %
    \begin{equation*}
        \frac{ \rationals[t] }{ (t^2 - d) }
            \cong \rationals \oplus \rationals
            \cong \rationals(\sqrt{d})
    \end{equation*}
    %
    of abelian groups, where the first comes from Proposition~4.6 and the second is clear from the constraints on $d$. To see that this is also an isomorphism of rings, we simply take two elements in the quotient ring and see that the multiplication matches the multiplication on $\rationals(\sqrt{d})$.
\end{solutionsec}
\end{solution}


\begin{exerciseframed}[3.4.11]
    Let $R$ be a commutative ring, $a \in R$, and $f_1(x), \ldots, f_r(x) \in R[x]$.
    %
    \begin{enumerate}
        \item Prove the equality of ideals
        %
        \begin{equation*}
            (f_1(x), \ldots, f_r(x), x-a)
                = (f_1(a), \ldots, f_r(a), x-a).
        \end{equation*}

        \item Prove the useful substitution trick
        %
        \begin{equation*}
            \frac{R[x]}{(f_1(x), \ldots, f_r(x), x-a)}
                \cong \frac{R}{(f_1(a), \ldots, f_r(a))}.
        \end{equation*}
    \end{enumerate}
\end{exerciseframed}

\begin{solution}
\begin{solutionsec}
    \item By division with remainder we have
    %
    \begin{equation*}
        f_i(x)
            = (x-a) q_i(x) + r_i
    \end{equation*}
    %
    for some $q_i(x) \in R[x]$ and $r_i \in R$. Evaluating in $x = a$ we find that $r_i = f_i(a)$, so the claim follows.

    \item Consider the evaluation map $\phi \colon R[x] \to R$ given by $f(x) \mapsto f(a)$. This is surjective (since it is constant on constant polynomials) with $\ker\phi = (x-a)$, so the first isomorphism theorem implies that
    %
    \begin{equation*}
        \frac{R[x]}{(x-a)} \cong R.
    \end{equation*}
    %
    Also notice that the image of $(f_1(x), \ldots, f_r(x))$ under $\phi$ is $(f_1(a), \ldots, f_r(a))$. Since
    %
    \begin{equation*}
        (f_1(a), \ldots, f_r(a), x-a)
            = (f_1(a), \ldots, f_r(a)) + (x-a),
    \end{equation*}
    %
    it follows from \exref{3.3.3} that
    %
    \begin{equation*}
        \frac{R[x]}{(f_1(x), \ldots, f_r(x), x-a)}
            \cong \frac{R[x]/(x-a)}{(f_1(a), \ldots, f_r(a))}
            \cong \frac{R}{(f_1(a), \ldots, f_r(a))}
    \end{equation*}
    %
    as desired.
\end{solutionsec}
\end{solution}


\begin{exerciseframed}[3.4.12]
    Let $R$ be a commutative ring and $a_1, \ldots, a_n$ elements of $R$. Prove that
    %
    \begin{equation*}
        \frac{R[x_1, \ldots, x_n]}{(x_1 - a_1, \ldots, x_n - a_n)}
            \cong R.
    \end{equation*}
\end{exerciseframed}

\begin{solution}
    By \exref{3.4.11} we have
    %
    \begin{equation*}
        \frac{R[x_1, \ldots, x_n]}{(x_1 - a_1, \ldots, x_n - a_n)}
            \cong \frac{R[x_1, \ldots, x_{n-1}][x_n]}{(x_1 - a_1, \ldots, x_n - a_n)}
            \cong \frac{R[x_1, \ldots, x_{n-1}]}{(x_1 - a_1, \ldots, x_{n-1} - a_{n-1})},
    \end{equation*}
    %
    and repeating this $n-1$ times yields the claim.
\end{solution}


\begin{exerciseframed}[3.4.13]
    Let $R$ be an integral domain. For all $k = 1, \ldots, n$ prove that $(x_1, \ldots, x_k)$ is prime in $R[x_1, \ldots, x_n]$.
\end{exerciseframed}

\begin{solution}
    By \exref{3.4.12} we have
    %
    \begin{equation*}
        \frac{R[x_1, \ldots, x_n]}{(x_1, \ldots, x_k)}
            \cong \frac{R[x_{k+1}, \ldots, x_n][x_1, \ldots, x_k]}{(x_1, \ldots, x_k)}
            \cong R[x_{k+1}, \ldots, x_n],
    \end{equation*}
    %
    which is an integral domain. Hence $(x_1, \ldots, x_k)$ is by definition prime.
\end{solution}


\begin{exerciseframed}[3.4.17]
    Let $K$ be a compact topological space, and let $R$ be the ring of continuous real-valued functions on $K$, with addition and multiplication defined pointwise.
    %
    \begin{enumerate}[label=(\roman*)]
        \item For $p \in K$, let $M_p = \set{f \in R}{f(p) = 0}$. Prove that $M_p$ is a maximal ideal in $R$.
        \item Prove that if $f_1, \ldots, f_r \in R$ have no common zeros, then $(f_1, \ldots, f_r) = (1)$.
        \item Prove that every maximal ideal $M$ in $R$ is of the form $M_p$ for some $p \in K$.
    \end{enumerate}
    %
    If further $K$ is Hausdorff, prove that $p \mapsto M_p$ defines a bijection from $K$ to the set of maximal ideals of $R$.
\end{exerciseframed}

\begin{solution}
\begin{solutionsec}[label=(\roman*)]
    \item Let $p \in K$ and consider the map $\phi \colon R \to \reals$ given by $\phi(f) = f(p)$. This is easily seen to be a surjective ring homomorphism with kernel $M_p$, so the first isomorphism theorem implies that $R/M_p \cong \reals$. Hence $R/M_p$ is a field, so $M_p$ is a maximal ideal.

    \item Let $f = f_1^2 + \cdots + f_r^2 \in (f_1, \ldots, f_r)$. Then $f$ is strictly positive everywhere, so $1/f$ is well-defined and continuous, i.e. an element of $R$. But then $1 = (1/f)f \in (f_1, \ldots, f_r)$.

    \item Let $I$ be an ideal in $R$ not contained in any $M_p$. (Every maximal ideal not on the form $M_p$ must have this property.) For every $p \in K$ there is then a function $f_p \in I$ such that $f_p(p) \neq 0$. Since $f_p$ is continuous, there is an open neighbourhood $U_p \subseteq K$ of $p$ such that $0 \not\in f_p(U_p)$. The collection $\{U_p\}_{p \in K}$ is an open cover of $K$, so by compactness it has a finite subcover $U_{p_1}, \ldots, U_{p_r}$. Every $p \in K$ lies in some $U_{p_i}$, so $f_{p_i}(p) \neq 0$. Thus $f_{p_1}, \ldots, f_{p_r}$ have no common zeros, so $(f_{p_1}, \ldots, f_{p_r}) = (1)$. It follows that $I = R$.
    
    \item[] \noindent Finally, also assume that $K$ is Hausdorff. It suffices to show that the map $p \mapsto M_p$ is injective. If $p \neq q$ are points in $K$, then Urysohn's lemma furnishes a function $f \in R$ such that $f(p) = 0$ and $f(q) = 1$. But then $f \in M_p$ and $f \not\in M_q$, so the claim follows.
    
    In fact, notice that the map $p \mapsto M_p$ is surjective if $K$ is compact and injective if $K$ is locally compact Hausdorff.
\end{solutionsec}
\end{solution}


\begin{exerciseframed}[3.4.18]
    Let $R$ be a commutative ring, and let $N$ be its nilradical (cf. \exref{3.3.12}). Prove that $N$ is contained in every prime ideal in $R$.
\end{exerciseframed}

\begin{solution}
    Let $P$ be a prime ideal in $R$, and consider $a \in N$. Then there is an $n \in \naturals$ such that $a^n = 0$, and this lies in $P$ since $P$ is a subgroup of $R$. But since $P$ is prime, it follows that either $a \in P$ or $a^{n-1} \in P$. Continuing this process yields $a \in P$, so $N \subseteq P$.
\end{solution}


\begin{exerciseframed}[3.4.19]
    Let $R$ be a commutative ring, let $P$ be a prime ideal in $R$, and let $I_j$ be ideals in $R$.
    %
    \begin{enumerate}[label=(\roman*)]
        \item Assume that $I_1 \cdots I_r \subseteq P$; prove that $I_j \subseteq P$ for some $j$.
        \item By (i), if $P \supseteq \bigintersect_{j=1}^r I_j$, then $P$ contains one of the ideals $I_j$. Prove or disprove: if $P \supseteq \bigintersect_{j=1}^\infty I_j$, then $P$ contains one of the ideals $I_j$.
    \end{enumerate}
\end{exerciseframed}

\begin{solution}
\begin{solutionsec}[label=(\roman*)]
    \item If at least one of the ideals $I_2, \ldots, I_r$ are contained in $P$, then we are done, so assume that neither of them are. Thus there exist $i_2 \in I_2, \ldots, i_r \in I_r$, none of which lie in $P$. Now let $i_1 \in I_1$. Then $i_1 \cdots i_r \in P$ and since $P$ is prime at least one of $i_1, \ldots, i_r$ lie in $P$. But none of the elements $i_2, \ldots, i_r$ do, so we must have $i_1 \in P$. Hence $I_1 \subseteq P$.
    
    \item We give a counterexample. Notice that
    %
    \begin{equation*}
        \bigintersect_{n=3}^\infty n\ints
            = 0\ints
            \subseteq 2\ints,
    \end{equation*}
    %
    but none of the $n\ints$ are contained in $2\ints$.
\end{solutionsec}
\end{solution}


\begin{exerciseframed}[3.4.20]
    Let $M$ be a two-sided ideal in a (not necessarily commutative) ring $R$. Prove that $M$ is maximal if and only if $R/M$ is a simple ring (cf. \exref{3.3.9}).
\end{exerciseframed}

\begin{solution}
    Aluffi does not provide a definition of maximal ideals in noncommutative rings, so we give one: We say that a two-sided ideal $I \neq (1)$ of a ring $R$ is \emph{maximal} if, for every two-sided ideal $J \subseteq R$, $I \subseteq J$ implies that either $I = J$ or $J = R$. In other words, $I$ is maximal with respect to set inclusion among all proper two-sided ideals of $R$.

    With this definition, the claim follows as in the proof of Proposition~4.11 from the one-to-one correspondence, preserving inclusion, between ideals of $R/M$ and ideals of $R$ containing $M$.
\end{solution}


\begin{exerciseframed}[3.4.21]
    Let $k$ be an algebraically closed field, and let $I \subseteq k[x]$ be an ideal. Prove that $I$ is maximal if and only if $I = (x-c)$ for some $c \in k$.
\end{exerciseframed}

\begin{solution}
    First assume that $I = (x-c)$ for some $c \in k$, and let $J$ be an ideal in $k[x]$ that properly contains $I$. Consider a polynomial $f(x) \in J \setminus I$. If $f(x)$ is constant it is a unit, hence $J = k[x]$. If $\deg f(x) = 1$ we may assume that $f(x)$ is monic, i.e. that $f(x) = x - b$ for some $b \in k$. It follows that $b \neq c$, hence $J$ contains the constant polynomial $(x - b) - (x - c) = c - b \neq 0$, so again $J = k[x]$. Finally assume that $\deg f(x) \geq 2$. By division with remainder there exist $q(x) \in k[x]$ and $r \in k$ such that
    %
    \begin{equation*}
        f(x) = q(x)(x-c) + r.
    \end{equation*}
    %
    It follows that $r \in J$ since $J$ contains $I$. We must also have $r \neq 0$, since otherwise $f(x) \in I$, so again we have $J = k[x]$.

    Conversely, let $I$ be a maximal ideal in $k[x]$, and let $f(x) \in I$ be nonconstant. Since $k$ is algebraically closed, there exists an $r \in k$ such that $f(r) = 0$. Division with remainder then yields $q(x) \in k[x]$ and $s \in k$ such that
    %
    \begin{equation*}
        f(x) = q(x)(x-r) + s.
    \end{equation*}
    %
    We find that $s = f(r) = 0$, so $f(x) \in (x-r)$. Thus $I \subseteq (x-r)$, and since $I$ is maximal the opposite inclusion also holds. This proves the claim.
\end{solution}


\begin{exerciseframed}[3.4.22]
    Prove that $(x^2 + 1)$ is maximal in $\reals[x]$.
\end{exerciseframed}

\begin{solution}
    Let $I$ be an ideal in $\reals[x]$ that properly contains $(x^2 + 1)$. Since $\reals[x]$ is a PID by \exref{3.4.4}, there is a polynomial $f(x) \in \reals[x]$ such that $I = (f(x))$. Then $x^2 + 1 = q(x)f(x)$ for some $q(x) \in \reals[x]$. If $q(x) \in \reals$, then $f(x) \in (x^2 + 1)$ which is impossible, so we must have $0 < \deg q(x) \leq 2$. If $\deg q(x) = 1$, then $q(x)$ has a root in $\reals$. This is then also a root of $x^2 + 1$, which is also impossible. Hence $\deg q(x) = 2$, which implies that $f(x) \in \reals$. Thus $I = \reals[x]$.
\end{solution}


\section{Modules over a ring}

\begin{remarkbreak}[Modules in universal algebra]
    Let $R$ be a fixed ring. A (left) $R$-module is an algebra
    %
    \begin{equation*}
        \mathfrak{M} = \bigl\langle M, +, -, 0, \langle \lambda_r \mid r \in R \rangle \bigr\rangle
    \end{equation*}
    %
    such that $\langle M, +, -, 0 \rangle$ is an abelian group, each $\lambda_r$ is a unitary operation, and which satisfies the usual identities. Hence, an $R$-module homomorphism $\phi \colon \mathfrak{M} \to \mathfrak{N}$ (where $\mathfrak{N}$ has underlying set $N$) is a homomorphism of the underlying abelian groups, for which the diagrams
    %
    \begin{equation*}
        \begin{tikzcd}
            M
                \ar[r, "\phi"]
                \ar[d, "\lambda_r^{\mathfrak{M}}", swap]
            & N
                \ar[d, "\lambda_r^{\mathfrak{N}}"]
            \\
            M
                \ar[r, "\phi", swap]
            & N
        \end{tikzcd}
    \end{equation*}
    %
    commute for all $r \in R$.
\end{remarkbreak}


\begin{remarkbreak}[$R$-algebras]
    We elaborate on Example~5.6 and the definition of an $R$-algebra in Definition~5.7. Given a ring $S$, left multiplication $\lambda \colon S \to \End_\catAb(S)$ is a ring homomorphism by Proposition~2.7. If $\alpha \colon R \to S$ is a ring homomorphism, we can equip $S$ with the $R$-module structure
    %
    \begin{equation*}
        \sigma \colon R \to \End_\catAb(S)
    \end{equation*}
    %
    given by $\sigma = \lambda \circ \alpha$. That is, $\sigma(r) = \lambda_{\alpha(r)}$ is multiplication in $S$ by $\alpha(r)$. Hence
    %
    \begin{equation*}
        \rho(r,s)
            = \sigma(r)(s)
            = \lambda_{\alpha(r)}(s)
            = \alpha(r)s,
    \end{equation*}
    %
    as seen in Example~5.6.

    In the case where $R$ is \emph{commutative}, Aluffi defines an $R$-algebra as a ring homomorphism $\alpha \colon R \to S$ such that $\alpha(R)$ lies in the centre of $S$. In this case we can also make $S$ into a module using the construction above, and multiplication in $S$ is furthermore $R$-bilinear: it is in this sense that the module structure $\sigma$ and the multiplication in $S$ are compatible.

    We can thus construct $R$-algebras by taking rings and equipping them with a compatible $R$-module structure, taking advantage of the ring structure to do so.

    In general we can think of an (associative) $R$-algebra $S$ as a ring that is also an $R$-module such that the ring and module structures are compatible. By this we mean that the ring addition and the module addition coincide, and that
    %
    \begin{equation}
        \label{eq:ring-module-agree}
        (rs)s' = r(ss') = s(rs')
    \end{equation}
    %
    for all $r \in R$ and $s,s' \in S$. To recover the definition in terms of ring homomorphisms, define a map $\alpha \colon R \to S$ by $\alpha(r) = r 1_S$, where $1_S$ is the ring identity in $S$. We easily see that $\alpha$ is a ring homomorphism. The map $\sigma \colon R \to \End_\catAb(S)$ given by $\sigma = \lambda \circ \alpha$ is thus also a ring homomorphism, and it thus defines an $R$-module structure on $S$. This is more explicitly given by
    %
    \begin{equation*}
        \rho(r,s)
            = \sigma(r)(s)
            = \lambda_{\alpha(r)}(s)
            = \alpha(r)s
            = (r 1_S) s
            = r (1_S s)
            = rs,
    \end{equation*}
    %
    which agrees with the original module structure on $S$. This uses the first equality in \cref{eq:ring-module-agree}. Finally we show that $\alpha(R)$ lies in the centre of $S$. For $r \in R$ and $s \in S$ we have
    %
    \begin{equation*}
        \alpha(r)s
            = (r 1_S)s
            = r (1_S s)
            = rs
    \end{equation*}
    %
    using the first equality in \cref{eq:ring-module-agree}, and the second equality yields
    %
    \begin{equation*}
        s \alpha(r)
            = s (r 1_S)
            = r (s 1_S)
            = rs.
    \end{equation*}
    %
    Thus $\alpha(r)$ and $s$ commute as claimed.
\end{remarkbreak}

\newcommand{\frakS}{\mathfrak{S}}

\begin{remarkbreak}[The category of $R$-algebras]
    Above we saw two ways of thinking of algebras over a commutative ring $R$. In fact we have at least four ways of thinking about $R$-algebras:
    %
    \begin{enumerate}
        \item An $R$-algebra is a ring homomorphism $\alpha \colon R \to S$ whose image $\alpha(R)$ lies in the centre of $S$. This (even without this latter assumption) induces a module structure on $S$ given by $\lambda \circ \alpha$, where $\lambda$ is left-multiplication in $S$. Hence an $R$-algebra is a ring equipped with an $R$-action.

        \item An $R$-algebra is simultaneously an $R$-module and a ring, and the two structures are compatible: The ring addition and the module addition coincide, and the $R$-action commutes with the ring multiplication. The latter precisely corresponds to the above requirement that $\alpha(R)$ lie in the centre of $S$.
        
        \item We may also consider $R$-algebras as algebraic structures in the sense of universal algebra. Thus an $R$-algebra is a structure
        %
        \begin{equation*}
            \frakS
                = \bigl\langle S, +, \,\cdot\,, -, 0, \langle \lambda_r \mid r \in R \rangle \bigr\rangle,
        \end{equation*}
        %
        such that $\langle S, +, \cdot, -, 0 \rangle$ is a ring, $\langle S, +, -, 0, \langle \lambda_r \mid r \in R \rangle \rangle$ is an $R$-module, and which satisfies the laws
        %
        \begin{equation*}
            \lambda_r(s) \cdot s'
                = \lambda_r(s \cdot s')
                = s \cdot \lambda_r(s')
        \end{equation*}
        %
        for all $r \in R$. This is essentially the same as the above.

        \item Finally we may construct the category $\catRAlg$ of $R$-algebras more categorically and see that we recover the above definition. Indeed we may define this category as the subcategory of the coslice category $R/\catRing$ whose objects (which are homomorphisms in $\catRing$) have the property that their image lies in the centre of their codomain.
        
        More explicitly, an object in $\catRAlg$ is a ring homomorphism $\alpha \colon R \to S$ such that $\alpha(R)$ lies in the centre of $S$, and an arrow from $\alpha$ to $\beta \colon R \to T$ in $\catRAlg$ is a commutative diagram
        %
        \begin{equation*}
            \begin{tikzcd}[column sep=small]
                & R
                    \ar[dl, "\alpha", swap]
                    \ar[dr, "\beta"]
                \\
                S
                    \ar[rr, "\phi", swap]
                && T
            \end{tikzcd}
        \end{equation*}
        %
        in $\catRing$. Since $\phi$ is a ring homomorphism, this means that
        %
        \begin{equation*}
            \phi(rs)
                = \phi(\alpha(r)s)
                = \phi(\alpha(r)) \phi(s)
                = \beta(r) \phi(s)
                = r \phi(s).
        \end{equation*}
        %
        Hence $\phi$ respects the module structures on $S$ and $T$. Conversely, if $\phi \colon S \to T$ respects both the ring and module structures on $S$ and $T$, then
        %
        \begin{equation*}
            \phi(\alpha(r)) \phi(s)
                = \phi(\alpha(r)s)
                = \phi(rs)
                = r \phi(s)
                = \beta(r) \phi(s),
        \end{equation*}
        %
        and letting $s = 1_S$ yields $\phi \circ \alpha = \beta$. Hence this definition of $\catRAlg$ is equivalent to the ones above.

        Furthermore, since $R$ is already assumed commutative, the category of \emph{commutative} $R$-algebras may simply be defined as the coslice category $R/\catCRing$.
    \end{enumerate}
\end{remarkbreak}


\begin{remarkbreak}[Every ring is a $\ints$-algebra]
    Just as every abelian group is a $\ints$-module, every ring $R$ is a $\ints$-algebra in a unique way: First of all, there is a unique ring homomorphism $\iota \colon \ints \to R$. Thus we only need to verify that $\iota(\ints)$ lies in the centre of $R$. But this is clear since the centre $Z(R)$ of $R$ is a subring (of course containing $1_R$), so
    %
    \begin{equation*}
        \iota(\ints)
            = \iota(\gen{1})
            = \gen{\iota(1)}
            = \gen{1_R}
            \subseteq Z(R).
    \end{equation*}
    %
    Hence the categories $\catRing$ and $\catAlg{\ints}$ are the same.
\end{remarkbreak}


\begin{remark}
    In ¬ß5.2 we see that $\Hom_\catRMod(M,N)$ is itself an $R$-module if $R$ is commutative. In fact, for any ring $R$ and any set $A$ the set of functions $N^A$ is a module with operations defined pointwise (recall from ¬ßII.4.4 that $H^A$ is a group if $H$ is a \emph{commutative} group, which is also the case here). Of course we may then restrict to module homomorphisms $M \to N$, but it is only when $R$ is commutative that this class is closed under scalar multiplication.
\end{remark}


\begin{exerciseframed}[3.5.4]
    Let $R$ be a ring. A nonzero $R$-module $M$ is \emph{simple} (or \emph{irreducible}) if its only submodules are $\{0\}$ and $M$. Let $M, N$ be simple modules, and let $\phi \colon M \to N$ be a homomorphism of $R$-modules. Prove that either $\phi = 0$ or $\phi$ is an isomorphism.
\end{exerciseframed}

\begin{solution}
    Assume that $\phi \neq 0$. The image of $\phi$ is a submodule of $N$, but since $\phi \neq 0$ it cannot be $\{0\}$. Hence it must be $N$, so $\phi$ is surjective. Similarly, the kernel of $\phi$ is a submodule of $N$, but since $\phi \neq 0$ it cannot be $M$. Thus it must be $\{0\}$, so $\phi$ is injective. In total, $\phi$ is bijective hence an isomorphism.
\end{solution}


\begin{exerciseframed}[3.5.5]
    Let $R$ be a commutative ring, viewed as an $R$-module over itself, and let $M$ be an $R$-module. Prove that $\Hom_\catRMod(R,M) \cong M$ as $R$-modules.
\end{exerciseframed}
%
Recall (cf. ¬ß5.3) that if $M$ and $N$ are modules over a (not necessarily commutative ring) $R$, then $\Hom_\catRMod(M,N)$ is an abelian group, but that we need $R$ to be commutative for this to also be a module in general.

\begin{solution}
    Define a map $\alpha \colon \Hom_\catRMod(R,M) \to M$ by $\alpha(\phi) = \phi(1_R)$. This is easily seen to be a module homomorphism. For $m \in M$ define a map $\beta_m \colon R \to M$ by $\beta_m(r) = rm$, which is clearly a module homomorphism, and further define $\beta \colon M \to \Hom_\catRMod(R,M)$ by $\beta(m) = \beta_m$. This is also a module homomorphism: For $m,m' \in M$ and $r,s \in R$ we have
    %
    \begin{equation*}
        \beta(rm+m')(s)
            = \beta_{rm+m'}(s)
            = s(rm+m')
            = r(sm) + sm'
            = (r\beta(m) + \beta(m'))(s),
    \end{equation*}
    %
    so $\beta(rm+m') = r\beta(m) + \beta(m')$.
    
    For $\phi \in \Hom_\catRMod(R,M)$ and $r \in R$ we have
    %
    \begin{equation*}
        \beta_{\phi(1_R)}(r)
            = r \phi(1_R)
            = \phi(r),
    \end{equation*}
    %
    so $\beta(\phi(1_R)) = \phi$. It follows that
    %
    \begin{equation*}
        (\beta \circ \alpha)(\phi)
            = \beta(\phi(1_R))
            = \phi.
    \end{equation*}
    %
    Conversely, for $m \in M$ we have
    %
    \begin{equation*}
        (\alpha \circ \beta)(m)
            = \alpha(\beta_m)
            = \beta_m(1_R)
            = 1_R m
            = m.
    \end{equation*}
    %
    Thus $\beta$ is the inverse (in $\catSet$) of $\alpha$, hence a module homomorphism. In total, $\alpha$ is an isomorphism in $\catRMod$.

    More simply put, each homomorphism $R \to M$ is multiplication by $m$ for some $m \in M$.
\end{solution}


\begin{exerciseframed}[3.5.6]
    Let $G$ be an abelian group. Prove that if $G$ has a structure of $\rationals$-vector space, then it has only one such structure.
\end{exerciseframed}



\begin{solution}
    A $\rationals$-vector space structure on $G$ is a ring homomorphism
    %
    \begin{equation*}
        \rationals
            \to \End_\catAb(G).
    \end{equation*}
    %
    Let $\sigma$ and $\tau$ be two such structures, and let $\iota \colon \ints \to \rationals$ be the unique ring homomorphism. Since there is also a unique ring homomorphism $\ints \to \End_\catAb(G)$, we must have $\sigma \circ \iota = \tau \circ \iota$. But $\iota$ is an epimorphism, so this implies that $\sigma = \tau$.
\end{solution}


\begin{exerciseframed}[3.5.7]
    Let $K$ be a field, and let $k \subseteq K$ be a subfield of $K$. Show that $K$ is a vector space over $k$ (and in fact a $k$-algebra) in a natural way.
\end{exerciseframed}

\begin{solution}
    Let $\iota \colon k \to K$ be the inclusion map. If $\mu \colon K \to \End_\catAb(K)$ is multiplication on $K$, then $\sigma = \mu \circ \iota$ is a $k$-module structure on $K$. Explicitly, this induces an action $\rho \colon k \prod K \to K$ given by
    %
    \begin{equation*}
        \rho(c,a)
            = \mu_{\iota(c)}(a)
            = \iota(c)a
            = ca.
    \end{equation*}
    %
    Since $K$ is a field, the image of $\iota$ obviously lies in the centre of $K$, so $K$ is a $k$-algebra.
\end{solution}


\begin{exerciseframed}[3.5.9]
    Let $R$ be a commutative ring, and let $M$ be an $R$-module.
    %
    \begin{enumerate}
        \item Prove that the operation of composition on the $R$-module $\End_\catRMod(M)$ makes the latter into an $R$-algebra in a natural way.
        \item Prove that $\calM_n(R)$ is an $R$-algebra, in a natural way.
    \end{enumerate}
\end{exerciseframed}

\begin{solution}
\begin{solutionsec}
    \item Recall that composition makes $\End_\catRMod(M) \subseteq \End_\catAb(M)$ into a ring, since composition preserves module homomorphisms. We equip $\End_\catRMod(M)$ with $R$-module structure using the method in Example~5.6: Define a map $\lambda \colon R \to \End_\catRMod(M)$ given by $\lambda(r) = \lambda_r$, where $\lambda_r(m) = rm$ for $r \in R$ and $m \in M$. This is clearly a ring homomorphism. This induces an action $\rho \colon R \prod \End_\catRMod(M) \to \End_\catRMod(M)$ given by
    %
    \begin{equation*}
        \rho(r,\phi)
            = \lambda(r) \circ \phi
            = \lambda_r \circ \phi
            = r \phi
    \end{equation*}
    %
    for $r \in R$ and $\phi \in \End_\catRMod(M)$. Furthermore, since $\phi$ is a module homomorphism we also have $\lambda_r \circ \phi = \phi \circ \lambda_r$, so the image of $\lambda$ lies in the centre of $\End_\catRMod(M)$.
\end{solutionsec}
\end{solution}


\begin{exerciseframed}[3.5.11]
    Let $R$ be a commutative ring, and let $M$ be an $R$-module. Prove that there is a bijection between the set of $R[x]$-module structures (extending the given $R$-module structure) on $M$ and $\End_\catRMod(M)$.
\end{exerciseframed}
%
This result says that, given an $R$-module structure on $M$, the only thing needed to extend this to an $R[x]$-module structure is to fix what the action of $x$ is. If we think of $R[x]$ the ring generated by $R$ along with an element $x$, it makes sense that the $R[x]$-module structure should be \enquote{generated} by $R$ and some endomorphism of $M$ (respecting the $R$-module structure, it turns out).

\begin{solution}
    The $R$-module structure on $M$ is a ring homomorphism
    %
    \begin{equation*}
        \sigma \colon R \to \End_\catAb(M).
    \end{equation*}
    %
    Given an $R$-module homomorphism $\phi \colon M \to M$, it is easy to see that $\phi \circ \sigma(r) = \sigma(r) \circ \phi$ for all $r \in R$. Example~2.3 then yields a unique ring homomorphism
    %
    \begin{equation*}
        \overline\sigma \colon R[x] \to \End_\catAb(M)
    \end{equation*}
    %
    extending $\sigma$ and sending $x$ to $\phi$, i.e. an $R[x]$-module structure on $M$ extending the given $R$-module structure $\sigma$. This defines a map
    %
    \begin{equation*}
        \Phi \colon \End_\catRMod(M)
            \to \Hom_\catRing(R[x], \End_\catAb(M))
    \end{equation*}
    %
    sending $\sigma$ to $\overline\sigma$, and by the uniqueness above $\Phi$ is injective.

    Conversely, let $\tau \colon R[x] \to \End_\catAb(M)$ be an $R[x]$-module structure on $M$ extending $\sigma$. Then $\phi = \tau(x) \colon M \to M$ is a ring homomorphism, and we claim that it is in fact an $R$-module homomorphism. For\footnote{The expression $rx$ denotes the product in $R[x]$ of the polynomials $r$ and $x$, and similarly for $xr$.} $rx = xr$ in $R[x]$ for all $r \in R$, so since $\tau$ is a ring homomorphism we have
    %
    \begin{equation*}
        \tau(r) \circ \phi
            = \tau(rx)
            = \tau(xr)
            = \phi \circ \tau(r).
    \end{equation*}
    %
    But then $\tau = \Phi(\phi)$ by the uniqueness of $\overline\sigma$ above, so $\Phi$ is also surjective, hence a bijection as claimed.
\end{solution}


\section[Products, coproducts, etc., in R-Mod][Products, coproducts, etc., in $\catRMod$]{Products, coproducts, etc., in $\catRMod$}

\begin{remarkbreak}[Free commutative $R$-algebras]
    In the proof of Proposition~6.4, Aluffi proves that, given a commutative $R$-algebra $S$ and a set function $f \colon A \to S$ from a finite set $A = \{1, \ldots, n\}$, there exists a unique \emph{ring homomorphism} $\phi \colon R[A] \to S$ such that the diagram
    %
    \begin{equation*}
        \begin{tikzcd}
            R[A]
                \ar[r, "\phi"]
            & S
            \\
            A
                \ar[u, "j"]
                \ar[ur, "f", swap]
        \end{tikzcd}
    \end{equation*}
    %
    commutes. He then claims that $\phi$ is automatically an $R$-module homomorphism. Indeed, Aluffi constructs $\phi$ by extending the structure $\alpha \colon R \to S$ making $S$ an $R$-algebra, and thus it is in fact an $R$-module homomorphism: Denoting the $R$-algebra structure on $R[A]$ by $\iota \colon R \to R[A]$ we find that
    %
    \begin{equation*}
        \phi(rm)
            = \phi(\iota(r)m)
            = \phi(\iota(r)) \phi(m)
            = \alpha(r) \phi(m)
            = r \phi(m)
    \end{equation*}
    %
    for $r \in R$ and $m \in R[A]$. Here we use that $\phi$ is a ring homomorphism, and that $\phi \circ \iota = \alpha$ since $\phi$ extends $\alpha$.

    However, this only shows that $\phi$ is unique among $R$-algebra homomorphisms that extend $\alpha$. If $\psi \colon R[A] \to S$ is any $R$-algebra homomorphism we have
    %
    \begin{equation*}
        \psi(r)
            = \psi(\iota(r) 1_{R[A]})
            = \psi(\iota(r)) \psi(1_{R[A]})
            = \alpha(r),
    \end{equation*}
    %
    now using that $\psi \circ \iota = \alpha$ since $\psi$ is an $R$-algebra homomorphism. But then we see that $\psi$ must extend $\alpha$, and so $\phi$ above is indeed unique.
\end{remarkbreak}


\begin{exerciseframed}[3.6.3]
    Let $R$ be a ring, $M$ an $R$-module, and $p \colon M \to M$ an $R$-module homomorphism such that $p^2 = p$. (Such a map is called a projection.) Prove that $M = \ker p \oplus \im p$.
\end{exerciseframed}

\begin{solution}
    Consider the map $\ker p \oplus \im p \to M$ given by $(n,m) \mapsto n + m$, which is easily seen to be a homomorphism. It has the inverse $m \mapsto (m - p(m), p(m))$, hence is an isomorphism.
\end{solution}


\begin{exerciseframed}[3.6.4]
    Let $R$ be a ring, and let $n > 1$. View $R^{\oplus (n-1)}$ as a submodule of $R^{\oplus n}$, via the injective homomorphism $R^{\oplus (n-1)} \hookrightarrow R^{\oplus n}$ defined by
    %
    \begin{equation*}
        (r_1, \ldots, r_{n-1})
            \mapsto (r_1, \ldots, r_{n-1}, 0)
    \end{equation*}
    %
    Give a one-line proof that
    %
    \begin{equation*}
        \frac{R^{\oplus n}}{R^{\oplus (n-1)}}
            \cong R.
    \end{equation*}
\end{exerciseframed}

\begin{solution}
    This follows from the first isomorphism theorem since the projection $\pi_n \colon R^{\oplus n} \to R$ given by $\pi_n(r_1, \ldots, r_n) = r_n$ is surjective and has kernel $R^{\oplus (n-1)}$.
\end{solution}


\begin{exerciseframed}[3.6.9]
    Let $R$ be a ring, $F$ a nonzero free $R$-module, and let $\phi \colon M \to N$ be a homomorphism of $R$-modules. Prove that $\phi$ is onto if and only if for all $R$-module homomorphisms $\alpha \colon F \to N$ there exists an $R$-module homomorphism $\beta \colon F \to M$ such that $\alpha = \phi \circ \beta$:
    %
    \begin{equation*}
        \begin{tikzcd}[column sep=tiny]
            & F
                \ar[dl, "\beta", swap, dashed]
                \ar[dr, "\alpha"]
            \\
            M
                \ar[rr, "\phi", swap, two heads]
            && N
        \end{tikzcd}
    \end{equation*}
    %
    (Free modules are \emph{projective}, as we will see in Chapter~VIII. See also \cref{rem:projective-modules}.)
\end{exerciseframed}

\begin{solution}
    Say that $F = F^R(A)$ for a nonempty set $A$. Assume first that $\phi$ is surjective, and let $\alpha \colon F \to N$ be a homomorphism. For each $n \in \im\alpha$, choose some $m_n \in \phi\preim(\{n\})$, and let $A_n = \alpha\preim(\{n\})$. Notice that the sets $A_n$ constitute a partition of $A$. Then define a set function $f \colon A \to M$ by $f(a) = m_n$ for $a \in A_n$. This extends to a homomorphism $\beta \colon F \to M$. Then notice that, for $a \in A_n$,
    %
    \begin{equation*}
        \phi(\beta(a))
            = \phi(f(a))
            = \phi(m_n)
            = n
            = \alpha(a),
    \end{equation*}
    %
    so $\phi \circ \beta$ and $\alpha$ agree on $A$, hence on $F$ by the uniqueness part of the universal property of $F$.

    Conversely, let $n \in N$ and define a set function $g \colon A \to N$ by $g(a) = n$ for all $a \in A$. By the universal property of $F$ there exists an $R$-module homomorphism $\alpha \colon F \to N$ extending $g$. Then there further exists a homomorphism $\beta \colon F \to M$ such that $\alpha = \phi \circ \beta$, so $n$ lies in the image of $\phi$. Since $n$ was arbitrary, this shows that $\phi$ is surjective.
\end{solution}


\begin{exerciseframed}[3.6.13]
    Prove that every homomorphic image of a finitely generated module is finitely generated.
\end{exerciseframed}

\begin{solution}
    Let $M$ be a finitely generated $R$-module, and let $\phi \colon M \to N$ be a homomorphism. We may assume that $\phi$ is surjective. Since $M$ is finitely generated, there is an $n \in \naturals$ and a surjective homomorphism $R^{\oplus n} \twoheadrightarrow M$. Composing this with $\phi$ yields a surjective homomorphism $R^{\oplus n} \twoheadrightarrow N$, so $N$ is also finitely generated.
\end{solution}


\begin{exerciseframed}[3.6.14]
    Prove that the ideal $(x_1, x_2, \ldots)$ of the ring $R = \ints[x_1, x_2, \ldots]$ is not finitely generated (as an ideal, i.e., as an $R$-module).
\end{exerciseframed}

\begin{solution}
    Let $A$ be a finite subset of $R$ that generates a module contained in $(x_1, x_2, \ldots)$. In particular, $A$ cannot contain $1$. Furthermore, since $A$ is finite and each element of $A$ is a polynomial, there is a largest $n \in \naturals$ such that $x_n$ appears in a polynomial in $A$. Then it is clear that $x_{n+1} \not\in (x_1, x_2, \ldots)$ since $1 \not\in A$.
\end{solution}


\begin{exerciseframed}[3.6.18]
    Let $M$ be an $R$-module, and let $N$ be a submodule of $M$. Prove that if $N$ and $M/N$ are both finitely generated, then $M$ is finitely generated.
\end{exerciseframed}

\begin{solution}
    Choose $a_1, \ldots, a_m, b_1, \ldots, a_n \in M$ such that
    %
    \begin{equation*}
        \frac{M}{N}
            = \gen{a_1 + N, \ldots, a_m + N}
        \quad \text{and} \quad
        N
            = \gen{b_1, \ldots, b_n}.
    \end{equation*}
    %
    For $m \in M$ we thus have
    %
    \begin{equation*}
        m + N
            = \sum_{i=1}^m r_i(a_i + N)
            = \sum_{i=1}^m r_i a_i + N,
    \end{equation*}
    %
    so $m - \sum_{i=1}^m r_i a_i \in N$. Hence
    %
    \begin{equation*}
        m
            = \sum_{i=1}^m r_i a_i + \sum_{j=1}^n s_i b_i,
    \end{equation*}
    %
    so $M = \gen{a_1, \ldots, a_m, b_1, \ldots, a_n}$.
\end{solution}


\section{Complexes and homology}

\begin{remarkbreak}[Split exact sequences]
    \label{rem:split-exact-sequences}
    If
    %
    \begin{equation}
        \label{eq:short-exact-seq-remark}
        \begin{tikzcd}
            0
                \ar[r]
            & L
                \ar[r, "\alpha"]
            & M
                \ar[r, "\beta"]
            & N
                \ar[r]
            & 0
        \end{tikzcd}
    \end{equation}
    %
    is exact, then $L \cong \ker\beta$ and $N \cong \coker\alpha$. On the other hand we may immediately extend the exact sequence
    %
    \begin{equation*}
        \begin{tikzcd}
            0
                \ar[r]
            & L
                \ar[r, "\alpha"]
            & M,
        \end{tikzcd}
    \end{equation*}
    %
    which precisely says that $\alpha$ is injective, to the exact sequence
    %
    \begin{equation*}
        \begin{tikzcd}
            0
                \ar[r]
            & L
                \ar[r, "\alpha"]
            & M
                \ar[r]
            & \coker\alpha
                \ar[r]
            & 0.
        \end{tikzcd}
    \end{equation*}
    %
    These then carry exactly the same information, assuming that $\alpha$ is injective. But this extended sequence also allows us to talk about splitting, so we have ways of characterising both monomorphisms and split monomorphisms using short exact sequences. Of course we may do similarly with epimorphisms.

    Since any sequence on the form \cref{eq:short-exact-seq-remark} is isomorphic to one on the form
    %
    \begin{equation*}
        \begin{tikzcd}
            0
                \ar[r]
            & \ker\beta
                \ar[r, "\alpha"]
            & M
                \ar[r, "\beta"]
            & \coker\alpha
                \ar[r]
            & 0,
        \end{tikzcd}
    \end{equation*}
    %
    the content of Proposition~7.5 is in fact that this splits iff $\alpha$ has a left-inverse iff $\beta$ has a right-inverse.
\end{remarkbreak}


\newcommand{\catC}{\cat{C}}

\begin{remarkbreak}[Projective modules]
    \label{rem:projective-modules}
    In any category $\catC$, an object $P$ is said to be \emph{projective} if, given an epimorphism $e \colon A \twoheadrightarrow B$, for any arrow $f \colon P \to B$ there is an arrow $\overline{f} \colon P \to A$ such that $f = e \circ \overline{f}$, i.e. such that the diagram
    %
    \begin{equation*}
        \begin{tikzcd}[column sep=tiny]
            & P
                \ar[dl, "\overline{f}", swap, dashed]
                \ar[dr, "f"]
            \\
            A
                \ar[rr, "e", swap, two heads]
            && B
        \end{tikzcd}
    \end{equation*}
    %
    commutes. Note that $\overline{f}$ is not required to be unique; this is not a universal property of $P$. In particular, if $B = P$ then $e$ has a right-inverse, namely $\overline{1}_A$.

    In $\catRMod$, the following are equivalent for an $R$-module $P$:
    %
    \begin{enumerate}
        \item $P$ is projective in $\catRMod$.
        
        \item Every short exact sequence
        %
        \begin{equation*}
            \begin{tikzcd}
                0 \ar[r]
                & M \ar[r]
                & N \ar[r]
                & P \ar[r]
                & 0
            \end{tikzcd}
        \end{equation*}
        %
        splits.
        
        \item $P$ is a direct summand of a free module, i.e. there exist $R$-modules $F$ and $Q$, with $F$ free, such that $F \cong P \oplus Q$.
    \end{enumerate}
    
    \begin{proofsec}
        \item[(a) $\implies$ (b)]
        By Proposition~7.5, the sequence above splits happens exactly when the homomorphism $\beta \colon N \to P$ has a right-inverse. But since $\beta$ is an epimorphism, we noted above that it has a right-inverse when $P$ is projective.

        \item[(b) $\implies$ (c)]
        Next assume that any such sequence splits. Consider the free module $R^{\oplus P}$ and the surjection $\phi \colon R^{\oplus P} \to P$ induced by the set function $\id_P$. As in \cref{rem:split-exact-sequences} this gives rise to the short exact sequence
        %
        \begin{equation*}
            \begin{tikzcd}
                0 \ar[r]
                & \ker\phi \ar[r]
                & R^{\oplus P} \ar[r, "\phi"]
                & P \ar[r]
                & 0,
            \end{tikzcd}
        \end{equation*}
        %
        and this splits by assumption, so $R^{\oplus P} \cong P \oplus \ker\phi$.

        \item[(c) $\implies$ (a)]
        Finally assume that $F \cong P \oplus Q$, let $\phi \colon M \to N$ be an epimorphism, and let $\psi \colon P \to N$ be any homomorphism. We must then find a $\overline{\psi} \colon P \to M$ so that the diagram
        %
        \begin{equation*}
            \begin{tikzcd}[column sep=tiny]
                & P
                    \ar[dl, "\overline{\psi}", swap, dashed]
                    \ar[dr, "\psi"]
                \\
                M
                    \ar[rr, "\phi", swap, two heads]
                && N
            \end{tikzcd}
        \end{equation*}
        %
        commutes. Since $F$ is free it is projective by \exref{3.6.9}, so there is a homomorphism $\chi \colon P \oplus Q \to M$ such that $\phi \circ \chi$ equals the coproduct map $[\psi, 0]$, where $0 \colon Q \to N$ is the zero homomorphism (though any homomorphism will do). Letting $\iota \colon P \to P \oplus Q$ be the canonical injection and putting $\overline{\psi} = \chi \circ \iota$, notice that the diagram
        %
        \begin{equation*}
            \begin{tikzcd}[column sep=tiny]
                & P
                    \ar[d, "\iota"]
                    \ar[ddl, "\overline{\psi}", swap, bend right]
                    \ar[ddr, "\psi", bend left]
                \\
                & P \oplus Q
                    \ar[dl, "\chi", swap]
                    \ar[dr, "{[\psi, 0]}" {xshift=-5pt}]
                \\
                M
                    \ar[rr, "\phi", swap, two heads]
                && N
            \end{tikzcd}
        \end{equation*}
        %
        commutes, so $\phi \circ \overline{\psi} = \psi$ as desired.
    \end{proofsec}
\end{remarkbreak}


\begin{exerciseframed}[3.7.1]
    Assume that the complex
    %
    \begin{equation*}
        \begin{tikzcd}
            \cdots \ar[r]
            & 0 \ar[r]
            & M \ar[r]
            & 0 \ar[r]
            & \cdots
        \end{tikzcd}
    \end{equation*}
    %
    is exact. Prove that $M \cong 0$.
\end{exerciseframed}

\begin{solution}
    The kernel of the map $M \to 0$ is $M$ itself, and this equals the image of the map $0 \to M$. Hence $M$ must be zero.
\end{solution}


\begin{exerciseframed}[3.7.2]
    Assume that the complex
    %
    \begin{equation*}
        \begin{tikzcd}
            \cdots \ar[r]
            & 0 \ar[r, "\alpha"]
            & M \ar[r, "\phi"]
            & M' \ar[r, "\beta"]
            & 0 \ar[r]
            & \cdots
        \end{tikzcd}
    \end{equation*}
    %
    is exact. Prove that $M \cong M'$
\end{exerciseframed}

\begin{solution}
    We have $\ker\phi = \im\alpha = 0$, so $\phi$ is injective. We further have $\im\phi = \ker\beta = M'$ since $\beta$ is trivial, so $\phi$ is surjective.

    Alternatively we may simply apply Examples~7.1 and 7.2.
\end{solution}


\begin{exerciseframed}[3.7.3]
    Assume that the complex
    %
    \begin{equation*}
        \begin{tikzcd}
            \cdots \ar[r]
            & 0 \ar[r, "\alpha"]
            & L \ar[r, "\beta"]
            & M \ar[r, "\phi"]
            & M' \ar[r, "\gamma"]
            & N \ar[r, "\delta"]
            & 0 \ar[r]
            & \cdots
        \end{tikzcd}
    \end{equation*}
    %
    is exact. Show that, up to natural identifications, $L = \ker\phi$ and $N = \coker\phi$.
\end{exerciseframed}

\begin{solution}
    Notice that $\ker\beta = \im\alpha = 0$, so $L \cong \im\beta = \ker\phi$. Furthermore, $\im\gamma = \ker\delta = N$, and
    %
    \begin{equation*}
        N
            = \im\gamma
            \cong \frac{M'}{\ker\gamma}
            = \frac{M'}{\im\phi}
            = \coker\phi.
    \end{equation*}
\end{solution}


\begin{exerciseframed}[3.7.5]
    Assume that the complex
    %
    \begin{equation*}
        \begin{tikzcd}
            \cdots \ar[r]
            & L \ar[r]
            & M \ar[r]
            & N \ar[r]
            & \cdots
        \end{tikzcd}
    \end{equation*}
    %
    is exact and that $L$ and $N$ are Noetherian. Prove that $M$ is Noetherian.
\end{exerciseframed}

\begin{solution}
    The discussion in ¬ß7.1 shows that $N \cong M/L$, so this follows directly from Proposition~6.7.
\end{solution}


\begin{exerciseframed}[3.7.7]
    Let
    %
    \begin{equation*}
        \begin{tikzcd}
            0
                \ar[r]
            & M
                \ar[r, "\alpha"]
            & N
                \ar[r, "\beta"]
            & P
                \ar[r]
            & 0
        \end{tikzcd}
    \end{equation*}
    %
    be a short exact sequence of $R$-modules, and let $L$ be an $R$-module.
    %
    \begin{enumerate}
        \item Prove that there is an exact sequence
        %
        \begin{equation*}
            \begin{tikzcd}
                0
                    \ar[r]
                & \Hom_R(P,L)
                    \ar[r]
                & \Hom_R(N,L)
                    \ar[r]
                & \Hom_R(M,L)
            \end{tikzcd}
        \end{equation*}
        %
        of $R$-modules if $R$ is commutative, and of abelian groups otherwise.

        \item TODO
        \item TODO
        
        \item Show that if the original sequence splits, then the rightmost homomorphism in (i) is onto. % TODO reference
    \end{enumerate}
\end{exerciseframed}

\begin{solution}
\begin{solutionsec}
    \item We claim that the sequence
    %
    \begin{equation*}
        \begin{tikzcd}
            0
                \ar[r]
            & \Hom_R(P,L)
                \ar[r, "\beta^*"]
            & \Hom_R(N,L)
                \ar[r, "\alpha^*"]
            & \Hom_R(M,L)
        \end{tikzcd}
    \end{equation*}
    %
    is exact, where $\beta^*$ and $\alpha^*$ are the pullbacks of $\beta$ and $\alpha$ respectively. We first show that $\beta^*$ is a homomorphism of abelian groups. For $\phi, \psi \in \Hom_R(P,L)$ and $n \in N$ we have
    %
    \begin{equation*}
        \beta^*(\phi + \psi)
            = (\phi + \psi) \circ \beta(n)
            = \phi(\beta(n)) + \psi(\beta(n))
            = (\beta^*\phi + \beta^*\psi)(n)
    \end{equation*}
    %
    as claimed. Assuming that $R$ is commutative and $r \in R$, the map $r\phi$ is also an $R$-module homomorphism. In this case we have
    %
    \begin{equation*}
        \beta^*(r\phi)(n)
            = (r\phi) \circ \beta(n)
            = r\phi(\beta(n))
            = r(\beta^*\phi)(n)
    \end{equation*}
    %
    as desired. Similarly for $\alpha^*$.

    Next note that $\beta^*$ is injective, proving exactness at $\Hom_R(P,L)$:
    %
    \begin{equation*}
        \phi \circ \beta
            = \beta^*\phi
            = \beta^*\psi
            = \psi \circ \beta,
    \end{equation*}
    %
    which implies that $\phi = \psi$ since $\beta$ is surjective.

    Finally we show exactness at $\Hom_R(N,L)$. First notice that
    %
    \begin{equation*}
        (\alpha^* \circ \beta^*)(\psi)
            = \psi \circ \beta \circ \alpha
            = \psi \circ 0,
    \end{equation*}
    %
    showing that $\im\beta^* \subseteq \ker\alpha^*$. For the opposite inclusion, let $\psi \in \ker\alpha^*$. Then $\im\alpha \subseteq \ker\psi$, so since
    %
    \begin{equation*}
        P
            \cong \frac{N}{\ker\beta}
            = \frac{N}{\im\alpha},
    \end{equation*}
    %
    by the universal property of quotients $\psi$ factors (uniquely) through $\beta$, so $\psi = \phi \circ \beta$ for some $\phi \colon P \to L$. That is, the diagram
    %
    \begin{equation*}
        \begin{tikzcd}
            M
                \ar[r, "\alpha"]
                \ar[dr, "0", swap]
            & N
                \ar[r, "\beta"]
                \ar[d, "\psi"]
            & P
                \ar[dl, "\phi"]
            \\
            & L
        \end{tikzcd}
    \end{equation*}
    %
    commutes. Thus $\psi = \beta^*\phi$, so $\psi \in \im\beta^*$, showing exactness.

    \item 
    \item 

    \item If the original sequence splits, then $\alpha$ has a left-inverse $\gamma$ by Proposition~7.5. For $\chi \in \Hom_R(M,L)$ we have
    %
    \begin{equation*}
        \alpha^*(\chi \circ \gamma)
            = (\chi \circ \gamma) \circ \alpha
            = \chi \circ (\gamma \circ \alpha)
            = \chi,
    \end{equation*}
    %
    so $\alpha^*$ is surjective.
\end{solutionsec}
\end{solution}


\begin{exerciseframed}[3.7.8]
    Prove that every exact sequence
    %
    \begin{equation*}
        \begin{tikzcd}
            0
                \ar[r]
            & M
                \ar[r]
            & N
                \ar[r]
            & F
                \ar[r]
            & 0
        \end{tikzcd}
    \end{equation*}
    %
    of $R$-modules, with $F$ \emph{free}, splits.
\end{exerciseframed}

\begin{solution}
    Since the map $\phi \colon N \to F$ above is surjective, \exref{3.6.9} (with $\alpha = \id_F$) yields an $R$-module homomorphism $\beta \colon F \to N$ such that $\id_F = \phi \circ \beta$. That is, $\phi$ has a right-inverse, so Proposition~7.5 implies that the sequence splits.
\end{solution}


\begin{exerciseframed}[3.7.9]
    Let
    %
    \begin{equation*}
        \begin{tikzcd}
            0
                \ar[r]
            & M
                \ar[r]
            & N
                \ar[r]
            & F
                \ar[r]
            & 0
        \end{tikzcd}
    \end{equation*}
    %
    be a short exact sequence of $R$-modules, with $F$ free, and let $L$ be an $R$-module. Prove that there is an exact sequence
    %
    \begin{equation*}
        \begin{tikzcd}
            0
                \ar[r]
            & \Hom_R(F,L)
                \ar[r]
            & \Hom_R(N,L)
                \ar[r]
            & \Hom_R(M,L)
                \ar[r]
            & 0.
        \end{tikzcd}
    \end{equation*}
\end{exerciseframed}

\begin{solution}
    \exref{3.7.7} already yields an exact sequence
    %
    \begin{equation*}
        \begin{tikzcd}
            0
                \ar[r]
            & \Hom_R(F,L)
                \ar[r]
            & \Hom_R(N,L)
                \ar[r]
            & \Hom_R(M,L).
        \end{tikzcd}
    \end{equation*}
    %
    By \exref{3.7.8} the first sequence above splits, so \exref{3.7.7} again implies that the homomorphism $\Hom_R(N,L) \to \Hom_R(M,L)$ is surjective, yielding the rest of the sequence.
\end{solution}


\begin{exerciseframed}[3.7.11]
    Let
    %
    \begin{equation}
        \label{eq:split-exact-exercise}
        \begin{tikzcd}
            0
                \ar[r]
            & M_1
                \ar[r]
            & N
                \ar[r]
            & M_2
                \ar[r]
            & 0
        \end{tikzcd}
    \end{equation}
    %
    be an exact sequence of $R$-modules. (This may be called an \enquote{extension} of $M_2$ by $M_1$.) Suppose there is \emph{any} $R$-module homomorphism $\phi \colon N \to M_1 \oplus M_2$ making the diagram
    %
    \begin{equation*}
        \begin{tikzcd} % TODO the equals signs have a white background, fix
            0
                \ar[r]
            & M_1
                \ar[r, "\alpha"]
                \ar[d, equal]
            & N
                \ar[r, "\beta"]
                \ar[d, "\phi", dashed]
            & M_2
                \ar[r]
                \ar[d, equal]
            & 0
            \\
            0   
                \ar[r]
            & M_1
                \ar[r, "\iota", swap]
            & M_1 \oplus M_2
                \ar[r, "\pi_2", swap]
            & M_2
                \ar[r]
            & 0
        \end{tikzcd}
    \end{equation*}
    %
    commute, where the bottom sequence is the standard sequence of a direct sum. Prove that \cref{eq:split-exact-exercise} splits.
\end{exerciseframed}

\begin{solution}
    It suffices to show that $\phi$ is bijective. To prove injectivity, let $n \in \ker\phi$. Then $\phi(n) = 0 = \iota(0)$, so $n$ lies in the image of $\alpha$. Pick $m \in M_1$ such that $\alpha(m) = n$, and notice that
    %
    \begin{equation*}
        \iota(m)
            = \phi(\alpha(m))
            = \phi(n)
            = \iota(0),
    \end{equation*}
    %
    and since $\iota$ is injective, we have $m = 0$.

    To prove surjectivity, let $m_1 \in M_1$ and $m_2 \in M_2$. Since $\beta$ is surjective there exists an $n' \in N$ such that $\beta(n') = m_2$. Now let $n = \alpha(m_1 - \pi_1(\phi(n'))) + n'$ and notice that
    %
    \begin{align*}
        \phi(n)
            &= \iota \bigl( m_1 - \pi_1(\phi(n')) \bigr) + \phi(n') \\
            &= \bigl( m_1 - \pi_1(\phi(n')), 0 \bigr) + \bigl( \pi_1(\phi(n')), \pi_2(\phi(n')) \bigr) \\
            &= \bigl( m_1, \pi_2(\phi(n')) \bigr) \\
            &= (m_1, \beta(n')) \\
            &= (m_1, m_2).
    \end{align*}
    %
    Hence $\phi$ is also surjective and thus an isomorphism.
\end{solution}



\chapter{Groups, second encounter}

\section{The conjugation action}

\begin{remarkbreak}[Normalisers and centralisers]
    Let $G$ be a group. For $g \in G$ we recall the inner automorphism $\gamma_g \colon G \to G$ given by $\gamma_g(a) = gag\inv$. This induces a map $\Gamma_g \colon 2^G \to 2^G$ given by $\Gamma_g(A) = \gamma_g(A) = gAg\inv$, and the map $g \mapsto \Gamma_g$ is then an action on $2^G$. The stabiliser subgroup of a set $A \subseteq G$ with respect to this action is called the \emph{normaliser} of $A$, denoted $N_G(A)$. More explicitly we have
    %
    \begin{align*}
        N_G(A)
            &= \set{g \in G}{\Gamma_g(A) = A} \\
            &= \set{g \in G}{gAg\inv = A}.
    \end{align*}
    %
    If $A$ is a singleton $\{a\}$, then we simply write $N_G(a)$ for its normaliser. If $g \in N_G(A)$ then $gAg\inv \subseteq A$. In other words, the set $A$ is invariant under $\gamma_g$. Since $\gamma_g$ is injective, if $A$ is finite then the above inclusion is also sufficient for $g$ to lie in $N_G(A)$, but this is not the case in general.

    A subgroup $H$ of $G$ is a normal subgroup of the normaliser $N_G(H)$. In fact, $N_G(H)$ is the largest subgroup of $G$ in which $H$ is normal. It follows that $H$ is normal in $G$ if and only if $N_G(H) = G$. The orbit of $H$ under the action on $2^G$ is the set $[H]$ of subgroups of $H$ conjugate to $H$, i.e. its conjugacy class. The orbit-stabiliser theorem thus yields a bijection $G/N_G(H) \cong_\catSet [H]$. If finite, the number of subgroups conjugate to $H$ is thus the index $[G : N_G(H)]$ (this is Lemma~1.13).

    If $g \in N_G(A)$ then $A$ is invariant under $\gamma_g$, so $\gamma_g$ restricts to a well-defined map $\gamma_g|_A \colon A \to A$. The \emph{centraliser} of $A$ is the subset of $N_G(A)$ of elements $g$ such that $\gamma_g|_A$ is the identity on $A$, i.e.
    %
    \begin{align*}
        Z_G(A)
            &= \set{g \in N_G(A)}{\gamma_g|_A = 1_A} \\
            &= \set{g \in G}{\forall a \in A \colon gag\inv = a}.
    \end{align*}
    %
    For a singleton $\{a\}$ we write $Z_G(a)$ for its centraliser. Notice that $Z_G(a) = N_G(a)$, and that this is the stabiliser of $a$ under the conjugation action on $G$. If $\{A_i\}_{i \in I}$ is a family of subsets of $G$, then we clearly have
    %
    \begin{equation*}
        Z_G \Bigl( \bigunion_{i \in I} A_i \Bigr)
            = \bigintersect_{i \in I} Z_G(A_i).
    \end{equation*}
    %
    In particular we have
    %
    \begin{equation*}
        Z_G(A)
            = \bigintersect_{a \in A} Z_G(a),
    \end{equation*}
    %
    and so $Z_G(A)$ is a subgroup of $G$. Also notice that if $A \subseteq B \subseteq G$, then $Z_G(B) \subseteq Z_G(A)$. In particular,
    %
    \begin{equation*}
        \bigunion_{i \in I} Z_G(A_i)
            \subseteq Z_G \Bigl( \bigintersect_{i \in I} A_i \Bigr).
    \end{equation*}
    %
    The opposite inclusion probably (clearly?) doesn't hold in general, since the union of subgroups isn't necessarily a subgroup. Perhaps there is a closer relationship between the two sides of the inclusion, I don't know.

    The \emph{centre} of $G$ is the centraliser $Z_G(G)$, denoted $Z(G)$. Equivalently, this is the kernel of the action $G \to \Aut_\catGrp(G)$. Notice that this is the set of fixed points of the conjugation action on $G$, and contains the elements that commute with \emph{all} elements in $G$.
\end{remarkbreak}


\begin{exerciseframed}[4.1.1]
    Let $p$ be a prime integer, let $G$ be a $p$-group, and let $S$ be a finite set such that $\card{S} \not\equiv 0 \pmod p$. If $G$ acts on $S$, prove that the action must have fixed points.
\end{exerciseframed}

\begin{solution}
    If $Z$ denotes the set of fixed points of the action, then Corollary~1.3 implies that
    %
    \begin{equation*}
        \card{Z}
            \equiv \card{S}
            \not\equiv 0
            \mod p.
    \end{equation*}
    %
    In particular, $\card{Z} \neq 0$, so the action has fixed points.

    (Remark: Notice that we cannot use this type of argument to conclude from $\card{S} \equiv 0 \pmod p$ that the action has \emph{no} fixed points: The number of fixed points just has to be a multiple of $p$.)
\end{solution}


\begin{exerciseframed}[4.1.4]
    Let $G$ be a group, and let $N$ be a subgroup of $Z(G)$. Prove that $N$ is normal in $G$.
\end{exerciseframed}

\begin{solution}
    For $g \in G$ and $n \in N$ we have $gn = ng$. But then $gN = Ng$, so $N$ is normal in $G$.
\end{solution}


\begin{exerciseframed}[4.1.5]
    Let $G$ be a group. Prove that $G/Z(G)$ is isomorphic to the group $\Inn(G)$ of inner automorphisms of $G$. (Cf. \exref{2.4.8}.) Then prove Lemma~1.5 again by using the result of \exref{2.6.7}.
\end{exerciseframed}

\begin{solution}
    For $g \in G$ let $\gamma_g \colon G \to G$ be the corresponding inner automorphism given by $\gamma_g(a) = gag\inv$. Define a map $\phi \colon G \to \Inn(G)$ by $\phi(g) = \gamma_g$. Notice that $\gamma_g$ is the identity on $G$ exactly when $g$ commutes with every element of $G$, i.e. when $g \in Z(G)$. Hence $\ker\phi = Z(G)$, and since $\phi$ is surjective the first isomorphism theorem implies that $G/Z(G) \cong \Inn(G)$ as claimed.

    By \exref{2.6.7} we know that $\Inn(G)$ is trivial if it is cyclic. If $G/Z(G)$ is cyclic the above thus implies that it is in fact trivial, and hence that $G = Z(G)$, i.e. that $G$ is commutative.
\end{solution}


\begin{exerciseframed}[4.1.6]
    Let $p, q$ be prime integers, and let $G$ be a group of order $pq$. Prove that either $G$ is commutative or the center of $G$ is trivial. Conclude (using Corollary~1.9) that every group of order $p^2$, for a prime $p$, is commutative.
\end{exerciseframed}

\begin{solution}
    Assume that the centre $Z(G)$ is nontrivial. Since it is a subgroup of $G$, Lagrange's theorem implies that $\card{Z(G)}$ divides $\card{G} = pq$, so assume that $\card{Z(G)} = p$. Hence $[G : Z(G)] = q$, but then Example~II.8.16 implies that $G/Z(G)$ is cyclic. It follows from Lemma~1.5 that $G$ is commutative.

    In the case $p = q$, $G$ is a $p$-group so Corollary~1.9 implies that its centre is nontrivial. The above then shows that in fact $Z(G) = G$.
\end{solution}


\begin{exerciseframed}[4.1.8]
    Let $p$ be a prime number, and let $G$ be a $p$-group: $\card{G} = p^r$. Prove that $G$ contains a normal subgroup of order $p^k$ for every nonnegative $k \leq r$. 
\end{exerciseframed}

\begin{solution}
    We argue by induction on $r$. If $r = 0$ then this is obvious, so assume that $r > 0$. The centre of $G$ is nontrivial by Corollary~1.9 and a $p$-group, and since $Z(G)$ is commutative it contains an element $h$ of order $p$ by \exref{2.8.17}. Then $\gen{h}$ is a subgroup of $Z(G)$ of order $p$, hence is normal in $G$ by \exref{4.1.4}. The quotient group $G/\gen{h}$ is of order $p^{r-1}$, so by induction it has a normal subgroup $\tilde H_k$ of order $p^k$ for $k = 0, \ldots, r-1$. By Proposition~8.9 each $\tilde H_k$ is on the form $H_k/\gen{h}$ for some subgroup $H_k$ of $G$, and the third isomorphism theorem implies that $H_k$ is normal in $G$. Also notice that
    %
    \begin{equation*}
        \ord{H_k}
            = [H_k : \gen{g}] \, \ord{g}
            = \ord{\tilde H_k} \, \ord{g}
            = p^k p
            = p^{k+1}.
    \end{equation*}
    %
    Hence $G$ has normal subgroups of order $p^k$ for $k = 1, \ldots, r$ as desired.
\end{solution}


\begin{exerciseframed}[4.1.9]
    Let $p$ be a prime number, $G$ a $p$-group, and $H$ a nontrivial normal subgroup of $G$. Prove that $H \intersect Z(G) \neq \{e\}$.
\end{exerciseframed}

\begin{solution}
    Notice that $H$ must also be a $p$-group. Also notice that $H$ is a union of conjugacy classes since it is normal, and the order of each nontrivial conjugacy class divides the order of $G$, so they are divisible by $p$. It follows that the number of fixed points in $H$ is also divisible by $p$, which proves the claim.
\end{solution}


\begin{exerciseframed}[4.1.14]
    Let $G$ be a group, and assume $[G : Z(G)] = n$ is finite. Let $A \subseteq G$ be any subset. Prove that the number of conjugates of $A$ is at most $n$.
\end{exerciseframed}

\begin{solution}
    Notice that the conjugates of $A$ are precisely the images $\gamma_g(A) = gAg\inv$ of $A$ under inner automorphisms for all $g \in G$. The number of conjugates of $A$ are thus at most the number of inner automorphisms, and since $G/Z(G) \cong \Inn(G)$ by \exref{4.1.5}, this number is $n$.
\end{solution}


\begin{exerciseframed}[4.1.17]
    Let $H$ be a proper subgroup of a finite group $G$. Prove that $G$ is not the union of the conjugates of $H$.
\end{exerciseframed}

\begin{solution}
    By Lemma~1.13, the number of conjugates of $H$ is $[G : N_G(H)]$. Since each of these conjugates overlap (at least in the identity), the number of elements in the union of the conjugates of $H$ is \emph{strictly} less than
    %
    \begin{equation*}
        [G : N_G(H)] \, \card{H}
            \leq [G : H] \, \card{H}
            = \card{G}.
    \end{equation*}
    %
    The union of the conjugates of $H$ is thus properly contained in $G$.
\end{solution}


\begin{exerciseframed}[4.1.18]
    Let $S$ be a set endowed with a transitive action of a finite group $G$, and assume $\card{S} \geq 2$. Prove that there exists a $g \in G$ without fixed points in $S$, that is, such that $gs \neq s$ for all $s \in S$.
\end{exerciseframed}

\begin{solution}
    By the orbit-stabiliser theorem we may assume that $S = G/H$ with $H$ a proper subgroup of $G$ (since $\card{S} \geq 2$), and that the action on $G/H$ is left-multiplication. By \exref{4.1.17} there is a $g \in G$ that lies outside any conjugate of $H$, so that $g \not\in aHa\inv$ for all $a \in G$. Hence $ga \not\in aH$, so $gaH \neq aH$.
\end{solution}


\begin{exerciseframed}[4.1.20]
    Let $G = \GL{2}{\complex}$, and let $H$ be the subgroup consisting of upper triangular matrices (Exercise~II.6.2). Prove that $G$ is the union of the conjugates of $H$. Thus, the finiteness hypothesis in \exref{4.1.17} is necessary.
\end{exerciseframed}

\begin{solution}
    This in fact holds for $\GL{n}{\complex}$ for $n > 0$: If $A \in \GL{n}{\complex}$, then Schur's theorem yields a unitary matrix $Q$ and an upper triangular matrix $U$ such that $A = QUQ\inv$.
\end{solution}


\section{The Sylow theorems}

\begin{exerciseframed}[4.2.1]
    Prove Claim~2.2: Let $G$ be a finite group, let $p$ be a prime divisor of $\card{G}$, and let $N$ be the number of cyclic subgroups of $G$ of order $p$. Then $N \equiv 1 \pmod p$.
\end{exerciseframed}

\begin{solution}
    Notice that an element of $G$ lies in a (cyclic) subgroup of order $p$ if and only if it is either the identity or of order $p$ (by Lagrange's theorem). Thus any non-identity element of such a subgroup is a generator. Notice also that two distict such subgroups only intersect at the identity. It follows that, in the notation of the proof of Theorem~2.1, $\card{Z} = Np - N + 1$. Hence $N(p-1) + 1 \equiv 0 \pmod p$, which implies that
    %
    \begin{equation*}
        N
            \equiv -N(p-1)
            \equiv 1
            \mod p
    \end{equation*}
    %
    as desired.
\end{solution}


\begin{exerciseframed}[4.2.2]
    Let $G$ be a group. A subgroup $H$ of $G$ is \emph{characteristic} if $\phi(H) \subseteq H$ for every automorphism $\phi$ of $G$.
    %
    \begin{enumerate}
        \item Prove that characteristic subgroups are normal.
        
        \item Let $H \subseteq K \subseteq G$, with $H$ characteristic in $K$ and $K$ normal in $G$. Prove that $H$ is normal in $G$.
        
        \item Let $G, K$ be groups, and assume that $G$ contains a single subgroup $H$ isomorphic to $K$. Prove that $H$ is normal in $G$.

        \item Let $K$ be a normal subgroup of a finite group $G$, and assume that $\card{K}$ and $\card{G/K}$ are relatively prime. Prove that $K$ is characteristic in $G$.
    \end{enumerate}
\end{exerciseframed}

\begin{solution}
\begin{solutionsec}
    \item This is obvious, since then $gHg\inv \subseteq H$ for all $g \in G$.

    \item Let $g \in G$ and consider the inner automorphism $\gamma_g \in \Inn(G)$. Since $K$ is normal in $G$, this restricts to an automorphism $\gamma_g|_K \colon K \to K$. And since $H$ is characteristic in $K$, this implies that
    %
    \begin{equation*}
        gHg\inv
            = \gamma_g|_K(H)
            \subseteq H
    \end{equation*}
    %
    so $H$ is normal in $G$.

    \item For each $g \in G$, the subgroup $gHg\inv$ is isomorphic to $H$, hence to $K$. But then we must have $gHg\inv = H$, so $H$ is normal.

    \item We prove the following lemma:
    %
    \begin{displaytheorem}
        Let $G$ be a group, $H$ a finite subgroup, and $N$ a normal subgroup such that $[G : N]$ is finite. If $\card{H}$ and $[G : N]$ are relatively prime, then $H \subseteq N$.
    \end{displaytheorem}
    %
    By the third isomorphism theorem we have
    %
    \begin{equation*}
        \frac{HN}{N}
            \cong
            \frac{H}{H \intersect N},
    \end{equation*}
    %
    and these groups are finite since $H$ is. Notice that $[H : H \intersect N]$ is then a divisor of $\card{H}$, and also a divisor of $[G : N]$ since $[HN : N]$ is. But then $[H : H \intersect N] = 1$ by relative primality, showing that $H \intersect N = H$.

    We now prove the original claim. If $\phi$ is an automorphism of $G$, then $\phi(K)$ is a subgroup of $G$ with $\card{\phi(K)} = \card{K}$. Hence $\card{\phi(K)}$ and $[G : K]$ are relatively prime, so the lemma implies that $\phi(K) \subseteq K$. Since the two subgroups are finite with the same cardinality, this inclusion is in fact an equality.
\end{solutionsec}
\end{solution}


\begin{exerciseframed}[4.2.3]
    Prove that a nonzero abelian group $G$ is simple if and only if $G \cong \ints/p\ints$ for some positive prime integer $p$.
\end{exerciseframed}

\begin{solution}
    First notice that $G$ must be cyclic: If $g \neq e$, then $\gen{g}$ is a nontrivial normal subgroup of $G$, hence must equal $G$. Hence it suffices to find the simple cyclic groups. But these are precisely the groups $\ints/p\ints$ for primes $p$.
\end{solution}


\begin{exerciseframed}[4.2.4]
    Prove that a group $G$ is simple if and only if its only homomorphic images (i.e., groups $G'$ such that there is an onto homomorphism $G \to G'$) are the trivial group and $G$ itself (up to isomorphism). 
\end{exerciseframed}

\begin{solution}
    First assume that $G$ is simple, and let $\phi \colon G \to G'$ be a homomorphism. This is either trivial or injective, and in the latter case $G \cong \phi(G)$.

    Conversely, assume that $G$ is \emph{not} simple, and let $H$ be a nontrivial proper normal subgroup of $G$. Then $G/H$ is the image of the quotient map $G \to G/H$, and this is neither trivial nor isomorphic to $G$.
\end{solution}


\begin{exerciseframed}[4.2.5]
    Let G be a simple group, and assume $\phi \colon G \to G'$ is a nontrivial group homomorphism. Prove that $\phi$ is injective.
\end{exerciseframed}

\begin{solution}
    The kernel of $\phi$ is a normal subgroup of $G$, so this is either $\{e\}$ or $G$. The latter is impossible since $\phi$ is nontrivial, so $\ker \phi = \{e\}$. Hence $\phi$ is injective.
\end{solution}


\chapter{Irreducibility and factorization in integral domains}

\section{Chain conditions and existence of factorizations}

\section{UFDs, PIDs, Euclidean domains}

\section{Intermezzo: Zorn's lemma}

\section{Unique factorization in polynomial rings}

\begin{exerciseframed}[5.4.17]
    Let $F$ be a field, and recall the notion of characteristic of a ring (Definition III.3.7); the characteristic of a field is either $0$ or a prime integer (\exref{3.3.14}).
    %
    \begin{enumerate}
        \item Show that $F$ has characteristic $0$ if and only if it contains a copy of $\rationals$ and that $F$ has characteristic $p$ if and only if it contains a copy of the field $\ints/p\ints$.

        \item Show that (in both cases) this determines the smallest subfield of $F$; it is called the \emph{prime subfield} of $F$.
    \end{enumerate}
\end{exerciseframed}

\begin{solution}
    First assume that $F$ has characteristic $0$. Then the unique ring homomorphism $\iota \colon \ints \to F$ is injective, so $\iota(n)$ is a unit for $n \neq 0$. Define the map $\phi \colon \rationals \to F$ by
    %
    \begin{equation*}
        \phi \biggl( \frac{m}{n} \biggr)
            = \iota(m) \iota(n)\inv.
    \end{equation*}
    %
    This is clearly well-defined. Notice that
    %
    \begin{equation*}
        \phi \biggl( \frac{m}{n} \frac{p}{q} \biggr)
            = \phi \biggl( \frac{mp}{nq} \biggr)
            = \iota(mp) \iota(nq)\inv
            = \iota(m) \iota(n)\inv \iota(p) \iota(q)\inv
            = \phi \biggl( \frac{m}{n} \biggr) \phi \, \biggl( \frac{p}{q} \biggr),
    \end{equation*}
    %
    and that
    %
    \begin{align*}
        \phi \biggl( \frac{m}{n} + \frac{p}{q} \biggr)
            &= \phi \biggl( \frac{mq+pn}{nq} \biggr)
             = \iota(mq+pn) \iota(nq)\inv \\
            &= \iota(mq) \iota(nq)\inv + \iota(pn) \iota(nq)\inv \\
            &= \iota(m) \iota(n)\inv + \iota(p) \iota(q)\inv \\
            &= \phi \biggl( \frac{m}{n} \biggr) + \phi \biggl( \frac{p}{q} \biggr).
    \end{align*}
    %
    Thus $\phi$ is a ring homomorphism, and since $\rationals$ is a field it is also injective. Hence $F$ contains a copy of $\rationals$. Conversely, if there is an injective map $\rationals \to F$ then the diagram
    %
    \begin{equation*}
        \begin{tikzcd}[column sep=small]
            &\ints
                \ar[dl]
                \ar[dr]
            \\
            \rationals
                \ar[rr, hook]
            && F
        \end{tikzcd}
    \end{equation*}
    %
    commutes by uniqueness. Hence $\ints \to \rationals$ and $\ints \to F$ have the same kernel, implying that $\chr F = \chr \rationals = 0$.

    Next assume that $F$ has characteristic $p$. The unique ring homomorphism $\iota \colon \ints \to F$ then has kernel $p\ints$ so it induces an injection $\tilde{\iota} \colon \ints/p\ints \to F$ by the canonical decomposition. The converse follows as before.

    TODO (b)?
\end{solution}


\section{Irreducibility of polynomials}

\section{Further remarks and examples}

\begin{exerciseframed}[5.6.8]
    Let $n \in \ints$ be a positive integer and $n = p_1^{\alpha_1} \cdots p_r^{\alpha_r}$ its prime factorisation. By the classification theorem for finite abelian groups (or, in fact, simpler considerations; cf. \exref{2.4.9})
    %
    \begin{equation*}
        \frac{\ints}{(n)}
            \cong \frac{\ints}{(p_1^{\alpha_1})}
                  \prod \cdots
                  \prod \frac{\ints}{(p_r^{\alpha_r})}
    \end{equation*}
    %
    \emph{as abelian groups}.
    %
    \begin{enumerate}
        \item Use the CRT to prove that this is in fact a \emph{ring} isomorphism.

        \item Prove that
        %
        \begin{equation*}
            \biggl( \frac{\ints}{(n)} \biggr)^*
                \cong \biggl( \frac{\ints}{(p_1^{\alpha_1})} \biggr)^*
                      \prod \cdots
                      \prod \biggl( \frac{\ints}{(p_r^{\alpha_r})} \biggr)^*
        \end{equation*}
        %
        (recall that $(\ints/n\ints)^*$ denotes the group of units of $\ints/n\ints$).

        \item Recall (\exref{2.6.14}) that \emph{Euler's $\phi$-function $\phi(n)$} denotes the number of
        positive integers $< n$ that are relatively prime to $n$. Prove that
        %
        \begin{equation*}
            \phi(n)
                = p_1^{\alpha_1 - 1}(p_1 - 1) \cdots p_r^{\alpha_r - 1}(p_r - 1).
        \end{equation*}
    \end{enumerate}
\end{exerciseframed}

\begin{solution}
\begin{solutionsec}
    \item We prove this without appealing to the CRT. Simply notice that the maps $\pi^{mn}_m$ and $\pi^{mn}_n$ are ring isomorphisms for coprime $m$ and $n$. The claim then follows by induction.

    \item We prove that $(\ints/mn\ints)^* \cong (\ints/m\ints)^* \prod (\ints/n\ints)^*$ for coprime $m$ and $n$, where the (multiplicative) group isomorphism is just the restriction of the ring isomorphism above; the claim will then follow by induction. If $[a]_{mn} \in (\ints/mn\ints)^*$, then $a$ is also relatively prime to both $m$ and $n$, so $[a]_m \in (\ints/m\ints)^*$ and similarly for $n$.
    
    Conversely, if $[b]_m \in (\ints/m\ints)^*$ and $[c]_n \in (\ints/n\ints)^*$, then there exists an $a \in \ints$ such that $\pi^{mn}_m([a]_{mn}) = [b]_m$ and $\pi^{mn}_n([a]_{mn}) = [c]_n$. But then $a \equiv b \pmod m$ and $a \equiv c \pmod n$, so $a$ is relatively prime to both $m$ and $n$, hence to $mn$ as desired.

    \item We first notice that, by (b), $\phi(mn) = \phi(m) \phi(n)$ for coprime $m$ and $n$. Next notice that $\phi(p^k) = p^k - p^{k-1}$ for a prime $p$. The claim follows.
\end{solutionsec}
\end{solution}


\chapter{Linear algebra}

\chapter{Fields}

\section{Field extensions, I}



\begin{remarkbreak}[Initial objects in $\catFld_0$ and $\catFld_p$]
    We first show that $\rationals$ is initial in $\catFld_0$. Let $k$ be a field of characteristic $0$. Then \exref{5.4.17} implies the existence of a ring homomorphism $\phi \colon \rationals \to k$. Let $\psi \colon \rationals \to k$ be another homomorphism, and let $\iota \colon \ints \to \rationals$ be the unique ring homomorphism. Then $\phi \circ \iota = \psi \circ \iota$ by uniqueness, and since $\iota$ is an epimorphism by ¬ßIII.2.3 it follows that $\phi = \psi$.

    The same argument works in $\catFld_p$ with the quotient map $\pi_p$ playing the role of $\iota$, so $\field_p$ is initial in $\catFld_p$.
\end{remarkbreak}


\begin{exerciseframed}[7.1.1]
    Prove that if $k \subseteq K$ is a field extension, then $\chr k = \chr K$. Prove that the category $\catFld$ has no initial object.
\end{exerciseframed}

\begin{solution}
    Let $\phi \colon k \to K$ be a field extension, and let $\iota_k \colon \ints \to k$ and $\iota_K \colon \ints \to K$ be the unique ring homomorphisms. Notice that $\phi$ is injective by [ref], so since $\iota_K = \phi \circ \iota_k$ by uniqueness, we have $\ker \iota_k = \ker \iota_K$ and hence $\chr k = \chr K$.

    If $k$ is any field and $\chr k = p$, then there is no homomorphism $k \to \field_q$ for $q \neq p$. Hence $k$ is not initial in $\catFld$.
\end{solution}


\end{document}